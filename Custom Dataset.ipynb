{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Cityscapes\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Cityscapes\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CityscapesDataset(Cityscapes):\n",
    "    def __init__(self, root, split='train', mode='fine', target_type='semantic', shrinkToSize=None, \n",
    "                 cropHeight=512, cropWidth=1024, transform=None, target_transform=None):\n",
    "        super(CityscapesDataset, self).__init__(root, split=split, mode=mode, target_type=target_type)\n",
    "        \n",
    "        #defining constants\n",
    "        self.n_classes = 19\n",
    "        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n",
    "        self.valid_classes = [ 7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
    "        self.class_names = [\n",
    "            \"road\", \"sidewalk\", \"building\", \"wall\", \"fence\", \"pole\", \"traffic_light\", \"traffic_sign\",\n",
    "            \"vegetation\", \"terrain\", \"sky\", \"person\", \"rider\", \"car\", \"truck\", \"bus\", \"train\", \"motorcycle\", \n",
    "            \"bicycle\",\n",
    "        ]\n",
    "\n",
    "        self.ignore_index = -1\n",
    "        self.class_map = dict(zip(self.valid_classes, range(self.n_classes)))\n",
    "        self.rev_class_map = dict(zip(range(self.n_classes), self.valid_classes))\n",
    "\n",
    "        \n",
    "        #defining transform params\n",
    "        self.op_size = (cropHeight, cropWidth)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        \n",
    "        if shrinkToSize is not None:\n",
    "            self.images = self.images[:shrinkToSize]\n",
    "            self.targets = self.targets[:shrinkToSize]\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, lbl = super().__getitem__(index)\n",
    "        img, lbl = self.transformImgMask(img, lbl)\n",
    "        \n",
    "        # remove void classes from label\n",
    "        lbl = self.encode_segmap(lbl)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            lbl = self.target_transform(lbl)\n",
    "        \n",
    "        return img, lbl\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def encode_segmap(self, mask):\n",
    "        # Put all void classes to zero\n",
    "        for _voidc in self.void_classes:\n",
    "            mask[mask == _voidc] = self.ignore_index\n",
    "        for _validc in self.valid_classes:\n",
    "            mask[mask == _validc] = self.class_map[_validc]\n",
    "        return mask\n",
    "    \n",
    "    def transformImgMask(self, image, mask):\n",
    "        \n",
    "        #crop only training images\n",
    "        if self.split=='train':\n",
    "            # Random Crop\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(image, output_size=self.op_size)\n",
    "            image = TF.crop(image, i, j, h, w)\n",
    "            mask = TF.crop(mask, i, j, h, w)\n",
    "        \n",
    "        # Implement other transforms like vertical and horizontal flips\n",
    "        \n",
    "        # Transform to Tensor\n",
    "        image = self.transformToTensor(image, normalize=True)\n",
    "        mask = self.transformToTensor(mask, normalize=False)\n",
    "        return image, mask\n",
    "    \n",
    "    def transformToTensor(self, img, normalize=False):\n",
    "        \n",
    "        res = torch.from_numpy(np.array(img, np.int64, copy=False))\n",
    "        res = res.view(img.size[1], img.size[0], len(img.getbands()))\n",
    "        res = res.permute((2, 0, 1)).contiguous()\n",
    "        if normalize:\n",
    "            return res.float().div(255)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = CityscapesDataset(\"./data\", split='val', target_type='semantic', shrinkToSize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, sem = cd[0]\n",
    "# cd.transform(img, sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 1024])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape\n",
    "sem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(cd, batch_size=bs, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498, 172, 512, 1024)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.RandomCrop.get_params(img, output_size=(512, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "o = torch.from_numpy(x)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6291456])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes()))\n",
    "pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([172, 206, 201,  ...,  55,  70,  63], dtype=torch.uint8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 1024])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lbl = torch.from_numpy(sem).long()\n",
    "# lbl.shape\n",
    "pic2 = torch.from_numpy(np.array(sem, np.int32, copy=False))\n",
    "pic2 = pic2.view(sem.size[1], sem.size[0], len(sem.getbands()))\n",
    "pic2 = pic2.permute((2, 0, 1)).contiguous()\n",
    "pic2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = np.unique(pic2)\n",
    "len(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlabeled\n",
      "ego vehicle\n",
      "out of roi\n",
      "static\n",
      "road\n",
      "sidewalk\n",
      "building\n",
      "pole\n",
      "traffic sign\n",
      "vegetation\n",
      "terrain\n",
      "sky\n",
      "person\n",
      "rider\n",
      "car\n",
      "bicycle\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(u)):\n",
    "    idx = u[i]\n",
    "    print (cd.classes[idx].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caluclate Weights for CrossEntropy Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "torch.Size([1, 1, 1024, 2048])\n",
      "done  1\n",
      "done  2\n",
      "done  3\n",
      "done  4\n",
      "done  5\n",
      "done  6\n",
      "done  7\n",
      "done  8\n",
      "done  9\n",
      "done  10\n",
      "done  11\n",
      "done  12\n",
      "done  13\n",
      "done  14\n",
      "done  15\n",
      "done  16\n",
      "done  17\n",
      "done  18\n",
      "done  19\n",
      "done  20\n",
      "done  21\n",
      "done  22\n",
      "done  23\n",
      "done  24\n",
      "done  25\n",
      "done  26\n",
      "done  27\n",
      "done  28\n",
      "done  29\n",
      "done  30\n",
      "done  31\n",
      "done  32\n",
      "done  33\n",
      "done  34\n",
      "done  35\n",
      "done  36\n",
      "done  37\n",
      "done  38\n",
      "done  39\n",
      "done  40\n",
      "done  41\n",
      "done  42\n",
      "done  43\n",
      "done  44\n",
      "done  45\n",
      "done  46\n",
      "done  47\n",
      "done  48\n",
      "done  49\n",
      "done  50\n",
      "[37077183.  5094904. 17139713.   343017.   953831.  2120990.   200740.\n",
      "   536166. 15092411.   711004.  3520355.   751515.   167691.  5188745.\n",
      "   708579.   302469.   101684.   211537.   298767.]\n"
     ]
    }
   ],
   "source": [
    "freq = np.zeros(cd.n_classes)\n",
    "voids=0\n",
    "p=0\n",
    "for img, lbls in dl:\n",
    "    \n",
    "#     fig, ax = plt.subplots(1,1)\n",
    "#     ax.imshow(np.transpose(img[0], [1,2,0]))\n",
    "#     plt.show()\n",
    "    \n",
    "    if p==0:\n",
    "        print(len(dl))\n",
    "        print(lbls.shape)\n",
    "    \n",
    "    p+=1\n",
    "    uniques, counts = np.unique(lbls, return_counts=True)\n",
    "    \n",
    "    for j,uni in enumerate(uniques):\n",
    "        if uni==-1:\n",
    "            voids+=counts[j]\n",
    "        else:\n",
    "            freq[uni] += counts[j]\n",
    "    \n",
    "    print(\"done \",p*bs)\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1  14336299 \t unlabelled \n",
      "0   37077183.0 \t  road   road\n",
      "1   5094904.0 \t  sidewalk   sidewalk\n",
      "2   17139713.0 \t  building   building\n",
      "3   343017.0 \t  wall   wall\n",
      "4   953831.0 \t  fence   fence\n",
      "5   2120990.0 \t  pole   pole\n",
      "6   200740.0 \t  traffic light   traffic_light\n",
      "7   536166.0 \t  traffic sign   traffic_sign\n",
      "8   15092411.0 \t  vegetation   vegetation\n",
      "9   711004.0 \t  terrain   terrain\n",
      "10   3520355.0 \t  sky   sky\n",
      "11   751515.0 \t  person   person\n",
      "12   167691.0 \t  rider   rider\n",
      "13   5188745.0 \t  car   car\n",
      "14   708579.0 \t  truck   truck\n",
      "15   302469.0 \t  bus   bus\n",
      "16   101684.0 \t  train   train\n",
      "17   211537.0 \t  motorcycle   motorcycle\n",
      "18   298767.0 \t  bicycle   bicycle\n"
     ]
    }
   ],
   "source": [
    "print(\"-1 \",voids,\"\\t unlabelled \")\n",
    "for i in range(cd.n_classes):\n",
    "    print(i,\" \",freq[i],\"\\t \",cd.classes[cd.rev_class_map[i]].name, \" \", cd.class_names[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images scanned: 50.0\n"
     ]
    }
   ],
   "source": [
    "print(\"images scanned: \"+str((np.sum(freq)+voids)/(1024*2048)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           1.        ]\n",
      " [  1.           7.27730748]\n",
      " [  2.           2.16323243]\n",
      " [  3.         108.09138614]\n",
      " [  4.          38.8718578 ]\n",
      " [  5.          17.48107393]\n",
      " [  6.         184.70251569]\n",
      " [  7.          69.15243227]\n",
      " [  8.           2.45667727]\n",
      " [  9.          52.14764333]\n",
      " [ 10.          10.53222843]\n",
      " [ 11.          49.3365841 ]\n",
      " [ 12.         221.10419164]\n",
      " [ 13.           7.1456938 ]\n",
      " [ 14.          52.32611043]\n",
      " [ 15.         122.5817621 ]\n",
      " [ 16.         364.63143661]\n",
      " [ 17.         175.27516699]\n",
      " [ 18.         124.10066373]]\n"
     ]
    }
   ],
   "source": [
    "max_val = np.max(freq)\n",
    "weights = np.zeros([cd.n_classes, 2])\n",
    "\n",
    "for i in range(cd.n_classes):\n",
    "    weights[i][0] = i//1\n",
    "    weights[i][1] = max_val/freq[i]\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df = pd.DataFrame(weights, columns=[ \"class_id\", \"class_weight\"])\n",
    "class_names_df = pd.Series(cd.class_names)\n",
    "weight_df = pd.concat((class_names_df.rename('class_name'), weight_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df.to_csv('class_weights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>class_name</th>\n",
       "      <th>class_id</th>\n",
       "      <th>class_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>road</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sidewalk</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.277307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>building</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.163232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>wall</td>\n",
       "      <td>3.0</td>\n",
       "      <td>108.091386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fence</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.871858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 class_name  class_id  class_weight\n",
       "0           0       road       0.0      1.000000\n",
       "1           1   sidewalk       1.0      7.277307\n",
       "2           2   building       2.0      2.163232\n",
       "3           3       wall       3.0    108.091386\n",
       "4           4      fence       4.0     38.871858"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try1 = pd.read_csv('class_weights.csv')\n",
    "try1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(try1['class_weight'])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
