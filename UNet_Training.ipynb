{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkqPa4xcfmhe"
   },
   "source": [
    "# U-Net Training Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jIorw8ehGOg"
   },
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_BGk0eBfrPN"
   },
   "source": [
    "### Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "id": "1DqKla09eScY",
    "outputId": "2a4e148a-1194-420e-c0e1-90888f7127b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "root_drive = \"/content/gdrive/My\\ Drive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l0-yKdz-fmZH"
   },
   "outputs": [],
   "source": [
    "path = root_drive + '/Learning/UCI-274P/Project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "kdV3p0lsfmUW",
    "outputId": "5038b516-829c-4a0b-9265-7257d41eb1b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Learning/UCI-274P/Project\n"
     ]
    }
   ],
   "source": [
    "#change directory to path\n",
    "% cd {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mF8tsYuCfvE8"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JOecvRXIf8-H"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim \n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from loader import cityscapesDataset\n",
    "from models import unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2p8YXpAf0cN"
   },
   "source": [
    "### CUDA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "BAJU4_QbfmTC",
    "outputId": "41a643f5-3248-47eb-85b2-dc4a59f604ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFofChPOoIEA"
   },
   "outputs": [],
   "source": [
    "def print_free_memory():\n",
    "  t = torch.cuda.get_device_properties(0).total_memory\n",
    "  torch.cuda.empty_cache()\n",
    "  c = torch.cuda.memory_cached(0)\n",
    "  a = torch.cuda.memory_allocated(0)\n",
    "  f = c-a  # free inside cache\n",
    "  print('Total CUDA: ',t/(1024*1024*1024),' GB Free space: ',f/(1024*1024) ,\" MB\")\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34KfTWzIf77w"
   },
   "source": [
    "This is a method to  save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vqn3Twq1SAZv"
   },
   "outputs": [],
   "source": [
    "savefolder = \"./saves/exp1/\"\n",
    "startEpoch = 0\n",
    "\n",
    "def saveState(filename, epoch, model, optimizer, loss):\n",
    "  savefile = savefolder + filename + str(epoch) + \".pth\"\n",
    "  torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, savefile)\n",
    "  return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNGze2XggAQx"
   },
   "source": [
    "## Load dataset images to Custom Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ccndtbSWfl7I"
   },
   "outputs": [],
   "source": [
    "cd_tr = cityscapesDataset.CityscapesDataset(\"./Cityscapes\", split='train', target_type='semantic', cropHeight=256, cropWidth=512)\n",
    "cd_val = cityscapesDataset.CityscapesDataset(\"./Cityscapes\", split='val', target_type='semantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "CFrtRnWLno-o",
    "outputId": "7ed56200-308d-4170-95cb-c09bb40fbcf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024, 2048])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, lbl = cd_val[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zrKHIdZgKgj"
   },
   "source": [
    "### Create Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "si4s0f06gtJ2"
   },
   "outputs": [],
   "source": [
    "learner = unet.UNet(3, cd_tr.n_classes).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RYmyXHZrgaNy"
   },
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abpTuuGzh5gX"
   },
   "outputs": [],
   "source": [
    "bs = 16\n",
    "n_workers=0\n",
    "n_epochs = 30\n",
    "lr = 0.00003\n",
    "mom=0.9# Not used\n",
    "wd = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-MV0TB0BgQcv"
   },
   "source": [
    "### Create Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JubX5M6ziBnx"
   },
   "outputs": [],
   "source": [
    "dl_train = DataLoader(cd_tr, batch_size=bs, num_workers=n_workers, shuffle=True, pin_memory=True) \n",
    "dl_val = DataLoader(cd_val, batch_size=1, num_workers=n_workers, shuffle=True, pin_memory=True)\n",
    "val_iter = iter(dl_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qILYWqtVgV8R"
   },
   "source": [
    "## Experiment 1 - Training using Adam and CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l0acdeALiK1Z"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(learner.parameters(), lr=lr, weight_decay=wd)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=cd_tr.ignore_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "c_ZFZxa_u3A-",
    "outputId": "f1077958-e114-46e6-851f-ac106ab32b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CUDA:  11.17303466796875  GB Free space:  13.42724609375  MB\n"
     ]
    }
   ],
   "source": [
    "print_free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ZiVKSJ1XIUz"
   },
   "outputs": [],
   "source": [
    "savefile=savefolder+\"save60.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "g70vYkruXMnc",
    "outputId": "02fa755f-496c-4667-b098-2280d39da5df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  60  Loaded.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(savefile)\n",
    "learner.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "startEpoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "print(\"Epoch \",startEpoch,\" Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Ont0q73grVU"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e76e1334a002410a852b0643a8bf8a57",
      "79f5f65cd3654286889f6705916bde59",
      "669fc0eba1af418f9e126691035eea72",
      "90c4b614c37746448e6eeac765fccea3",
      "fe5eca4753874493aae6868bcef356a2",
      "8a767a54133a4591a50ed379d03a450c",
      "58b6bc4a03b8490ea2df94edf76ccac9",
      "e15da71a58a1404d8aea8efee6243d4f"
     ]
    },
    "colab_type": "code",
    "id": "b4MC0NbSiRZU",
    "outputId": "59a59d7e-3e01-4b99-ea0e-14d36e291158"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76e1334a002410a852b0643a8bf8a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Train Iter:  79  Loss:  1.3101830127872998\n",
      "Train Iter:  80  Loss:  1.3091618411242962\n",
      "Train Iter:  81  Loss:  1.3077242293475586\n",
      "Train Iter:  82  Loss:  1.3055364424135627\n",
      "Train Iter:  83  Loss:  1.3145404759659824\n",
      "Train Iter:  84  Loss:  1.3173195712623142\n",
      "Train Iter:  85  Loss:  1.3173883459147284\n",
      "Train Iter:  86  Loss:  1.3150533610998199\n",
      "Train Iter:  87  Loss:  1.315201232488128\n",
      "Train Iter:  88  Loss:  1.3170169232921167\n",
      "Train Iter:  89  Loss:  1.3135996862743677\n",
      "Train Iter:  90  Loss:  1.3103956149684057\n",
      "Train Iter:  91  Loss:  1.3097714480462965\n",
      "Train Iter:  92  Loss:  1.3116806985243508\n",
      "Train Iter:  93  Loss:  1.3136344898131587\n",
      "Train Iter:  94  Loss:  1.3149363189301593\n",
      "Train Iter:  95  Loss:  1.3155137683215894\n",
      "Train Iter:  96  Loss:  1.3159661907702684\n",
      "Train Iter:  97  Loss:  1.3135252508920492\n",
      "Train Iter:  98  Loss:  1.3091807341089055\n",
      "Train Iter:  99  Loss:  1.3073585105664802\n",
      "Train Iter:  100  Loss:  1.3054163730144501\n",
      "Train Iter:  101  Loss:  1.3074378979088057\n",
      "Train Iter:  102  Loss:  1.3093813949940252\n",
      "Train Iter:  103  Loss:  1.30678910653568\n",
      "Train Iter:  104  Loss:  1.308579799074393\n",
      "Train Iter:  105  Loss:  1.307408805120559\n",
      "Train Iter:  106  Loss:  1.3073303497062538\n",
      "Train Iter:  107  Loss:  1.3075423474623777\n",
      "Train Iter:  108  Loss:  1.310249438992253\n",
      "Train Iter:  109  Loss:  1.3094074769851265\n",
      "Train Iter:  110  Loss:  1.30906231620095\n",
      "Train Iter:  111  Loss:  1.3088807963036202\n",
      "Train Iter:  112  Loss:  1.3073023983410426\n",
      "Train Iter:  113  Loss:  1.3090748850223237\n",
      "Train Iter:  114  Loss:  1.3094413824248732\n",
      "Train Iter:  115  Loss:  1.3076069717821868\n",
      "Train Iter:  116  Loss:  1.3085581670547355\n",
      "Train Iter:  117  Loss:  1.307337655980363\n",
      "Train Iter:  118  Loss:  1.3096001491708271\n",
      "Train Iter:  119  Loss:  1.3059959632008016\n",
      "Train Iter:  120  Loss:  1.305541568994522\n",
      "Train Iter:  121  Loss:  1.3067870258299772\n",
      "Train Iter:  122  Loss:  1.3076727351204294\n",
      "Train Iter:  123  Loss:  1.3095430785078344\n",
      "Train Iter:  124  Loss:  1.3118507660204364\n",
      "Train Iter:  125  Loss:  1.31065580368042\n",
      "Train Iter:  126  Loss:  1.3116638017079187\n",
      "Train Iter:  127  Loss:  1.3116351431748998\n",
      "Train Iter:  128  Loss:  1.3115395894274116\n",
      "Train Iter:  129  Loss:  1.3118305890135062\n",
      "Train Iter:  130  Loss:  1.309697295152224\n",
      "Train Iter:  131  Loss:  1.3085579835731564\n",
      "Train Iter:  132  Loss:  1.3077817369591107\n",
      "Train Iter:  133  Loss:  1.3072108953518975\n",
      "Train Iter:  134  Loss:  1.3096101212857374\n",
      "Train Iter:  135  Loss:  1.3089454253514607\n",
      "Train Iter:  136  Loss:  1.310027136522181\n",
      "Train Iter:  137  Loss:  1.3090558548043245\n",
      "Train Iter:  138  Loss:  1.3101923474367114\n",
      "Train Iter:  139  Loss:  1.3144155423418225\n",
      "Train Iter:  140  Loss:  1.3143334916659764\n",
      "Train Iter:  141  Loss:  1.317148709128089\n",
      "Train Iter:  142  Loss:  1.3174839447921431\n",
      "Train Iter:  143  Loss:  1.3187362372458398\n",
      "Train Iter:  144  Loss:  1.3169837196667988\n",
      "Train Iter:  145  Loss:  1.3165033348675432\n",
      "Train Iter:  146  Loss:  1.3140758243325639\n",
      "Train Iter:  147  Loss:  1.3109951785632543\n",
      "Train Iter:  148  Loss:  1.3102627094532993\n",
      "Train Iter:  149  Loss:  1.3112586428655073\n",
      "Train Iter:  150  Loss:  1.31067888935407\n",
      "Train Iter:  151  Loss:  1.3089004324761448\n",
      "Train Iter:  152  Loss:  1.308214002141827\n",
      "Train Iter:  153  Loss:  1.309091679021424\n",
      "Train Iter:  154  Loss:  1.310046911626667\n",
      "Train Iter:  155  Loss:  1.3091892300113555\n",
      "Train Iter:  156  Loss:  1.3092973816853304\n",
      "Train Iter:  157  Loss:  1.3081224553144661\n",
      "Train Iter:  158  Loss:  1.307887252750276\n",
      "Train Iter:  159  Loss:  1.3085462908324956\n",
      "Train Iter:  160  Loss:  1.3089724060148\n",
      "Train Iter:  161  Loss:  1.31121302724625\n",
      "Train Iter:  162  Loss:  1.3114248149924808\n",
      "Train Iter:  163  Loss:  1.3118744865516943\n",
      "Train Iter:  164  Loss:  1.311549402228216\n",
      "Train Iter:  165  Loss:  1.3122720888166717\n",
      "Train Iter:  166  Loss:  1.3115810733961772\n",
      "Train Iter:  167  Loss:  1.3120078423066053\n",
      "Train Iter:  168  Loss:  1.3112503322107452\n",
      "Train Iter:  169  Loss:  1.3094051487347078\n",
      "Train Iter:  170  Loss:  1.3120098664480098\n",
      "Train Iter:  171  Loss:  1.3121802998565093\n",
      "Train Iter:  172  Loss:  1.3121296129254407\n",
      "Train Iter:  173  Loss:  1.312315412683983\n",
      "Train Iter:  174  Loss:  1.3115766723265594\n",
      "Train Iter:  175  Loss:  1.3117544272967747\n",
      "Train Iter:  176  Loss:  1.3121540712361985\n",
      "Train Iter:  177  Loss:  1.3128226796785991\n",
      "Train Iter:  178  Loss:  1.3107037591130546\n",
      "Train Iter:  179  Loss:  1.311399466498604\n",
      "Train Iter:  180  Loss:  1.310882611407174\n",
      "Train Iter:  181  Loss:  1.3101499350690051\n",
      "Train Iter:  182  Loss:  1.3096300645188972\n",
      "Train Iter:  183  Loss:  1.308414801873796\n",
      "Train Iter:  184  Loss:  1.308038288484449\n",
      "Train Iter:  185  Loss:  1.3086617779087375\n",
      "Train Iter:  186  Loss:  1.3090045804618506\n",
      "Val Iter: Loss:  0.9929212331771851\n",
      "Epoch 3, Mean Train Loss: 1.3090045804618506, Val Loss (1 sample): 0.9929212331771851\n",
      "Train Iter:  1  Loss:  1.1500436067581177\n",
      "Train Iter:  2  Loss:  1.2627219557762146\n",
      "Train Iter:  3  Loss:  1.3200908501942952\n",
      "Train Iter:  4  Loss:  1.2610064148902893\n",
      "Train Iter:  5  Loss:  1.2702854871749878\n",
      "Train Iter:  6  Loss:  1.2321685155232747\n",
      "Train Iter:  7  Loss:  1.2216255494526453\n",
      "Train Iter:  8  Loss:  1.1822771281003952\n",
      "Train Iter:  9  Loss:  1.181358231438531\n",
      "Train Iter:  10  Loss:  1.1561238586902618\n",
      "Train Iter:  11  Loss:  1.1549529324878345\n",
      "Train Iter:  12  Loss:  1.1468272755543392\n",
      "Train Iter:  13  Loss:  1.1634751420754652\n",
      "Train Iter:  14  Loss:  1.167899604354586\n",
      "Train Iter:  15  Loss:  1.1671113133430482\n",
      "Train Iter:  16  Loss:  1.1968135796487331\n",
      "Train Iter:  17  Loss:  1.2020708918571472\n",
      "Train Iter:  18  Loss:  1.194158246119817\n",
      "Train Iter:  19  Loss:  1.1742762013485557\n",
      "Train Iter:  20  Loss:  1.188976275920868\n",
      "Train Iter:  21  Loss:  1.193760945683434\n",
      "Train Iter:  22  Loss:  1.1756543788042935\n",
      "Train Iter:  23  Loss:  1.1796926259994507\n",
      "Train Iter:  24  Loss:  1.1651292716463406\n",
      "Train Iter:  25  Loss:  1.175777380466461\n",
      "Train Iter:  26  Loss:  1.1729740523374999\n",
      "Train Iter:  27  Loss:  1.1775305028314944\n",
      "Train Iter:  28  Loss:  1.18359309222017\n",
      "Train Iter:  29  Loss:  1.1977082758114255\n",
      "Train Iter:  30  Loss:  1.2091860870520275\n",
      "Train Iter:  31  Loss:  1.2197352628554068\n",
      "Train Iter:  32  Loss:  1.219451678916812\n",
      "Train Iter:  33  Loss:  1.2208144357710173\n",
      "Train Iter:  34  Loss:  1.2248026605914621\n",
      "Train Iter:  35  Loss:  1.220787159034184\n",
      "Train Iter:  36  Loss:  1.2234282775057688\n",
      "Train Iter:  37  Loss:  1.228487361121822\n",
      "Train Iter:  38  Loss:  1.225022289313768\n",
      "Train Iter:  39  Loss:  1.2154745092758765\n",
      "Train Iter:  40  Loss:  1.222019724547863\n",
      "Train Iter:  41  Loss:  1.2184076469119003\n",
      "Train Iter:  42  Loss:  1.2164091311749958\n",
      "Train Iter:  43  Loss:  1.2090343708215758\n",
      "Train Iter:  44  Loss:  1.2086990692398765\n",
      "Train Iter:  45  Loss:  1.2096244520611232\n",
      "Train Iter:  46  Loss:  1.2136085992274077\n",
      "Train Iter:  47  Loss:  1.2148950328218175\n",
      "Train Iter:  48  Loss:  1.2245332648356755\n",
      "Train Iter:  49  Loss:  1.2273371827845672\n",
      "Train Iter:  50  Loss:  1.2275210118293762\n",
      "Train Iter:  51  Loss:  1.2245736940234315\n",
      "Train Iter:  52  Loss:  1.2219182390433092\n",
      "Train Iter:  53  Loss:  1.2318284241658337\n",
      "Train Iter:  54  Loss:  1.2346171692565635\n",
      "Train Iter:  55  Loss:  1.2368167660453102\n",
      "Train Iter:  56  Loss:  1.235810358609472\n",
      "Train Iter:  57  Loss:  1.2329924462134378\n",
      "Train Iter:  58  Loss:  1.237134390863879\n",
      "Train Iter:  59  Loss:  1.2361750340057631\n",
      "Train Iter:  60  Loss:  1.23572891553243\n",
      "Train Iter:  61  Loss:  1.2330100301836358\n",
      "Train Iter:  62  Loss:  1.2296546332297786\n",
      "Train Iter:  63  Loss:  1.237039531980242\n",
      "Train Iter:  64  Loss:  1.2345576472580433\n",
      "Train Iter:  65  Loss:  1.2358484579966618\n",
      "Train Iter:  66  Loss:  1.2344095725001711\n",
      "Train Iter:  67  Loss:  1.237332404549442\n",
      "Train Iter:  68  Loss:  1.240551282377804\n",
      "Train Iter:  69  Loss:  1.2406473280727\n",
      "Train Iter:  70  Loss:  1.2378698127610344\n",
      "Train Iter:  71  Loss:  1.2363878589280894\n",
      "Train Iter:  72  Loss:  1.2352022760444217\n",
      "Train Iter:  73  Loss:  1.2327089309692383\n",
      "Train Iter:  74  Loss:  1.2286388721015002\n",
      "Train Iter:  75  Loss:  1.2258911410967508\n",
      "Train Iter:  76  Loss:  1.230630644842198\n",
      "Train Iter:  77  Loss:  1.2292791643700043\n",
      "Train Iter:  78  Loss:  1.223676091585404\n",
      "Train Iter:  79  Loss:  1.222227553778057\n",
      "Train Iter:  80  Loss:  1.225749136507511\n",
      "Train Iter:  81  Loss:  1.2372092008590698\n",
      "Train Iter:  82  Loss:  1.2382775283441312\n",
      "Train Iter:  83  Loss:  1.2409142212695385\n",
      "Train Iter:  84  Loss:  1.244529474349249\n",
      "Train Iter:  85  Loss:  1.2475111400379855\n",
      "Train Iter:  86  Loss:  1.2465324595917102\n",
      "Train Iter:  87  Loss:  1.2445109959306389\n",
      "Train Iter:  88  Loss:  1.2433345372026616\n",
      "Train Iter:  89  Loss:  1.2418739447432958\n",
      "Train Iter:  90  Loss:  1.240683831108941\n",
      "Train Iter:  91  Loss:  1.2477456789750319\n",
      "Train Iter:  92  Loss:  1.2520278627457826\n",
      "Train Iter:  93  Loss:  1.2541203345021894\n",
      "Train Iter:  94  Loss:  1.2555294011501557\n",
      "Train Iter:  95  Loss:  1.2551895442761873\n",
      "Train Iter:  96  Loss:  1.2576291412115097\n",
      "Train Iter:  97  Loss:  1.2590169402741895\n",
      "Train Iter:  98  Loss:  1.2587528690999867\n",
      "Train Iter:  99  Loss:  1.2558540894527628\n",
      "Train Iter:  100  Loss:  1.2553927904367448\n",
      "Train Iter:  101  Loss:  1.2552521919259931\n",
      "Train Iter:  102  Loss:  1.2566723432026656\n",
      "Train Iter:  103  Loss:  1.2545445503540409\n",
      "Train Iter:  104  Loss:  1.255806678189681\n",
      "Train Iter:  105  Loss:  1.2560298391750881\n",
      "Train Iter:  106  Loss:  1.2580754875012163\n",
      "Train Iter:  107  Loss:  1.2595435085697708\n",
      "Train Iter:  108  Loss:  1.2583011670245066\n",
      "Train Iter:  109  Loss:  1.256671755138887\n",
      "Train Iter:  110  Loss:  1.2604664396155965\n",
      "Train Iter:  111  Loss:  1.2586914252590489\n",
      "Train Iter:  112  Loss:  1.2611950130334921\n",
      "Train Iter:  113  Loss:  1.266646411039133\n",
      "Train Iter:  114  Loss:  1.2686503838028824\n",
      "Train Iter:  115  Loss:  1.2676332893578903\n",
      "Train Iter:  116  Loss:  1.2702806895149166\n",
      "Train Iter:  117  Loss:  1.2687937882211473\n",
      "Train Iter:  118  Loss:  1.2700923666105433\n",
      "Train Iter:  119  Loss:  1.2699889751041638\n",
      "Train Iter:  120  Loss:  1.2696817134817442\n",
      "Train Iter:  121  Loss:  1.268101098616261\n",
      "Train Iter:  122  Loss:  1.2697894245874686\n",
      "Train Iter:  123  Loss:  1.268281153062495\n",
      "Train Iter:  124  Loss:  1.2688958390105156\n",
      "Train Iter:  125  Loss:  1.266978886127472\n",
      "Train Iter:  126  Loss:  1.2661534372776273\n",
      "Train Iter:  127  Loss:  1.267965359481301\n",
      "Train Iter:  128  Loss:  1.2676085163839161\n",
      "Train Iter:  129  Loss:  1.2683716887651488\n",
      "Train Iter:  130  Loss:  1.2690937615357913\n",
      "Train Iter:  131  Loss:  1.268511327623411\n",
      "Train Iter:  132  Loss:  1.2712470053723364\n",
      "Train Iter:  133  Loss:  1.2714827862897313\n",
      "Train Iter:  134  Loss:  1.271899516457942\n",
      "Train Iter:  135  Loss:  1.2706125961409676\n",
      "Train Iter:  136  Loss:  1.2767911847023403\n",
      "Train Iter:  137  Loss:  1.2746414089724964\n",
      "Train Iter:  138  Loss:  1.2746216676373412\n",
      "Train Iter:  139  Loss:  1.273526812200066\n",
      "Train Iter:  140  Loss:  1.2747741175549372\n",
      "Train Iter:  141  Loss:  1.2741596170351015\n",
      "Train Iter:  142  Loss:  1.2714045308005641\n",
      "Train Iter:  143  Loss:  1.2718047187044903\n",
      "Train Iter:  144  Loss:  1.2691215558184519\n",
      "Train Iter:  145  Loss:  1.2676671217227804\n",
      "Train Iter:  146  Loss:  1.2677427448638499\n",
      "Train Iter:  147  Loss:  1.2676249223501503\n",
      "Train Iter:  148  Loss:  1.2677581366654989\n",
      "Train Iter:  149  Loss:  1.2673194624433581\n",
      "Train Iter:  150  Loss:  1.2656989415486655\n",
      "Train Iter:  151  Loss:  1.2647006069587556\n",
      "Train Iter:  152  Loss:  1.268842851645068\n",
      "Train Iter:  153  Loss:  1.2678776995029324\n",
      "Train Iter:  154  Loss:  1.2680239584538844\n",
      "Train Iter:  155  Loss:  1.2676904139980194\n",
      "Train Iter:  156  Loss:  1.2687731660329378\n",
      "Train Iter:  157  Loss:  1.268438069683731\n",
      "Train Iter:  158  Loss:  1.2687633807146097\n",
      "Train Iter:  159  Loss:  1.2698800068981242\n",
      "Train Iter:  160  Loss:  1.2715305022895336\n",
      "Train Iter:  161  Loss:  1.2710481956138373\n",
      "Train Iter:  162  Loss:  1.2705111488883878\n",
      "Train Iter:  163  Loss:  1.2703703137263198\n",
      "Train Iter:  164  Loss:  1.2711860828283357\n",
      "Train Iter:  165  Loss:  1.271523454695037\n",
      "Train Iter:  166  Loss:  1.2708996742604726\n",
      "Train Iter:  167  Loss:  1.2694031164317787\n",
      "Train Iter:  168  Loss:  1.2704491154068993\n",
      "Train Iter:  169  Loss:  1.2684996283265966\n",
      "Train Iter:  170  Loss:  1.2685572294627918\n",
      "Train Iter:  171  Loss:  1.2677029712855468\n",
      "Train Iter:  172  Loss:  1.2668255262596662\n",
      "Train Iter:  173  Loss:  1.2671367708658208\n",
      "Train Iter:  174  Loss:  1.2668923872640763\n",
      "Train Iter:  175  Loss:  1.268683796610151\n",
      "Train Iter:  176  Loss:  1.268562490967187\n",
      "Train Iter:  177  Loss:  1.2701388227064057\n",
      "Train Iter:  178  Loss:  1.2709910621803797\n",
      "Train Iter:  179  Loss:  1.2706522788415409\n",
      "Train Iter:  180  Loss:  1.270584088563919\n",
      "Train Iter:  181  Loss:  1.270586173837356\n",
      "Train Iter:  182  Loss:  1.2701776813674759\n",
      "Train Iter:  183  Loss:  1.2679927805082394\n",
      "Train Iter:  184  Loss:  1.268130903010783\n",
      "Train Iter:  185  Loss:  1.2688550775115555\n",
      "Train Iter:  186  Loss:  1.2671419833936999\n",
      "Val Iter: Loss:  1.466815710067749\n",
      "Epoch 4, Mean Train Loss: 1.2671419833936999, Val Loss (1 sample): 1.466815710067749\n",
      "Train Iter:  1  Loss:  1.1763837337493896\n",
      "Train Iter:  2  Loss:  1.4017731547355652\n",
      "Train Iter:  3  Loss:  1.3382885456085205\n",
      "Train Iter:  4  Loss:  1.303524374961853\n",
      "Train Iter:  5  Loss:  1.2820257902145387\n",
      "Train Iter:  6  Loss:  1.2261834641297658\n",
      "Train Iter:  7  Loss:  1.2504743593079704\n",
      "Train Iter:  8  Loss:  1.2129945382475853\n",
      "Train Iter:  9  Loss:  1.207628693845537\n",
      "Train Iter:  10  Loss:  1.1850919604301453\n",
      "Train Iter:  11  Loss:  1.1840571815317327\n",
      "Train Iter:  12  Loss:  1.2054013510545094\n",
      "Train Iter:  13  Loss:  1.1941400307875414\n",
      "Train Iter:  14  Loss:  1.1815005370548792\n",
      "Train Iter:  15  Loss:  1.1987833658854166\n",
      "Train Iter:  16  Loss:  1.225171111524105\n",
      "Train Iter:  17  Loss:  1.2046653943903305\n",
      "Train Iter:  18  Loss:  1.216558251116011\n",
      "Train Iter:  19  Loss:  1.2221294516011287\n",
      "Train Iter:  20  Loss:  1.2245989978313445\n",
      "Train Iter:  21  Loss:  1.2178282226834978\n",
      "Train Iter:  22  Loss:  1.2177947109395808\n",
      "Train Iter:  23  Loss:  1.2370801231135493\n",
      "Train Iter:  24  Loss:  1.2241276949644089\n",
      "Train Iter:  25  Loss:  1.2161147975921631\n",
      "Train Iter:  26  Loss:  1.2169031913463886\n",
      "Train Iter:  27  Loss:  1.226772215631273\n",
      "Train Iter:  28  Loss:  1.233364645923887\n",
      "Train Iter:  29  Loss:  1.2289688504975418\n",
      "Train Iter:  30  Loss:  1.2269047935803732\n",
      "Train Iter:  31  Loss:  1.2175284354917464\n",
      "Train Iter:  32  Loss:  1.2125499621033669\n",
      "Train Iter:  33  Loss:  1.2258055715849905\n",
      "Train Iter:  34  Loss:  1.2225437269491308\n",
      "Train Iter:  35  Loss:  1.2231653996876308\n",
      "Train Iter:  36  Loss:  1.2159088485770755\n",
      "Train Iter:  37  Loss:  1.219226608405242\n",
      "Train Iter:  38  Loss:  1.217361619597987\n",
      "Train Iter:  39  Loss:  1.2194105845231276\n",
      "Train Iter:  40  Loss:  1.2152138262987138\n",
      "Train Iter:  41  Loss:  1.2184465629298513\n",
      "Train Iter:  42  Loss:  1.2143475271406627\n",
      "Train Iter:  43  Loss:  1.2148701728776443\n",
      "Train Iter:  44  Loss:  1.2150767987424678\n",
      "Train Iter:  45  Loss:  1.215994119644165\n",
      "Train Iter:  46  Loss:  1.2142435312271118\n",
      "Train Iter:  47  Loss:  1.213468130598677\n",
      "Train Iter:  48  Loss:  1.2123317122459412\n",
      "Train Iter:  49  Loss:  1.2125405389435437\n",
      "Train Iter:  50  Loss:  1.2094957208633423\n",
      "Train Iter:  51  Loss:  1.2110803548027487\n",
      "Train Iter:  52  Loss:  1.2077382229841673\n",
      "Train Iter:  53  Loss:  1.2164755699769505\n",
      "Train Iter:  54  Loss:  1.2179098394181993\n",
      "Train Iter:  55  Loss:  1.225726849382574\n",
      "Train Iter:  56  Loss:  1.2254118004015513\n",
      "Train Iter:  57  Loss:  1.230211481713412\n",
      "Train Iter:  58  Loss:  1.2324572349416798\n",
      "Train Iter:  59  Loss:  1.2317121534024256\n",
      "Train Iter:  60  Loss:  1.228910893201828\n",
      "Train Iter:  61  Loss:  1.230677893904389\n",
      "Train Iter:  62  Loss:  1.2238649975868963\n",
      "Train Iter:  63  Loss:  1.2204269662735954\n",
      "Train Iter:  64  Loss:  1.223288144916296\n",
      "Train Iter:  65  Loss:  1.2263344544630783\n",
      "Train Iter:  66  Loss:  1.230981859293851\n",
      "Train Iter:  67  Loss:  1.23408971793616\n",
      "Train Iter:  68  Loss:  1.2358551113044514\n",
      "Train Iter:  69  Loss:  1.240699025167935\n",
      "Train Iter:  70  Loss:  1.2394115839685713\n",
      "Train Iter:  71  Loss:  1.2400930968808457\n",
      "Train Iter:  72  Loss:  1.2394330650568008\n",
      "Train Iter:  73  Loss:  1.2407412888252571\n",
      "Train Iter:  74  Loss:  1.241054518802746\n",
      "Train Iter:  75  Loss:  1.2403994099299114\n",
      "Train Iter:  76  Loss:  1.2367946858468808\n",
      "Train Iter:  77  Loss:  1.239845340128069\n",
      "Train Iter:  78  Loss:  1.2394622342708783\n",
      "Train Iter:  79  Loss:  1.2369118453581123\n",
      "Train Iter:  80  Loss:  1.2376807935535907\n",
      "Train Iter:  81  Loss:  1.238386000380104\n",
      "Train Iter:  82  Loss:  1.2372469298723268\n",
      "Train Iter:  83  Loss:  1.2407695349440517\n",
      "Train Iter:  84  Loss:  1.2427170397270293\n",
      "Train Iter:  85  Loss:  1.2449389422641082\n",
      "Train Iter:  86  Loss:  1.2430008019125738\n",
      "Train Iter:  87  Loss:  1.2414843412651413\n",
      "Train Iter:  88  Loss:  1.2405855093489995\n",
      "Train Iter:  89  Loss:  1.2431254460570518\n",
      "Train Iter:  90  Loss:  1.2417605737845103\n",
      "Train Iter:  91  Loss:  1.2424721409986308\n",
      "Train Iter:  92  Loss:  1.24190217450909\n",
      "Train Iter:  93  Loss:  1.2408973331092505\n",
      "Train Iter:  94  Loss:  1.2415102612464985\n",
      "Train Iter:  95  Loss:  1.2405619539712605\n",
      "Train Iter:  96  Loss:  1.2416563425213099\n",
      "Train Iter:  97  Loss:  1.2408794404305135\n",
      "Train Iter:  98  Loss:  1.242966438434562\n",
      "Train Iter:  99  Loss:  1.2517432714953567\n",
      "Train Iter:  100  Loss:  1.2505732828378677\n",
      "Train Iter:  101  Loss:  1.249078395933208\n",
      "Train Iter:  102  Loss:  1.2457107019190694\n",
      "Train Iter:  103  Loss:  1.247106391828037\n",
      "Train Iter:  104  Loss:  1.246142821243176\n",
      "Train Iter:  105  Loss:  1.2447144048554557\n",
      "Train Iter:  106  Loss:  1.2457620383433576\n",
      "Train Iter:  107  Loss:  1.2500896682249052\n",
      "Train Iter:  108  Loss:  1.250150497864794\n",
      "Train Iter:  109  Loss:  1.250205335813925\n",
      "Train Iter:  110  Loss:  1.2486480046402324\n",
      "Train Iter:  111  Loss:  1.24854794332573\n",
      "Train Iter:  112  Loss:  1.2467000000178814\n",
      "Train Iter:  113  Loss:  1.2454667402579722\n",
      "Train Iter:  114  Loss:  1.2456389984540772\n",
      "Train Iter:  115  Loss:  1.2468939433927122\n",
      "Train Iter:  116  Loss:  1.2443885227729534\n",
      "Train Iter:  117  Loss:  1.2433461470481677\n",
      "Train Iter:  118  Loss:  1.2416145559084617\n",
      "Train Iter:  119  Loss:  1.2401193149951326\n",
      "Train Iter:  120  Loss:  1.2419789244731267\n",
      "Train Iter:  121  Loss:  1.2436304653971648\n",
      "Train Iter:  122  Loss:  1.2430616265437642\n",
      "Train Iter:  123  Loss:  1.2444033554899014\n",
      "Train Iter:  124  Loss:  1.2456076654695696\n",
      "Train Iter:  125  Loss:  1.243650086402893\n",
      "Train Iter:  126  Loss:  1.2444499135017395\n",
      "Train Iter:  127  Loss:  1.24581181158231\n",
      "Train Iter:  128  Loss:  1.2463225442916155\n",
      "Train Iter:  129  Loss:  1.247685425041258\n",
      "Train Iter:  130  Loss:  1.250428401506864\n",
      "Train Iter:  131  Loss:  1.247305078815868\n",
      "Train Iter:  132  Loss:  1.2474904471274577\n",
      "Train Iter:  133  Loss:  1.248878446288575\n",
      "Train Iter:  134  Loss:  1.248895823510725\n",
      "Train Iter:  135  Loss:  1.2483607879391423\n",
      "Train Iter:  136  Loss:  1.2488691846237463\n",
      "Train Iter:  137  Loss:  1.2472808452418251\n",
      "Train Iter:  138  Loss:  1.2470183117666107\n",
      "Train Iter:  139  Loss:  1.2476898050136704\n",
      "Train Iter:  140  Loss:  1.2473820877926691\n",
      "Train Iter:  141  Loss:  1.2470080890554063\n",
      "Train Iter:  142  Loss:  1.2475734424423164\n",
      "Train Iter:  143  Loss:  1.247647560976602\n",
      "Train Iter:  144  Loss:  1.2483855870862801\n",
      "Train Iter:  145  Loss:  1.2491602868869387\n",
      "Train Iter:  146  Loss:  1.2475903985435015\n",
      "Train Iter:  147  Loss:  1.2463150158220409\n",
      "Train Iter:  148  Loss:  1.250196705396111\n",
      "Train Iter:  149  Loss:  1.2486337491329884\n",
      "Train Iter:  150  Loss:  1.2492887898286185\n",
      "Train Iter:  151  Loss:  1.24588334086715\n",
      "Train Iter:  152  Loss:  1.2458699235790653\n",
      "Train Iter:  153  Loss:  1.2435562205470465\n",
      "Train Iter:  154  Loss:  1.2459344670370027\n",
      "Train Iter:  155  Loss:  1.2463490470763177\n",
      "Train Iter:  156  Loss:  1.2504462592112713\n",
      "Train Iter:  157  Loss:  1.2500099908014772\n",
      "Train Iter:  158  Loss:  1.2514062908631336\n",
      "Train Iter:  159  Loss:  1.2539093052066348\n",
      "Train Iter:  160  Loss:  1.2551154009997845\n",
      "Train Iter:  161  Loss:  1.2523779661759087\n",
      "Train Iter:  162  Loss:  1.2505130495554135\n",
      "Train Iter:  163  Loss:  1.2506485753264165\n",
      "Train Iter:  164  Loss:  1.2518585869451848\n",
      "Train Iter:  165  Loss:  1.2534589637409557\n",
      "Train Iter:  166  Loss:  1.254506921193686\n",
      "Train Iter:  167  Loss:  1.254488326832206\n",
      "Train Iter:  168  Loss:  1.2550174643596013\n",
      "Train Iter:  169  Loss:  1.2555077992952788\n",
      "Train Iter:  170  Loss:  1.2555275860954733\n",
      "Train Iter:  171  Loss:  1.2539770230215195\n",
      "Train Iter:  172  Loss:  1.2555341481469398\n",
      "Train Iter:  173  Loss:  1.2553120193453882\n",
      "Train Iter:  174  Loss:  1.2539029035760068\n",
      "Train Iter:  175  Loss:  1.253812383583614\n",
      "Train Iter:  176  Loss:  1.2549051903188229\n",
      "Train Iter:  177  Loss:  1.2562624643751457\n",
      "Train Iter:  178  Loss:  1.2566205440612321\n",
      "Train Iter:  179  Loss:  1.2558452653485304\n",
      "Train Iter:  180  Loss:  1.255313977599144\n",
      "Train Iter:  181  Loss:  1.255172625760347\n",
      "Train Iter:  182  Loss:  1.2545684657909058\n",
      "Train Iter:  183  Loss:  1.2546845006812466\n",
      "Train Iter:  184  Loss:  1.254606545943281\n",
      "Train Iter:  185  Loss:  1.2534207063752252\n",
      "Train Iter:  186  Loss:  1.2530241874597405\n",
      "Val Iter: Loss:  1.0787428617477417\n",
      "Epoch 5, Mean Train Loss: 1.2530241874597405, Val Loss (1 sample): 1.0787428617477417\n",
      "Train Iter:  1  Loss:  1.465148687362671\n",
      "Train Iter:  2  Loss:  1.3710927367210388\n",
      "Train Iter:  3  Loss:  1.2629154125849407\n",
      "Train Iter:  4  Loss:  1.2490977048873901\n",
      "Train Iter:  5  Loss:  1.2188266515731812\n",
      "Train Iter:  6  Loss:  1.196391264597575\n",
      "Train Iter:  7  Loss:  1.2467784881591797\n",
      "Train Iter:  8  Loss:  1.2015494629740715\n",
      "Train Iter:  9  Loss:  1.2035559879408941\n",
      "Train Iter:  10  Loss:  1.215373808145523\n",
      "Train Iter:  11  Loss:  1.2100549231876025\n",
      "Train Iter:  12  Loss:  1.1894569595654805\n",
      "Train Iter:  13  Loss:  1.2293214247776911\n",
      "Train Iter:  14  Loss:  1.2341993025371008\n",
      "Train Iter:  15  Loss:  1.228967507680257\n",
      "Train Iter:  16  Loss:  1.230131596326828\n",
      "Train Iter:  17  Loss:  1.2419220209121704\n",
      "Train Iter:  18  Loss:  1.2467958529790242\n",
      "Train Iter:  19  Loss:  1.237893311600936\n",
      "Train Iter:  20  Loss:  1.2620224952697754\n",
      "Train Iter:  21  Loss:  1.2562909126281738\n",
      "Train Iter:  22  Loss:  1.2425072382796893\n",
      "Train Iter:  23  Loss:  1.2468215719513271\n",
      "Train Iter:  24  Loss:  1.2625220740834873\n",
      "Train Iter:  25  Loss:  1.2602420258522034\n",
      "Train Iter:  26  Loss:  1.268620809683433\n",
      "Train Iter:  27  Loss:  1.275171079017498\n",
      "Train Iter:  28  Loss:  1.276567972132138\n",
      "Train Iter:  29  Loss:  1.2641930333499252\n",
      "Train Iter:  30  Loss:  1.2721851070721943\n",
      "Train Iter:  31  Loss:  1.2777558911231257\n",
      "Train Iter:  32  Loss:  1.276244018226862\n",
      "Train Iter:  33  Loss:  1.2693029461484966\n",
      "Train Iter:  34  Loss:  1.265705911552205\n",
      "Train Iter:  35  Loss:  1.260798566681998\n",
      "Train Iter:  36  Loss:  1.2520484179258347\n",
      "Train Iter:  37  Loss:  1.261416858917958\n",
      "Train Iter:  38  Loss:  1.2580787868876206\n",
      "Train Iter:  39  Loss:  1.2591734375709143\n",
      "Train Iter:  40  Loss:  1.2634357556700706\n",
      "Train Iter:  41  Loss:  1.259647216738724\n",
      "Train Iter:  42  Loss:  1.262422841219675\n",
      "Train Iter:  43  Loss:  1.2660693969837455\n",
      "Train Iter:  44  Loss:  1.2636654336344113\n",
      "Train Iter:  45  Loss:  1.2690216872427198\n",
      "Train Iter:  46  Loss:  1.2721828364807626\n",
      "Train Iter:  47  Loss:  1.269408128363021\n",
      "Train Iter:  48  Loss:  1.2723224374155204\n",
      "Train Iter:  49  Loss:  1.2682373438562666\n",
      "Train Iter:  50  Loss:  1.2623915457725525\n",
      "Train Iter:  51  Loss:  1.2582994185241998\n",
      "Train Iter:  52  Loss:  1.2508439261179705\n",
      "Train Iter:  53  Loss:  1.2466047057565652\n",
      "Train Iter:  54  Loss:  1.251503986340982\n",
      "Train Iter:  55  Loss:  1.2542852770198476\n",
      "Train Iter:  56  Loss:  1.251818495137351\n",
      "Train Iter:  57  Loss:  1.2490448449787341\n",
      "Train Iter:  58  Loss:  1.2503600202757736\n",
      "Train Iter:  59  Loss:  1.2500442731178414\n",
      "Train Iter:  60  Loss:  1.2515302300453186\n",
      "Train Iter:  61  Loss:  1.2465419495692018\n",
      "Train Iter:  62  Loss:  1.243581783386969\n",
      "Train Iter:  63  Loss:  1.2487453543950642\n",
      "Train Iter:  64  Loss:  1.2499966695904732\n",
      "Train Iter:  65  Loss:  1.2492496618857751\n",
      "Train Iter:  66  Loss:  1.255528242299051\n",
      "Train Iter:  67  Loss:  1.2552167515256512\n",
      "Train Iter:  68  Loss:  1.2582949207109564\n",
      "Train Iter:  69  Loss:  1.2578068073245063\n",
      "Train Iter:  70  Loss:  1.2568633845874242\n",
      "Train Iter:  71  Loss:  1.254867669562219\n",
      "Train Iter:  72  Loss:  1.2554984390735626\n",
      "Train Iter:  73  Loss:  1.2570939259986356\n",
      "Train Iter:  74  Loss:  1.2580458312421232\n",
      "Train Iter:  75  Loss:  1.2574953746795654\n",
      "Train Iter:  76  Loss:  1.2649460152575844\n",
      "Train Iter:  77  Loss:  1.2619651091563238\n",
      "Train Iter:  78  Loss:  1.26227389390652\n",
      "Train Iter:  79  Loss:  1.265773229961154\n",
      "Train Iter:  80  Loss:  1.2683957830071448\n",
      "Train Iter:  81  Loss:  1.2714828576570676\n",
      "Train Iter:  82  Loss:  1.2712218368925698\n",
      "Train Iter:  83  Loss:  1.2693790139922176\n",
      "Train Iter:  84  Loss:  1.268196858110882\n",
      "Train Iter:  85  Loss:  1.2718313665951\n",
      "Train Iter:  86  Loss:  1.2737594177556593\n",
      "Train Iter:  87  Loss:  1.2735943081735195\n",
      "Train Iter:  88  Loss:  1.275414450602098\n",
      "Train Iter:  89  Loss:  1.2732746346612995\n",
      "Train Iter:  90  Loss:  1.2746836185455321\n",
      "Train Iter:  91  Loss:  1.2734227547278771\n",
      "Train Iter:  92  Loss:  1.2765569660974585\n",
      "Train Iter:  93  Loss:  1.275803255778487\n",
      "Train Iter:  94  Loss:  1.2768003243081114\n",
      "Train Iter:  95  Loss:  1.2797126242988988\n",
      "Train Iter:  96  Loss:  1.2804333567619324\n",
      "Train Iter:  97  Loss:  1.2815759845615662\n",
      "Train Iter:  98  Loss:  1.2823857579912459\n",
      "Train Iter:  99  Loss:  1.284511067650535\n",
      "Train Iter:  100  Loss:  1.2840890276432038\n",
      "Train Iter:  101  Loss:  1.2859219595937446\n",
      "Train Iter:  102  Loss:  1.2888520874229132\n",
      "Train Iter:  103  Loss:  1.2867758493979\n",
      "Train Iter:  104  Loss:  1.287907681786097\n",
      "Train Iter:  105  Loss:  1.2886091448011852\n",
      "Train Iter:  106  Loss:  1.2903252221503347\n",
      "Train Iter:  107  Loss:  1.2880715109477534\n",
      "Train Iter:  108  Loss:  1.2867891843672152\n",
      "Train Iter:  109  Loss:  1.288121318598406\n",
      "Train Iter:  110  Loss:  1.286301362514496\n",
      "Train Iter:  111  Loss:  1.2865380070230983\n",
      "Train Iter:  112  Loss:  1.287380467568125\n",
      "Train Iter:  113  Loss:  1.2851196379788155\n",
      "Train Iter:  114  Loss:  1.284195836175952\n",
      "Train Iter:  115  Loss:  1.286529164728911\n",
      "Train Iter:  116  Loss:  1.2854682803153992\n",
      "Train Iter:  117  Loss:  1.2878392333658333\n",
      "Train Iter:  118  Loss:  1.286346434536627\n",
      "Train Iter:  119  Loss:  1.2873706286694824\n",
      "Train Iter:  120  Loss:  1.2881818602482478\n",
      "Train Iter:  121  Loss:  1.2896282682734088\n",
      "Train Iter:  122  Loss:  1.2898138458611534\n",
      "Train Iter:  123  Loss:  1.2889156225250988\n",
      "Train Iter:  124  Loss:  1.2859237285390976\n",
      "Train Iter:  125  Loss:  1.2851222214698792\n",
      "Train Iter:  126  Loss:  1.2838649886941154\n",
      "Train Iter:  127  Loss:  1.2846881597060857\n",
      "Train Iter:  128  Loss:  1.2845687777735293\n",
      "Train Iter:  129  Loss:  1.2804718779963116\n",
      "Train Iter:  130  Loss:  1.280230952684696\n",
      "Train Iter:  131  Loss:  1.2802962524290302\n",
      "Train Iter:  132  Loss:  1.2816907307415297\n",
      "Train Iter:  133  Loss:  1.2816233791803058\n",
      "Train Iter:  134  Loss:  1.2846388803489173\n",
      "Train Iter:  135  Loss:  1.2837834751164472\n",
      "Train Iter:  136  Loss:  1.2828806546681069\n",
      "Train Iter:  137  Loss:  1.2822371986660643\n",
      "Train Iter:  138  Loss:  1.2813130906526593\n",
      "Train Iter:  139  Loss:  1.2797384858131409\n",
      "Train Iter:  140  Loss:  1.2792042974914823\n",
      "Train Iter:  141  Loss:  1.2810376624689035\n",
      "Train Iter:  142  Loss:  1.2790236792094272\n",
      "Train Iter:  143  Loss:  1.282048145374218\n",
      "Train Iter:  144  Loss:  1.2851767283346918\n",
      "Train Iter:  145  Loss:  1.285699910130994\n",
      "Train Iter:  146  Loss:  1.2868205235429007\n",
      "Train Iter:  147  Loss:  1.2845056547599585\n",
      "Train Iter:  148  Loss:  1.2854324800742638\n",
      "Train Iter:  149  Loss:  1.2870731253751972\n",
      "Train Iter:  150  Loss:  1.289110252459844\n",
      "Train Iter:  151  Loss:  1.2888402500689424\n",
      "Train Iter:  152  Loss:  1.2891873164396537\n",
      "Train Iter:  153  Loss:  1.2874482437676074\n",
      "Train Iter:  154  Loss:  1.2864325143300093\n",
      "Train Iter:  155  Loss:  1.2873479693166672\n",
      "Train Iter:  156  Loss:  1.287579290759869\n",
      "Train Iter:  157  Loss:  1.2892562481248455\n",
      "Train Iter:  158  Loss:  1.2889207892025574\n",
      "Train Iter:  159  Loss:  1.2876165036885243\n",
      "Train Iter:  160  Loss:  1.2877734411507844\n",
      "Train Iter:  161  Loss:  1.2884284172739302\n",
      "Train Iter:  162  Loss:  1.2893950471907487\n",
      "Train Iter:  163  Loss:  1.289308234592157\n",
      "Train Iter:  164  Loss:  1.288390519778903\n",
      "Train Iter:  165  Loss:  1.2888076410149083\n",
      "Train Iter:  166  Loss:  1.289253354431635\n",
      "Train Iter:  167  Loss:  1.2876852625858284\n",
      "Train Iter:  168  Loss:  1.286597717730772\n",
      "Train Iter:  169  Loss:  1.2882658133845357\n",
      "Train Iter:  170  Loss:  1.2861148473094492\n",
      "Train Iter:  171  Loss:  1.2865955240545217\n",
      "Train Iter:  172  Loss:  1.2869948616554572\n",
      "Train Iter:  173  Loss:  1.2882716576488031\n",
      "Train Iter:  174  Loss:  1.2884661229862564\n",
      "Train Iter:  175  Loss:  1.289278987135206\n",
      "Train Iter:  176  Loss:  1.2886667336252602\n",
      "Train Iter:  177  Loss:  1.2883208878969743\n",
      "Train Iter:  178  Loss:  1.286354374349787\n",
      "Train Iter:  179  Loss:  1.2859034171983517\n",
      "Train Iter:  180  Loss:  1.2856589410040113\n",
      "Train Iter:  181  Loss:  1.285269519242134\n",
      "Train Iter:  182  Loss:  1.2858988917790926\n",
      "Train Iter:  183  Loss:  1.2851105511514216\n",
      "Train Iter:  184  Loss:  1.2854202933933423\n",
      "Train Iter:  185  Loss:  1.2846965293626527\n",
      "Train Iter:  186  Loss:  1.2818153462102335\n",
      "Val Iter: Loss:  1.10358726978302\n",
      "Epoch 6, Mean Train Loss: 1.2818153462102335, Val Loss (1 sample): 1.10358726978302\n",
      "Train Iter:  1  Loss:  1.2257317304611206\n",
      "Train Iter:  2  Loss:  1.1377676725387573\n",
      "Train Iter:  3  Loss:  1.2080223957697551\n",
      "Train Iter:  4  Loss:  1.155284196138382\n",
      "Train Iter:  5  Loss:  1.2230380773544312\n",
      "Train Iter:  6  Loss:  1.2232044339179993\n",
      "Train Iter:  7  Loss:  1.2619784218924386\n",
      "Train Iter:  8  Loss:  1.3198463916778564\n",
      "Train Iter:  9  Loss:  1.3300864431593153\n",
      "Train Iter:  10  Loss:  1.33746258020401\n",
      "Train Iter:  11  Loss:  1.329825152050365\n",
      "Train Iter:  12  Loss:  1.3229549527168274\n",
      "Train Iter:  13  Loss:  1.312271053974445\n",
      "Train Iter:  14  Loss:  1.278210244008473\n",
      "Train Iter:  15  Loss:  1.2780489325523376\n",
      "Train Iter:  16  Loss:  1.2676550932228565\n",
      "Train Iter:  17  Loss:  1.2712027009795694\n",
      "Train Iter:  18  Loss:  1.2638641960091062\n",
      "Train Iter:  19  Loss:  1.276484975689336\n",
      "Train Iter:  20  Loss:  1.2817566961050033\n",
      "Train Iter:  21  Loss:  1.2777395674160548\n",
      "Train Iter:  22  Loss:  1.2776706245812504\n",
      "Train Iter:  23  Loss:  1.2852192572925403\n",
      "Train Iter:  24  Loss:  1.2861339872082074\n",
      "Train Iter:  25  Loss:  1.2906097817420958\n",
      "Train Iter:  26  Loss:  1.2842340354736035\n",
      "Train Iter:  27  Loss:  1.2717349838327479\n",
      "Train Iter:  28  Loss:  1.278247697012765\n",
      "Train Iter:  29  Loss:  1.2906152873203671\n",
      "Train Iter:  30  Loss:  1.281031342347463\n",
      "Train Iter:  31  Loss:  1.2824254420495802\n",
      "Train Iter:  32  Loss:  1.2757431790232658\n",
      "Train Iter:  33  Loss:  1.2659337502537351\n",
      "Train Iter:  34  Loss:  1.2684343965614544\n",
      "Train Iter:  35  Loss:  1.2638836605208261\n",
      "Train Iter:  36  Loss:  1.2631750918096967\n",
      "Train Iter:  37  Loss:  1.2572465990040753\n",
      "Train Iter:  38  Loss:  1.271526281770907\n",
      "Train Iter:  39  Loss:  1.2717242011657128\n",
      "Train Iter:  40  Loss:  1.272482757270336\n",
      "Train Iter:  41  Loss:  1.2700895638000675\n",
      "Train Iter:  42  Loss:  1.2696509914738792\n",
      "Train Iter:  43  Loss:  1.2701565761898839\n",
      "Train Iter:  44  Loss:  1.264515439217741\n",
      "Train Iter:  45  Loss:  1.265795812341902\n",
      "Train Iter:  46  Loss:  1.2707672028437904\n",
      "Train Iter:  47  Loss:  1.27550571142359\n",
      "Train Iter:  48  Loss:  1.2687546573579311\n",
      "Train Iter:  49  Loss:  1.2690332458943736\n",
      "Train Iter:  50  Loss:  1.2681035149097442\n",
      "Train Iter:  51  Loss:  1.271175710593953\n",
      "Train Iter:  52  Loss:  1.2662171526597097\n",
      "Train Iter:  53  Loss:  1.2608423300509184\n",
      "Train Iter:  54  Loss:  1.2599350765899375\n",
      "Train Iter:  55  Loss:  1.2563014052130959\n",
      "Train Iter:  56  Loss:  1.2595509865454264\n",
      "Train Iter:  57  Loss:  1.2560522974583141\n",
      "Train Iter:  58  Loss:  1.258401554206322\n",
      "Train Iter:  59  Loss:  1.2593982441950653\n",
      "Train Iter:  60  Loss:  1.2616107801596324\n",
      "Train Iter:  61  Loss:  1.261741081222159\n",
      "Train Iter:  62  Loss:  1.2640373379953447\n",
      "Train Iter:  63  Loss:  1.2649444663335407\n",
      "Train Iter:  64  Loss:  1.2640266586095095\n",
      "Train Iter:  65  Loss:  1.2698170845325176\n",
      "Train Iter:  66  Loss:  1.26861838499705\n",
      "Train Iter:  67  Loss:  1.2661787467216379\n",
      "Train Iter:  68  Loss:  1.2632444991784937\n",
      "Train Iter:  69  Loss:  1.2637631409410117\n",
      "Train Iter:  70  Loss:  1.2673551610537939\n",
      "Train Iter:  71  Loss:  1.2616531009405432\n",
      "Train Iter:  72  Loss:  1.2634047054582171\n",
      "Train Iter:  73  Loss:  1.2635564052895323\n",
      "Train Iter:  74  Loss:  1.266949568245862\n",
      "Train Iter:  75  Loss:  1.2686695957183838\n",
      "Train Iter:  76  Loss:  1.268613537675456\n",
      "Train Iter:  77  Loss:  1.2656437576591195\n",
      "Train Iter:  78  Loss:  1.2652087012926738\n",
      "Train Iter:  79  Loss:  1.2602279118344755\n",
      "Train Iter:  80  Loss:  1.2639850847423078\n",
      "Train Iter:  81  Loss:  1.2659816602129994\n",
      "Train Iter:  82  Loss:  1.26431672965608\n",
      "Train Iter:  83  Loss:  1.2699649973088\n",
      "Train Iter:  84  Loss:  1.2684085972252346\n",
      "Train Iter:  85  Loss:  1.2665719992974225\n",
      "Train Iter:  86  Loss:  1.2687485723994498\n",
      "Train Iter:  87  Loss:  1.264877005555164\n",
      "Train Iter:  88  Loss:  1.2636140584945679\n",
      "Train Iter:  89  Loss:  1.2636384682708912\n",
      "Train Iter:  90  Loss:  1.2636107563972474\n",
      "Train Iter:  91  Loss:  1.2635664573082557\n",
      "Train Iter:  92  Loss:  1.2663733946240467\n",
      "Train Iter:  93  Loss:  1.270505093759106\n",
      "Train Iter:  94  Loss:  1.270682154817784\n",
      "Train Iter:  95  Loss:  1.2715110628228439\n",
      "Train Iter:  96  Loss:  1.2708702261249225\n",
      "Train Iter:  97  Loss:  1.2687494521288527\n",
      "Train Iter:  98  Loss:  1.2673304652681157\n",
      "Train Iter:  99  Loss:  1.2687046720524027\n",
      "Train Iter:  100  Loss:  1.2693599474430084\n",
      "Train Iter:  101  Loss:  1.2688239140085655\n",
      "Train Iter:  102  Loss:  1.2744859059651692\n",
      "Train Iter:  103  Loss:  1.2734030543021786\n",
      "Train Iter:  104  Loss:  1.2724277938787754\n",
      "Train Iter:  105  Loss:  1.2739748591468447\n",
      "Train Iter:  106  Loss:  1.272426483766088\n",
      "Train Iter:  107  Loss:  1.2737292441252237\n",
      "Train Iter:  108  Loss:  1.2726496193144057\n",
      "Train Iter:  109  Loss:  1.271681076889738\n",
      "Train Iter:  110  Loss:  1.2661284907297654\n",
      "Train Iter:  111  Loss:  1.2649689757072173\n",
      "Train Iter:  112  Loss:  1.2628537943320615\n",
      "Train Iter:  113  Loss:  1.2645093988528293\n",
      "Train Iter:  114  Loss:  1.2649070217944027\n",
      "Train Iter:  115  Loss:  1.266261699406997\n",
      "Train Iter:  116  Loss:  1.2646583776021827\n",
      "Train Iter:  117  Loss:  1.2657278047667608\n",
      "Train Iter:  118  Loss:  1.2654326593471785\n",
      "Train Iter:  119  Loss:  1.2698628456652665\n",
      "Train Iter:  120  Loss:  1.2711382413903871\n",
      "Train Iter:  121  Loss:  1.2735708237679537\n",
      "Train Iter:  122  Loss:  1.2721797086176325\n",
      "Train Iter:  123  Loss:  1.2715364515296812\n",
      "Train Iter:  124  Loss:  1.2743693739175797\n",
      "Train Iter:  125  Loss:  1.2730906929969787\n",
      "Train Iter:  126  Loss:  1.273428598566661\n",
      "Train Iter:  127  Loss:  1.2750608296844903\n",
      "Train Iter:  128  Loss:  1.2730786218307912\n",
      "Train Iter:  129  Loss:  1.2732044490732888\n",
      "Train Iter:  130  Loss:  1.2738956162562738\n",
      "Train Iter:  131  Loss:  1.2714596736522121\n",
      "Train Iter:  132  Loss:  1.2735544510863044\n",
      "Train Iter:  133  Loss:  1.2726561409190185\n",
      "Train Iter:  134  Loss:  1.2711029057182484\n",
      "Train Iter:  135  Loss:  1.2695322747583742\n",
      "Train Iter:  136  Loss:  1.271268554031849\n",
      "Train Iter:  137  Loss:  1.2735300285972817\n",
      "Train Iter:  138  Loss:  1.271777076565701\n",
      "Train Iter:  139  Loss:  1.2684606270824406\n",
      "Train Iter:  140  Loss:  1.2688137863363538\n",
      "Train Iter:  141  Loss:  1.26815971330548\n",
      "Train Iter:  142  Loss:  1.266935021104947\n",
      "Train Iter:  143  Loss:  1.2670732361453396\n",
      "Train Iter:  144  Loss:  1.2664492477973301\n",
      "Train Iter:  145  Loss:  1.2658758960921188\n",
      "Train Iter:  146  Loss:  1.2644320554929236\n",
      "Train Iter:  147  Loss:  1.2644130113173504\n",
      "Train Iter:  148  Loss:  1.264584654086345\n",
      "Train Iter:  149  Loss:  1.2631965587603167\n",
      "Train Iter:  150  Loss:  1.2629519367218018\n",
      "Train Iter:  151  Loss:  1.260123566681186\n",
      "Train Iter:  152  Loss:  1.2597854564848698\n",
      "Train Iter:  153  Loss:  1.2584584363145765\n",
      "Train Iter:  154  Loss:  1.2564709480706748\n",
      "Train Iter:  155  Loss:  1.2581141725663216\n",
      "Train Iter:  156  Loss:  1.256539876644428\n",
      "Train Iter:  157  Loss:  1.2572959038861997\n",
      "Train Iter:  158  Loss:  1.2576612392558326\n",
      "Train Iter:  159  Loss:  1.2603321607757665\n",
      "Train Iter:  160  Loss:  1.2588265970349313\n",
      "Train Iter:  161  Loss:  1.25938594193192\n",
      "Train Iter:  162  Loss:  1.2607472009128995\n",
      "Train Iter:  163  Loss:  1.2602030243610312\n",
      "Train Iter:  164  Loss:  1.2596226276420965\n",
      "Train Iter:  165  Loss:  1.2607291452812426\n",
      "Train Iter:  166  Loss:  1.2601062691355327\n",
      "Train Iter:  167  Loss:  1.2601757406474587\n",
      "Train Iter:  168  Loss:  1.262263910401435\n",
      "Train Iter:  169  Loss:  1.2620368391804442\n",
      "Train Iter:  170  Loss:  1.2637805440846612\n",
      "Train Iter:  171  Loss:  1.2641677633363602\n",
      "Train Iter:  172  Loss:  1.2631722737190336\n",
      "Train Iter:  173  Loss:  1.2607795923431484\n",
      "Train Iter:  174  Loss:  1.2627291425891307\n",
      "Train Iter:  175  Loss:  1.262478575025286\n",
      "Train Iter:  176  Loss:  1.2600657089867375\n",
      "Train Iter:  177  Loss:  1.2610437388473985\n",
      "Train Iter:  178  Loss:  1.260782585050283\n",
      "Train Iter:  179  Loss:  1.2643857937951328\n",
      "Train Iter:  180  Loss:  1.2637402928537793\n",
      "Train Iter:  181  Loss:  1.2651617925470047\n",
      "Train Iter:  182  Loss:  1.265624701649278\n",
      "Train Iter:  183  Loss:  1.265166378737799\n",
      "Train Iter:  184  Loss:  1.2659343898944233\n",
      "Train Iter:  185  Loss:  1.2664316129040074\n",
      "Train Iter:  186  Loss:  1.265955736239751\n",
      "Val Iter: Loss:  1.2031874656677246\n",
      "Epoch 7, Mean Train Loss: 1.265955736239751, Val Loss (1 sample): 1.2031874656677246\n",
      "Train Iter:  1  Loss:  1.330245852470398\n",
      "Train Iter:  2  Loss:  1.3169475197792053\n",
      "Train Iter:  3  Loss:  1.292731483777364\n",
      "Train Iter:  4  Loss:  1.4391252994537354\n",
      "Train Iter:  5  Loss:  1.4149760723114013\n",
      "Train Iter:  6  Loss:  1.4228006998697917\n",
      "Train Iter:  7  Loss:  1.3844140257154192\n",
      "Train Iter:  8  Loss:  1.3258077502250671\n",
      "Train Iter:  9  Loss:  1.370693975024753\n",
      "Train Iter:  10  Loss:  1.3640300393104554\n",
      "Train Iter:  11  Loss:  1.3558037064292214\n",
      "Train Iter:  12  Loss:  1.327170620361964\n",
      "Train Iter:  13  Loss:  1.3349708593808687\n",
      "Train Iter:  14  Loss:  1.3136619329452515\n",
      "Train Iter:  15  Loss:  1.3083516677220663\n",
      "Train Iter:  16  Loss:  1.3005989342927933\n",
      "Train Iter:  17  Loss:  1.296953509835636\n",
      "Train Iter:  18  Loss:  1.3067048258251615\n",
      "Train Iter:  19  Loss:  1.3150142431259155\n",
      "Train Iter:  20  Loss:  1.304082840681076\n",
      "Train Iter:  21  Loss:  1.303966482480367\n",
      "Train Iter:  22  Loss:  1.30835022167726\n",
      "Train Iter:  23  Loss:  1.3189978547718213\n",
      "Train Iter:  24  Loss:  1.3185383826494217\n",
      "Train Iter:  25  Loss:  1.330992774963379\n",
      "Train Iter:  26  Loss:  1.3336295439646795\n",
      "Train Iter:  27  Loss:  1.3280165725284152\n",
      "Train Iter:  28  Loss:  1.3241141097886222\n",
      "Train Iter:  29  Loss:  1.3272860584587887\n",
      "Train Iter:  30  Loss:  1.3232913454373678\n",
      "Train Iter:  31  Loss:  1.3327460404365294\n",
      "Train Iter:  32  Loss:  1.334902100265026\n",
      "Train Iter:  33  Loss:  1.3356027603149414\n",
      "Train Iter:  34  Loss:  1.3262863018933464\n",
      "Train Iter:  35  Loss:  1.3182453087397985\n",
      "Train Iter:  36  Loss:  1.3254501985179052\n",
      "Train Iter:  37  Loss:  1.3262068516499288\n",
      "Train Iter:  38  Loss:  1.3201271954335665\n",
      "Train Iter:  39  Loss:  1.3181615945620415\n",
      "Train Iter:  40  Loss:  1.3224128425121306\n",
      "Train Iter:  41  Loss:  1.3158092120798623\n",
      "Train Iter:  42  Loss:  1.3104861776034038\n",
      "Train Iter:  43  Loss:  1.3121731059495794\n",
      "Train Iter:  44  Loss:  1.3082310178063132\n",
      "Train Iter:  45  Loss:  1.3245127995808919\n",
      "Train Iter:  46  Loss:  1.3193885373032612\n",
      "Train Iter:  47  Loss:  1.3221340230170717\n",
      "Train Iter:  48  Loss:  1.3239953542749088\n",
      "Train Iter:  49  Loss:  1.3210878810104059\n",
      "Train Iter:  50  Loss:  1.3205141353607177\n",
      "Train Iter:  51  Loss:  1.3110584382917367\n",
      "Train Iter:  52  Loss:  1.3082381108632455\n",
      "Train Iter:  53  Loss:  1.309055507183075\n",
      "Train Iter:  54  Loss:  1.3069884412818484\n",
      "Train Iter:  55  Loss:  1.3066883509809322\n",
      "Train Iter:  56  Loss:  1.3059637110148157\n",
      "Train Iter:  57  Loss:  1.3093081221245884\n",
      "Train Iter:  58  Loss:  1.300491199411195\n",
      "Train Iter:  59  Loss:  1.300950149358329\n",
      "Train Iter:  60  Loss:  1.3022178808848064\n",
      "Train Iter:  61  Loss:  1.2993281903814098\n",
      "Train Iter:  62  Loss:  1.2996739187548239\n",
      "Train Iter:  63  Loss:  1.3041770874507843\n",
      "Train Iter:  64  Loss:  1.303052805364132\n",
      "Train Iter:  65  Loss:  1.3024109271856454\n",
      "Train Iter:  66  Loss:  1.3008950316544734\n",
      "Train Iter:  67  Loss:  1.2965814480141027\n",
      "Train Iter:  68  Loss:  1.2926849337185131\n",
      "Train Iter:  69  Loss:  1.2948226721390435\n",
      "Train Iter:  70  Loss:  1.295265941960471\n",
      "Train Iter:  71  Loss:  1.2922749015646922\n",
      "Train Iter:  72  Loss:  1.2919606450531218\n",
      "Train Iter:  73  Loss:  1.2950485732457409\n",
      "Train Iter:  74  Loss:  1.3018453701122388\n",
      "Train Iter:  75  Loss:  1.3018130938212076\n",
      "Train Iter:  76  Loss:  1.299948336262452\n",
      "Train Iter:  77  Loss:  1.2990512367966887\n",
      "Train Iter:  78  Loss:  1.2987744762347295\n",
      "Train Iter:  79  Loss:  1.2934890434711794\n",
      "Train Iter:  80  Loss:  1.2941771127283572\n",
      "Train Iter:  81  Loss:  1.2978258081424383\n",
      "Train Iter:  82  Loss:  1.302954904189924\n",
      "Train Iter:  83  Loss:  1.2984964718301613\n",
      "Train Iter:  84  Loss:  1.2960904595397769\n",
      "Train Iter:  85  Loss:  1.2963217861512129\n",
      "Train Iter:  86  Loss:  1.294625699520111\n",
      "Train Iter:  87  Loss:  1.2935230649750808\n",
      "Train Iter:  88  Loss:  1.2960603602907874\n",
      "Train Iter:  89  Loss:  1.2994239879458138\n",
      "Train Iter:  90  Loss:  1.2989610367351108\n",
      "Train Iter:  91  Loss:  1.300860740326263\n",
      "Train Iter:  92  Loss:  1.3076580892438474\n",
      "Train Iter:  93  Loss:  1.3061807617064445\n",
      "Train Iter:  94  Loss:  1.3077449253264894\n",
      "Train Iter:  95  Loss:  1.3076459382709704\n",
      "Train Iter:  96  Loss:  1.3065444206198056\n",
      "Train Iter:  97  Loss:  1.3050352949457071\n",
      "Train Iter:  98  Loss:  1.3023015954056565\n",
      "Train Iter:  99  Loss:  1.3014427086319587\n",
      "Train Iter:  100  Loss:  1.3029528844356537\n",
      "Train Iter:  101  Loss:  1.3002732413830143\n",
      "Train Iter:  102  Loss:  1.299209130745308\n",
      "Train Iter:  103  Loss:  1.3040499687194824\n",
      "Train Iter:  104  Loss:  1.302305396932822\n",
      "Train Iter:  105  Loss:  1.3014698028564453\n",
      "Train Iter:  106  Loss:  1.3052572158147704\n",
      "Train Iter:  107  Loss:  1.3045586102476745\n",
      "Train Iter:  108  Loss:  1.302845635899791\n",
      "Train Iter:  109  Loss:  1.3030723180245916\n",
      "Train Iter:  110  Loss:  1.3009798743508079\n",
      "Train Iter:  111  Loss:  1.2994688942625716\n",
      "Train Iter:  112  Loss:  1.3007641243083137\n",
      "Train Iter:  113  Loss:  1.2984885599760883\n",
      "Train Iter:  114  Loss:  1.2979727638395209\n",
      "Train Iter:  115  Loss:  1.2959673612014107\n",
      "Train Iter:  116  Loss:  1.2968640769350117\n",
      "Train Iter:  117  Loss:  1.2991839260117621\n",
      "Train Iter:  118  Loss:  1.3001076649811307\n",
      "Train Iter:  119  Loss:  1.2996568619704045\n",
      "Train Iter:  120  Loss:  1.3029306014378865\n",
      "Train Iter:  121  Loss:  1.3005590852627085\n",
      "Train Iter:  122  Loss:  1.301790292145776\n",
      "Train Iter:  123  Loss:  1.3000233793646339\n",
      "Train Iter:  124  Loss:  1.3000999852534263\n",
      "Train Iter:  125  Loss:  1.298641032218933\n",
      "Train Iter:  126  Loss:  1.2993202985279144\n",
      "Train Iter:  127  Loss:  1.2979225023524967\n",
      "Train Iter:  128  Loss:  1.2968213679268956\n",
      "Train Iter:  129  Loss:  1.2977595791336178\n",
      "Train Iter:  130  Loss:  1.2956720902369572\n",
      "Train Iter:  131  Loss:  1.2923774573639148\n",
      "Train Iter:  132  Loss:  1.290585534139113\n",
      "Train Iter:  133  Loss:  1.2874891085732252\n",
      "Train Iter:  134  Loss:  1.2848177387643216\n",
      "Train Iter:  135  Loss:  1.2826961451106602\n",
      "Train Iter:  136  Loss:  1.2871030380620676\n",
      "Train Iter:  137  Loss:  1.288746690663108\n",
      "Train Iter:  138  Loss:  1.2896199515764264\n",
      "Train Iter:  139  Loss:  1.2876750783954594\n",
      "Train Iter:  140  Loss:  1.2879677930048534\n",
      "Train Iter:  141  Loss:  1.2903822943673913\n",
      "Train Iter:  142  Loss:  1.2918978245325492\n",
      "Train Iter:  143  Loss:  1.2917821669912004\n",
      "Train Iter:  144  Loss:  1.2904443595972326\n",
      "Train Iter:  145  Loss:  1.2913946805329157\n",
      "Train Iter:  146  Loss:  1.2964234592979902\n",
      "Train Iter:  147  Loss:  1.2953327817170799\n",
      "Train Iter:  148  Loss:  1.292661805410643\n",
      "Train Iter:  149  Loss:  1.2924672621208548\n",
      "Train Iter:  150  Loss:  1.294113039970398\n",
      "Train Iter:  151  Loss:  1.2925402604980973\n",
      "Train Iter:  152  Loss:  1.2929870611742924\n",
      "Train Iter:  153  Loss:  1.2927814145493353\n",
      "Train Iter:  154  Loss:  1.2918938692514\n",
      "Train Iter:  155  Loss:  1.2935414137378816\n",
      "Train Iter:  156  Loss:  1.2977964618267157\n",
      "Train Iter:  157  Loss:  1.2963341892145241\n",
      "Train Iter:  158  Loss:  1.295584603200985\n",
      "Train Iter:  159  Loss:  1.2947218470603414\n",
      "Train Iter:  160  Loss:  1.2932574547827245\n",
      "Train Iter:  161  Loss:  1.2936153611781434\n",
      "Train Iter:  162  Loss:  1.294453767346747\n",
      "Train Iter:  163  Loss:  1.295746282565813\n",
      "Train Iter:  164  Loss:  1.2972457685121677\n",
      "Train Iter:  165  Loss:  1.295388855717399\n",
      "Train Iter:  166  Loss:  1.2940905449620212\n",
      "Train Iter:  167  Loss:  1.2949542338976603\n",
      "Train Iter:  168  Loss:  1.2947774660729228\n",
      "Train Iter:  169  Loss:  1.2948009399972724\n",
      "Train Iter:  170  Loss:  1.294224871256772\n",
      "Train Iter:  171  Loss:  1.292807730666378\n",
      "Train Iter:  172  Loss:  1.2909373107344606\n",
      "Train Iter:  173  Loss:  1.292744735072803\n",
      "Train Iter:  174  Loss:  1.2927498933912693\n",
      "Train Iter:  175  Loss:  1.292667555809021\n",
      "Train Iter:  176  Loss:  1.2934890199791302\n",
      "Train Iter:  177  Loss:  1.290100220569783\n",
      "Train Iter:  178  Loss:  1.291875601819392\n",
      "Train Iter:  179  Loss:  1.292737957818548\n",
      "Train Iter:  180  Loss:  1.29305040538311\n",
      "Train Iter:  181  Loss:  1.2927163664807273\n",
      "Train Iter:  182  Loss:  1.2931423485279083\n",
      "Train Iter:  183  Loss:  1.2921000982894273\n",
      "Train Iter:  184  Loss:  1.2905460482706195\n",
      "Train Iter:  185  Loss:  1.2886658926267882\n",
      "Train Iter:  186  Loss:  1.2871684963985155\n",
      "Val Iter: Loss:  1.3364009857177734\n",
      "Epoch 8, Mean Train Loss: 1.2871684963985155, Val Loss (1 sample): 1.3364009857177734\n",
      "Train Iter:  1  Loss:  0.9249602556228638\n",
      "Train Iter:  2  Loss:  1.3028432726860046\n",
      "Train Iter:  3  Loss:  1.3464539845784504\n",
      "Train Iter:  4  Loss:  1.3378759920597076\n",
      "Train Iter:  5  Loss:  1.2651697754859925\n",
      "Train Iter:  6  Loss:  1.271048774321874\n",
      "Train Iter:  7  Loss:  1.240961730480194\n",
      "Train Iter:  8  Loss:  1.3085337057709694\n",
      "Train Iter:  9  Loss:  1.2754091885354784\n",
      "Train Iter:  10  Loss:  1.2906477868556976\n",
      "Train Iter:  11  Loss:  1.2687675140120767\n",
      "Train Iter:  12  Loss:  1.3183080106973648\n",
      "Train Iter:  13  Loss:  1.3048693537712097\n",
      "Train Iter:  14  Loss:  1.2858414607388633\n",
      "Train Iter:  15  Loss:  1.280460544427236\n",
      "Train Iter:  16  Loss:  1.2771653644740582\n",
      "Train Iter:  17  Loss:  1.2803135864874895\n",
      "Train Iter:  18  Loss:  1.3041516310638852\n",
      "Train Iter:  19  Loss:  1.310297053111227\n",
      "Train Iter:  20  Loss:  1.3114100366830825\n",
      "Train Iter:  21  Loss:  1.2883789028440202\n",
      "Train Iter:  22  Loss:  1.2722823023796082\n",
      "Train Iter:  23  Loss:  1.267857162848763\n",
      "Train Iter:  24  Loss:  1.2699163903792698\n",
      "Train Iter:  25  Loss:  1.2548317837715148\n",
      "Train Iter:  26  Loss:  1.2548820628569677\n",
      "Train Iter:  27  Loss:  1.2666566570599873\n",
      "Train Iter:  28  Loss:  1.2581943422555923\n",
      "Train Iter:  29  Loss:  1.259889514281832\n",
      "Train Iter:  30  Loss:  1.2601060529549917\n",
      "Train Iter:  31  Loss:  1.2504818574074776\n",
      "Train Iter:  32  Loss:  1.2547105234116316\n",
      "Train Iter:  33  Loss:  1.2506148182984553\n",
      "Train Iter:  34  Loss:  1.2405023136559654\n",
      "Train Iter:  35  Loss:  1.2359736493655613\n",
      "Train Iter:  36  Loss:  1.25148909787337\n",
      "Train Iter:  37  Loss:  1.247852684678258\n",
      "Train Iter:  38  Loss:  1.2498139942947186\n",
      "Train Iter:  39  Loss:  1.2572825734431927\n",
      "Train Iter:  40  Loss:  1.2647872671484948\n",
      "Train Iter:  41  Loss:  1.2641590237617493\n",
      "Train Iter:  42  Loss:  1.266259524084273\n",
      "Train Iter:  43  Loss:  1.2689233644064082\n",
      "Train Iter:  44  Loss:  1.2616462842984633\n",
      "Train Iter:  45  Loss:  1.2601245853636\n",
      "Train Iter:  46  Loss:  1.2565396790919097\n",
      "Train Iter:  47  Loss:  1.2604090330448556\n",
      "Train Iter:  48  Loss:  1.2592767377694447\n",
      "Train Iter:  49  Loss:  1.262948294075168\n",
      "Train Iter:  50  Loss:  1.2700100779533385\n",
      "Train Iter:  51  Loss:  1.2699286049487544\n",
      "Train Iter:  52  Loss:  1.264765556042011\n",
      "Train Iter:  53  Loss:  1.2609438918671518\n",
      "Train Iter:  54  Loss:  1.2642633495507416\n",
      "Train Iter:  55  Loss:  1.2595767172900114\n",
      "Train Iter:  56  Loss:  1.2567816163812364\n",
      "Train Iter:  57  Loss:  1.25519075100882\n",
      "Train Iter:  58  Loss:  1.2580167795049733\n",
      "Train Iter:  59  Loss:  1.2577123985452168\n",
      "Train Iter:  60  Loss:  1.2555288036664327\n",
      "Train Iter:  61  Loss:  1.2608249187469482\n",
      "Train Iter:  62  Loss:  1.2694983713088497\n",
      "Train Iter:  63  Loss:  1.2697387858042641\n",
      "Train Iter:  64  Loss:  1.2701510414481163\n",
      "Train Iter:  65  Loss:  1.2710080293508677\n",
      "Train Iter:  66  Loss:  1.2724362178282305\n",
      "Train Iter:  67  Loss:  1.2718346421398334\n",
      "Train Iter:  68  Loss:  1.275738354991464\n",
      "Train Iter:  69  Loss:  1.2738231610560762\n",
      "Train Iter:  70  Loss:  1.2735024060521807\n",
      "Train Iter:  71  Loss:  1.2731900752430232\n",
      "Train Iter:  72  Loss:  1.268676137758626\n",
      "Train Iter:  73  Loss:  1.2708134055137634\n",
      "Train Iter:  74  Loss:  1.267926178268484\n",
      "Train Iter:  75  Loss:  1.2708157658576966\n",
      "Train Iter:  76  Loss:  1.269590960521447\n",
      "Train Iter:  77  Loss:  1.277129632311982\n",
      "Train Iter:  78  Loss:  1.2784491234864943\n",
      "Train Iter:  79  Loss:  1.2749060860163048\n",
      "Train Iter:  80  Loss:  1.280527801811695\n",
      "Train Iter:  81  Loss:  1.2812958484814492\n",
      "Train Iter:  82  Loss:  1.2823120224766615\n",
      "Train Iter:  83  Loss:  1.2767518823405346\n",
      "Train Iter:  84  Loss:  1.2752548562628883\n",
      "Train Iter:  85  Loss:  1.2743339166921728\n",
      "Train Iter:  86  Loss:  1.2760856851588849\n",
      "Train Iter:  87  Loss:  1.2766374178316402\n",
      "Train Iter:  88  Loss:  1.2759458876468919\n",
      "Train Iter:  89  Loss:  1.2754513889216306\n",
      "Train Iter:  90  Loss:  1.2764587183793386\n",
      "Train Iter:  91  Loss:  1.2771612045529124\n",
      "Train Iter:  92  Loss:  1.2780145763055137\n",
      "Train Iter:  93  Loss:  1.2784521432333096\n",
      "Train Iter:  94  Loss:  1.276890662756372\n",
      "Train Iter:  95  Loss:  1.2793057535824022\n",
      "Train Iter:  96  Loss:  1.2837424309303362\n",
      "Train Iter:  97  Loss:  1.279959513969028\n",
      "Train Iter:  98  Loss:  1.279082247189113\n",
      "Train Iter:  99  Loss:  1.2790475216778843\n",
      "Train Iter:  100  Loss:  1.2793257546424865\n",
      "Train Iter:  101  Loss:  1.2811142902563113\n",
      "Train Iter:  102  Loss:  1.2842634191700057\n",
      "Train Iter:  103  Loss:  1.283537994310694\n",
      "Train Iter:  104  Loss:  1.2846783835154314\n",
      "Train Iter:  105  Loss:  1.2890655460811797\n",
      "Train Iter:  106  Loss:  1.2939051524648126\n",
      "Train Iter:  107  Loss:  1.2905686057616617\n",
      "Train Iter:  108  Loss:  1.2933966749244266\n",
      "Train Iter:  109  Loss:  1.2915802056636285\n",
      "Train Iter:  110  Loss:  1.29255713224411\n",
      "Train Iter:  111  Loss:  1.2916035555504464\n",
      "Train Iter:  112  Loss:  1.2883619692708765\n",
      "Train Iter:  113  Loss:  1.287398080382727\n",
      "Train Iter:  114  Loss:  1.2856580484331699\n",
      "Train Iter:  115  Loss:  1.284313366205796\n",
      "Train Iter:  116  Loss:  1.283860234864827\n",
      "Train Iter:  117  Loss:  1.2842893513858828\n",
      "Train Iter:  118  Loss:  1.2853398328110324\n",
      "Train Iter:  119  Loss:  1.2836896095957075\n",
      "Train Iter:  120  Loss:  1.2833669717113176\n",
      "Train Iter:  121  Loss:  1.2874906176377918\n",
      "Train Iter:  122  Loss:  1.2898437747212708\n",
      "Train Iter:  123  Loss:  1.2916673901604443\n",
      "Train Iter:  124  Loss:  1.2889281924693816\n",
      "Train Iter:  125  Loss:  1.291167049407959\n",
      "Train Iter:  126  Loss:  1.2902233165407937\n",
      "Train Iter:  127  Loss:  1.28840179706183\n",
      "Train Iter:  128  Loss:  1.2885284693911672\n",
      "Train Iter:  129  Loss:  1.2883254132529562\n",
      "Train Iter:  130  Loss:  1.2896129140487085\n",
      "Train Iter:  131  Loss:  1.289453505559732\n",
      "Train Iter:  132  Loss:  1.2892109670422294\n",
      "Train Iter:  133  Loss:  1.2900561058431639\n",
      "Train Iter:  134  Loss:  1.2908474642839005\n",
      "Train Iter:  135  Loss:  1.2897692618546663\n",
      "Train Iter:  136  Loss:  1.2903827130794525\n",
      "Train Iter:  137  Loss:  1.2911051128902575\n",
      "Train Iter:  138  Loss:  1.2909113203269849\n",
      "Train Iter:  139  Loss:  1.2909063306643809\n",
      "Train Iter:  140  Loss:  1.2907585195132665\n",
      "Train Iter:  141  Loss:  1.2888517768670482\n",
      "Train Iter:  142  Loss:  1.2887642752956336\n",
      "Train Iter:  143  Loss:  1.2888528658793523\n",
      "Train Iter:  144  Loss:  1.2874169341391988\n",
      "Train Iter:  145  Loss:  1.2860959661417994\n",
      "Train Iter:  146  Loss:  1.2878181330145222\n",
      "Train Iter:  147  Loss:  1.2882652915253932\n",
      "Train Iter:  148  Loss:  1.2877261662805402\n",
      "Train Iter:  149  Loss:  1.2874529441731088\n",
      "Train Iter:  150  Loss:  1.2868754069010417\n",
      "Train Iter:  151  Loss:  1.286523372132257\n",
      "Train Iter:  152  Loss:  1.286463890420763\n",
      "Train Iter:  153  Loss:  1.2863952536988104\n",
      "Train Iter:  154  Loss:  1.2850794172906257\n",
      "Train Iter:  155  Loss:  1.284283552631255\n",
      "Train Iter:  156  Loss:  1.2841834861498613\n",
      "Train Iter:  157  Loss:  1.2851668520338217\n",
      "Train Iter:  158  Loss:  1.2848037677475168\n",
      "Train Iter:  159  Loss:  1.284806395476719\n",
      "Train Iter:  160  Loss:  1.2848121717572212\n",
      "Train Iter:  161  Loss:  1.2847060276114421\n",
      "Train Iter:  162  Loss:  1.284487122370873\n",
      "Train Iter:  163  Loss:  1.2850590432348428\n",
      "Train Iter:  164  Loss:  1.2851607225290158\n",
      "Train Iter:  165  Loss:  1.284004799524943\n",
      "Train Iter:  166  Loss:  1.2824544899434929\n",
      "Train Iter:  167  Loss:  1.280824160861398\n",
      "Train Iter:  168  Loss:  1.2793761470488139\n",
      "Train Iter:  169  Loss:  1.2819651026697554\n",
      "Train Iter:  170  Loss:  1.281870826552896\n",
      "Train Iter:  171  Loss:  1.2816439448741443\n",
      "Train Iter:  172  Loss:  1.2809118843355844\n",
      "Train Iter:  173  Loss:  1.2825241757266095\n",
      "Train Iter:  174  Loss:  1.2824367880821228\n",
      "Train Iter:  175  Loss:  1.2823898097446986\n",
      "Train Iter:  176  Loss:  1.283008499578996\n",
      "Train Iter:  177  Loss:  1.2833689127938221\n",
      "Train Iter:  178  Loss:  1.2837533944108512\n",
      "Train Iter:  179  Loss:  1.2809859098002898\n",
      "Train Iter:  180  Loss:  1.2791903518968157\n",
      "Train Iter:  181  Loss:  1.2801661415653334\n",
      "Train Iter:  182  Loss:  1.2813553584145976\n",
      "Train Iter:  183  Loss:  1.2805945941659271\n",
      "Train Iter:  184  Loss:  1.282375214216502\n",
      "Train Iter:  185  Loss:  1.2833168883581418\n",
      "Train Iter:  186  Loss:  1.2847732097230933\n",
      "Val Iter: Loss:  1.1332918405532837\n",
      "Epoch 9, Mean Train Loss: 1.2847732097230933, Val Loss (1 sample): 1.1332918405532837\n",
      "Train Iter:  1  Loss:  1.124287724494934\n",
      "Train Iter:  2  Loss:  1.023837834596634\n",
      "Train Iter:  3  Loss:  1.2029974659283955\n",
      "Train Iter:  4  Loss:  1.2139332741498947\n",
      "Train Iter:  5  Loss:  1.2419474720954895\n",
      "Train Iter:  6  Loss:  1.2362426221370697\n",
      "Train Iter:  7  Loss:  1.2472301636423384\n",
      "Train Iter:  8  Loss:  1.203284539282322\n",
      "Train Iter:  9  Loss:  1.194518850909339\n",
      "Train Iter:  10  Loss:  1.1720895648002625\n",
      "Train Iter:  11  Loss:  1.1634963100606746\n",
      "Train Iter:  12  Loss:  1.1402019262313843\n",
      "Train Iter:  13  Loss:  1.1390983966680674\n",
      "Train Iter:  14  Loss:  1.1464331235204424\n",
      "Train Iter:  15  Loss:  1.1681809266408285\n",
      "Train Iter:  16  Loss:  1.153557363897562\n",
      "Train Iter:  17  Loss:  1.1581570015234106\n",
      "Train Iter:  18  Loss:  1.1669995685418446\n",
      "Train Iter:  19  Loss:  1.1818909927418357\n",
      "Train Iter:  20  Loss:  1.201822456717491\n",
      "Train Iter:  21  Loss:  1.2079387307167053\n",
      "Train Iter:  22  Loss:  1.2120234830812975\n",
      "Train Iter:  23  Loss:  1.2118040504662886\n",
      "Train Iter:  24  Loss:  1.2152634834249814\n",
      "Train Iter:  25  Loss:  1.2255081725120545\n",
      "Train Iter:  26  Loss:  1.2241550981998444\n",
      "Train Iter:  27  Loss:  1.2383938122678686\n",
      "Train Iter:  28  Loss:  1.2479408915553774\n",
      "Train Iter:  29  Loss:  1.2519136646698261\n",
      "Train Iter:  30  Loss:  1.2547755539417267\n",
      "Train Iter:  31  Loss:  1.2439093589782715\n",
      "Train Iter:  32  Loss:  1.251504722982645\n",
      "Train Iter:  33  Loss:  1.2438760562376543\n",
      "Train Iter:  34  Loss:  1.2340498110827278\n",
      "Train Iter:  35  Loss:  1.2348006963729858\n",
      "Train Iter:  36  Loss:  1.2313165995809767\n",
      "Train Iter:  37  Loss:  1.2394000259605613\n",
      "Train Iter:  38  Loss:  1.2428171697415804\n",
      "Train Iter:  39  Loss:  1.2361838894012647\n",
      "Train Iter:  40  Loss:  1.2324911639094354\n",
      "Train Iter:  41  Loss:  1.226527228588011\n",
      "Train Iter:  42  Loss:  1.233855820837475\n",
      "Train Iter:  43  Loss:  1.2440384986788728\n",
      "Train Iter:  44  Loss:  1.2396567599339918\n",
      "Train Iter:  45  Loss:  1.2444684849845038\n",
      "Train Iter:  46  Loss:  1.2465640591538472\n",
      "Train Iter:  47  Loss:  1.245134140582795\n",
      "Train Iter:  48  Loss:  1.2502054671446483\n",
      "Train Iter:  49  Loss:  1.253494717636887\n",
      "Train Iter:  50  Loss:  1.256570234298706\n",
      "Train Iter:  51  Loss:  1.2527439243653242\n",
      "Train Iter:  52  Loss:  1.2512792921983278\n",
      "Train Iter:  53  Loss:  1.241927962258177\n",
      "Train Iter:  54  Loss:  1.2444726747495156\n",
      "Train Iter:  55  Loss:  1.2394012928009033\n",
      "Train Iter:  56  Loss:  1.2379004721130644\n",
      "Train Iter:  57  Loss:  1.241853722354822\n",
      "Train Iter:  58  Loss:  1.246181290725182\n",
      "Train Iter:  59  Loss:  1.2474955744662528\n",
      "Train Iter:  60  Loss:  1.242809072136879\n",
      "Train Iter:  61  Loss:  1.2424891806039653\n",
      "Train Iter:  62  Loss:  1.2420067931375196\n",
      "Train Iter:  63  Loss:  1.2416793060681177\n",
      "Train Iter:  64  Loss:  1.2452799575403333\n",
      "Train Iter:  65  Loss:  1.2447062886678255\n",
      "Train Iter:  66  Loss:  1.2518676835479159\n",
      "Train Iter:  67  Loss:  1.2571707207765153\n",
      "Train Iter:  68  Loss:  1.2515489125953\n",
      "Train Iter:  69  Loss:  1.2488731802373692\n",
      "Train Iter:  70  Loss:  1.248811321599143\n",
      "Train Iter:  71  Loss:  1.2420768410387173\n",
      "Train Iter:  72  Loss:  1.2452029660344124\n",
      "Train Iter:  73  Loss:  1.2419249774658516\n",
      "Train Iter:  74  Loss:  1.2434252234729561\n",
      "Train Iter:  75  Loss:  1.24777880748113\n",
      "Train Iter:  76  Loss:  1.2514081354203976\n",
      "Train Iter:  77  Loss:  1.2506400339015118\n",
      "Train Iter:  78  Loss:  1.2494263488512773\n",
      "Train Iter:  79  Loss:  1.2492427818382843\n",
      "Train Iter:  80  Loss:  1.2495881460607052\n",
      "Train Iter:  81  Loss:  1.2487735211113353\n",
      "Train Iter:  82  Loss:  1.2451785039610979\n",
      "Train Iter:  83  Loss:  1.241416269038097\n",
      "Train Iter:  84  Loss:  1.243179874760764\n",
      "Train Iter:  85  Loss:  1.2417410093195298\n",
      "Train Iter:  86  Loss:  1.2411447250565817\n",
      "Train Iter:  87  Loss:  1.2424231699143333\n",
      "Train Iter:  88  Loss:  1.2464578964493491\n",
      "Train Iter:  89  Loss:  1.2509725830528173\n",
      "Train Iter:  90  Loss:  1.2492484715249803\n",
      "Train Iter:  91  Loss:  1.2481065752742055\n",
      "Train Iter:  92  Loss:  1.2494109570980072\n",
      "Train Iter:  93  Loss:  1.2476967265529018\n",
      "Train Iter:  94  Loss:  1.2461525430070592\n",
      "Train Iter:  95  Loss:  1.24295839259499\n",
      "Train Iter:  96  Loss:  1.2414018188913662\n",
      "Train Iter:  97  Loss:  1.244866342888665\n",
      "Train Iter:  98  Loss:  1.2410298585891724\n",
      "Train Iter:  99  Loss:  1.24305087990231\n",
      "Train Iter:  100  Loss:  1.2473773908615113\n",
      "Train Iter:  101  Loss:  1.2489288927304862\n",
      "Train Iter:  102  Loss:  1.2494360348757576\n",
      "Train Iter:  103  Loss:  1.2514995984660768\n",
      "Train Iter:  104  Loss:  1.2515124873473094\n",
      "Train Iter:  105  Loss:  1.2538265421277002\n",
      "Train Iter:  106  Loss:  1.2554697922940523\n",
      "Train Iter:  107  Loss:  1.253588255320754\n",
      "Train Iter:  108  Loss:  1.250966183565281\n",
      "Train Iter:  109  Loss:  1.2479771616262034\n",
      "Train Iter:  110  Loss:  1.2436327918009324\n",
      "Train Iter:  111  Loss:  1.2457593663318738\n",
      "Train Iter:  112  Loss:  1.2457614940192019\n",
      "Train Iter:  113  Loss:  1.243017168171638\n",
      "Train Iter:  114  Loss:  1.242248156614471\n",
      "Train Iter:  115  Loss:  1.2429759813391643\n",
      "Train Iter:  116  Loss:  1.2406658216797073\n",
      "Train Iter:  117  Loss:  1.2401745344838526\n",
      "Train Iter:  118  Loss:  1.2428425868689004\n",
      "Train Iter:  119  Loss:  1.2420378487651087\n",
      "Train Iter:  120  Loss:  1.24015022367239\n",
      "Train Iter:  121  Loss:  1.2393011036983206\n",
      "Train Iter:  122  Loss:  1.2383151684628158\n",
      "Train Iter:  123  Loss:  1.2362591546725452\n",
      "Train Iter:  124  Loss:  1.2356887291516028\n",
      "Train Iter:  125  Loss:  1.2345640873908996\n",
      "Train Iter:  126  Loss:  1.2364932937281472\n",
      "Train Iter:  127  Loss:  1.237820809751045\n",
      "Train Iter:  128  Loss:  1.2368774083442986\n",
      "Train Iter:  129  Loss:  1.236980422984722\n",
      "Train Iter:  130  Loss:  1.2354692096893605\n",
      "Train Iter:  131  Loss:  1.2365107040368875\n",
      "Train Iter:  132  Loss:  1.2377410860675755\n",
      "Train Iter:  133  Loss:  1.237129107005614\n",
      "Train Iter:  134  Loss:  1.2372839490869152\n",
      "Train Iter:  135  Loss:  1.236905539918829\n",
      "Train Iter:  136  Loss:  1.235894333352061\n",
      "Train Iter:  137  Loss:  1.2347519141044059\n",
      "Train Iter:  138  Loss:  1.2336152131142823\n",
      "Train Iter:  139  Loss:  1.232528630349276\n",
      "Train Iter:  140  Loss:  1.2316883278744561\n",
      "Train Iter:  141  Loss:  1.2326190188421424\n",
      "Train Iter:  142  Loss:  1.2327944338321686\n",
      "Train Iter:  143  Loss:  1.2333060032004244\n",
      "Train Iter:  144  Loss:  1.233264222327206\n",
      "Train Iter:  145  Loss:  1.2348894953727723\n",
      "Train Iter:  146  Loss:  1.2349387542025683\n",
      "Train Iter:  147  Loss:  1.2350901855903418\n",
      "Train Iter:  148  Loss:  1.2345142900138288\n",
      "Train Iter:  149  Loss:  1.234758036648667\n",
      "Train Iter:  150  Loss:  1.2381367607911429\n",
      "Train Iter:  151  Loss:  1.2372181577398287\n",
      "Train Iter:  152  Loss:  1.2372685635560436\n",
      "Train Iter:  153  Loss:  1.2371928656802458\n",
      "Train Iter:  154  Loss:  1.236918979263925\n",
      "Train Iter:  155  Loss:  1.2370215042944877\n",
      "Train Iter:  156  Loss:  1.236529373587706\n",
      "Train Iter:  157  Loss:  1.237307082695566\n",
      "Train Iter:  158  Loss:  1.2374462323098243\n",
      "Train Iter:  159  Loss:  1.236309419263084\n",
      "Train Iter:  160  Loss:  1.2373871553689242\n",
      "Train Iter:  161  Loss:  1.2362790133642114\n",
      "Train Iter:  162  Loss:  1.2369579095163463\n",
      "Train Iter:  163  Loss:  1.2361549757009636\n",
      "Train Iter:  164  Loss:  1.2350146207140713\n",
      "Train Iter:  165  Loss:  1.234890217853315\n",
      "Train Iter:  166  Loss:  1.233742135475917\n",
      "Train Iter:  167  Loss:  1.2329380116062965\n",
      "Train Iter:  168  Loss:  1.2323592365497635\n",
      "Train Iter:  169  Loss:  1.2319035484240606\n",
      "Train Iter:  170  Loss:  1.2303848652278675\n",
      "Train Iter:  171  Loss:  1.2309889152036075\n",
      "Train Iter:  172  Loss:  1.2300186372080515\n",
      "Train Iter:  173  Loss:  1.2282185154843193\n",
      "Train Iter:  174  Loss:  1.2283876044996853\n",
      "Train Iter:  175  Loss:  1.2306642396109444\n",
      "Train Iter:  176  Loss:  1.2316583645614712\n",
      "Train Iter:  177  Loss:  1.230817397435506\n",
      "Train Iter:  178  Loss:  1.2300070490729942\n",
      "Train Iter:  179  Loss:  1.231579109277139\n",
      "Train Iter:  180  Loss:  1.229653564095497\n",
      "Train Iter:  181  Loss:  1.2278269002450763\n",
      "Train Iter:  182  Loss:  1.2279086866221585\n",
      "Train Iter:  183  Loss:  1.2290124443710828\n",
      "Train Iter:  184  Loss:  1.2313239354154337\n",
      "Train Iter:  185  Loss:  1.2317263686979139\n",
      "Train Iter:  186  Loss:  1.2328249901853583\n",
      "Val Iter: Loss:  1.1285834312438965\n",
      "Epoch 10, Mean Train Loss: 1.2328249901853583, Val Loss (1 sample): 1.1285834312438965\n",
      "Train Iter:  1  Loss:  1.4536371231079102\n",
      "Train Iter:  2  Loss:  1.2182247936725616\n",
      "Train Iter:  3  Loss:  1.1167067289352417\n",
      "Train Iter:  4  Loss:  1.119113951921463\n",
      "Train Iter:  5  Loss:  1.1600660800933837\n",
      "Train Iter:  6  Loss:  1.2022748390833538\n",
      "Train Iter:  7  Loss:  1.1712427735328674\n",
      "Train Iter:  8  Loss:  1.202349029481411\n",
      "Train Iter:  9  Loss:  1.198648300435808\n",
      "Train Iter:  10  Loss:  1.215077954530716\n",
      "Train Iter:  11  Loss:  1.215932017022913\n",
      "Train Iter:  12  Loss:  1.2393253693977992\n",
      "Train Iter:  13  Loss:  1.2377879482049208\n",
      "Train Iter:  14  Loss:  1.2439940869808197\n",
      "Train Iter:  15  Loss:  1.240894329547882\n",
      "Train Iter:  16  Loss:  1.2370046339929104\n",
      "Train Iter:  17  Loss:  1.2231695266330944\n",
      "Train Iter:  18  Loss:  1.2215660711129506\n",
      "Train Iter:  19  Loss:  1.2165543123295433\n",
      "Train Iter:  20  Loss:  1.250826421380043\n",
      "Train Iter:  21  Loss:  1.2517297125997997\n",
      "Train Iter:  22  Loss:  1.2550180527296932\n",
      "Train Iter:  23  Loss:  1.2533728532169177\n",
      "Train Iter:  24  Loss:  1.249176340798537\n",
      "Train Iter:  25  Loss:  1.249346158504486\n",
      "Train Iter:  26  Loss:  1.2603045862454634\n",
      "Train Iter:  27  Loss:  1.2659487790531583\n",
      "Train Iter:  28  Loss:  1.267007127404213\n",
      "Train Iter:  29  Loss:  1.2757983762642433\n",
      "Train Iter:  30  Loss:  1.2723760386308034\n",
      "Train Iter:  31  Loss:  1.2718292570883227\n",
      "Train Iter:  32  Loss:  1.2702486496418715\n",
      "Train Iter:  33  Loss:  1.280815642891508\n",
      "Train Iter:  34  Loss:  1.287031171952977\n",
      "Train Iter:  35  Loss:  1.2854395168168204\n",
      "Train Iter:  36  Loss:  1.2914615985420015\n",
      "Train Iter:  37  Loss:  1.2944917984910913\n",
      "Train Iter:  38  Loss:  1.2970074838713597\n",
      "Train Iter:  39  Loss:  1.29551701209484\n",
      "Train Iter:  40  Loss:  1.2946985080838203\n",
      "Train Iter:  41  Loss:  1.291761635280237\n",
      "Train Iter:  42  Loss:  1.2934563826946985\n",
      "Train Iter:  43  Loss:  1.298192557900451\n",
      "Train Iter:  44  Loss:  1.3034694397991353\n",
      "Train Iter:  45  Loss:  1.2994349148538378\n",
      "Train Iter:  46  Loss:  1.2944590306800345\n",
      "Train Iter:  47  Loss:  1.2876928778404886\n",
      "Train Iter:  48  Loss:  1.2908604977031548\n",
      "Train Iter:  49  Loss:  1.2913904932080482\n",
      "Train Iter:  50  Loss:  1.2926746499538422\n",
      "Train Iter:  51  Loss:  1.2848795465394562\n",
      "Train Iter:  52  Loss:  1.28510654431123\n",
      "Train Iter:  53  Loss:  1.2796684514801457\n",
      "Train Iter:  54  Loss:  1.2836597594949934\n",
      "Train Iter:  55  Loss:  1.2856265490705316\n",
      "Train Iter:  56  Loss:  1.2899434917739458\n",
      "Train Iter:  57  Loss:  1.2829549720412807\n",
      "Train Iter:  58  Loss:  1.281956133143655\n",
      "Train Iter:  59  Loss:  1.280703375905247\n",
      "Train Iter:  60  Loss:  1.2780226876338323\n",
      "Train Iter:  61  Loss:  1.2842005364230422\n",
      "Train Iter:  62  Loss:  1.2840846998076285\n",
      "Train Iter:  63  Loss:  1.2799079087045457\n",
      "Train Iter:  64  Loss:  1.2745597008615732\n",
      "Train Iter:  65  Loss:  1.2709950648821318\n",
      "Train Iter:  66  Loss:  1.2736434683655247\n",
      "Train Iter:  67  Loss:  1.2694368504766207\n",
      "Train Iter:  68  Loss:  1.2682903391473435\n",
      "Train Iter:  69  Loss:  1.2704835680947788\n",
      "Train Iter:  70  Loss:  1.2665546000003816\n",
      "Train Iter:  71  Loss:  1.2615417821306578\n",
      "Train Iter:  72  Loss:  1.2615823770562808\n",
      "Train Iter:  73  Loss:  1.2564670410874772\n",
      "Train Iter:  74  Loss:  1.253223049479562\n",
      "Train Iter:  75  Loss:  1.2478159109751383\n",
      "Train Iter:  76  Loss:  1.2448141935624575\n",
      "Train Iter:  77  Loss:  1.2497855935777937\n",
      "Train Iter:  78  Loss:  1.249703035904811\n",
      "Train Iter:  79  Loss:  1.2492018455191503\n",
      "Train Iter:  80  Loss:  1.2455044627189635\n",
      "Train Iter:  81  Loss:  1.2475502108350212\n",
      "Train Iter:  82  Loss:  1.2470571238820145\n",
      "Train Iter:  83  Loss:  1.2444816755961223\n",
      "Train Iter:  84  Loss:  1.2430113085678645\n",
      "Train Iter:  85  Loss:  1.2427321251700907\n",
      "Train Iter:  86  Loss:  1.2424564777418625\n",
      "Train Iter:  87  Loss:  1.2449971999245129\n",
      "Train Iter:  88  Loss:  1.2428071051836014\n",
      "Train Iter:  89  Loss:  1.2415243360433685\n",
      "Train Iter:  90  Loss:  1.2422743903266058\n",
      "Train Iter:  91  Loss:  1.243364000058436\n",
      "Train Iter:  92  Loss:  1.2474228286224862\n",
      "Train Iter:  93  Loss:  1.248874375897069\n",
      "Train Iter:  94  Loss:  1.249060609239213\n",
      "Train Iter:  95  Loss:  1.2533970870469746\n",
      "Train Iter:  96  Loss:  1.2520035654306412\n",
      "Train Iter:  97  Loss:  1.2522382895971083\n",
      "Train Iter:  98  Loss:  1.2510314133702491\n",
      "Train Iter:  99  Loss:  1.2506413796935418\n",
      "Train Iter:  100  Loss:  1.2506558763980866\n",
      "Train Iter:  101  Loss:  1.2542542927335985\n",
      "Train Iter:  102  Loss:  1.2516129209714777\n",
      "Train Iter:  103  Loss:  1.2580612579595696\n",
      "Train Iter:  104  Loss:  1.2559143322018476\n",
      "Train Iter:  105  Loss:  1.2572042834191095\n",
      "Train Iter:  106  Loss:  1.2572983823857218\n",
      "Train Iter:  107  Loss:  1.257115841468918\n",
      "Train Iter:  108  Loss:  1.2567502111196518\n",
      "Train Iter:  109  Loss:  1.2555419463630115\n",
      "Train Iter:  110  Loss:  1.2561846868558364\n",
      "Train Iter:  111  Loss:  1.2597801314817894\n",
      "Train Iter:  112  Loss:  1.263669774468456\n",
      "Train Iter:  113  Loss:  1.264690536840827\n",
      "Train Iter:  114  Loss:  1.2633405333025414\n",
      "Train Iter:  115  Loss:  1.2605428058168162\n",
      "Train Iter:  116  Loss:  1.259330462278991\n",
      "Train Iter:  117  Loss:  1.2586608478146741\n",
      "Train Iter:  118  Loss:  1.2581408241037595\n",
      "Train Iter:  119  Loss:  1.2586439532392166\n",
      "Train Iter:  120  Loss:  1.2619793916742006\n",
      "Train Iter:  121  Loss:  1.2629553537723446\n",
      "Train Iter:  122  Loss:  1.2608453655829195\n",
      "Train Iter:  123  Loss:  1.2591309552270222\n",
      "Train Iter:  124  Loss:  1.2604475343419659\n",
      "Train Iter:  125  Loss:  1.2598960070610046\n",
      "Train Iter:  126  Loss:  1.258372781295625\n",
      "Train Iter:  127  Loss:  1.2594551500373\n",
      "Train Iter:  128  Loss:  1.257941056508571\n",
      "Train Iter:  129  Loss:  1.2564591146254724\n",
      "Train Iter:  130  Loss:  1.2604989276482508\n",
      "Train Iter:  131  Loss:  1.2626636524236838\n",
      "Train Iter:  132  Loss:  1.263111183589155\n",
      "Train Iter:  133  Loss:  1.2617267240258985\n",
      "Train Iter:  134  Loss:  1.2606091014484861\n",
      "Train Iter:  135  Loss:  1.2635688600716768\n",
      "Train Iter:  136  Loss:  1.2631626782171868\n",
      "Train Iter:  137  Loss:  1.261140046763594\n",
      "Train Iter:  138  Loss:  1.2611728928227355\n",
      "Train Iter:  139  Loss:  1.2629156485735942\n",
      "Train Iter:  140  Loss:  1.260191056558064\n",
      "Train Iter:  141  Loss:  1.2607435596750138\n",
      "Train Iter:  142  Loss:  1.2608324834998226\n",
      "Train Iter:  143  Loss:  1.260101693493503\n",
      "Train Iter:  144  Loss:  1.2597166117694643\n",
      "Train Iter:  145  Loss:  1.2619762420654297\n",
      "Train Iter:  146  Loss:  1.2621932829896065\n",
      "Train Iter:  147  Loss:  1.2612767689893034\n",
      "Train Iter:  148  Loss:  1.2610544465683602\n",
      "Train Iter:  149  Loss:  1.2602337654805023\n",
      "Train Iter:  150  Loss:  1.2589394680658976\n",
      "Train Iter:  151  Loss:  1.2576073834438197\n",
      "Train Iter:  152  Loss:  1.2588151105140384\n",
      "Train Iter:  153  Loss:  1.2604242524290397\n",
      "Train Iter:  154  Loss:  1.2616372681283332\n",
      "Train Iter:  155  Loss:  1.2632177845124275\n",
      "Train Iter:  156  Loss:  1.2620970408121746\n",
      "Train Iter:  157  Loss:  1.2630669038007214\n",
      "Train Iter:  158  Loss:  1.263725143444689\n",
      "Train Iter:  159  Loss:  1.2639392164518248\n",
      "Train Iter:  160  Loss:  1.2632335759699345\n",
      "Train Iter:  161  Loss:  1.2631598385224432\n",
      "Train Iter:  162  Loss:  1.263732255976877\n",
      "Train Iter:  163  Loss:  1.2615081126704537\n",
      "Train Iter:  164  Loss:  1.2616009577745344\n",
      "Train Iter:  165  Loss:  1.2600170413653056\n",
      "Train Iter:  166  Loss:  1.2577033649726086\n",
      "Train Iter:  167  Loss:  1.2570315367447402\n",
      "Train Iter:  168  Loss:  1.2565547451376915\n",
      "Train Iter:  169  Loss:  1.2554215104622248\n",
      "Train Iter:  170  Loss:  1.2557665912544027\n",
      "Train Iter:  171  Loss:  1.2569254249857182\n",
      "Train Iter:  172  Loss:  1.2571530851513841\n",
      "Train Iter:  173  Loss:  1.2572224763776525\n",
      "Train Iter:  174  Loss:  1.2577413049922592\n",
      "Train Iter:  175  Loss:  1.257864555290767\n",
      "Train Iter:  176  Loss:  1.2571168985556473\n",
      "Train Iter:  177  Loss:  1.256818818170472\n",
      "Train Iter:  178  Loss:  1.2580915666028354\n",
      "Train Iter:  179  Loss:  1.2584308469095709\n",
      "Train Iter:  180  Loss:  1.2589122510618633\n",
      "Train Iter:  181  Loss:  1.261009400391447\n",
      "Train Iter:  182  Loss:  1.2605517031727256\n",
      "Train Iter:  183  Loss:  1.260588074316744\n",
      "Train Iter:  184  Loss:  1.262611999783827\n",
      "Train Iter:  185  Loss:  1.2645227087510598\n",
      "Train Iter:  186  Loss:  1.2669839836576933\n",
      "Val Iter: Loss:  1.284637212753296\n",
      "Epoch 11, Mean Train Loss: 1.2669839836576933, Val Loss (1 sample): 1.284637212753296\n",
      "Train Iter:  1  Loss:  1.6634379625320435\n",
      "Train Iter:  2  Loss:  1.4353816509246826\n",
      "Train Iter:  3  Loss:  1.376306414604187\n",
      "Train Iter:  4  Loss:  1.3469050228595734\n",
      "Train Iter:  5  Loss:  1.373178482055664\n",
      "Train Iter:  6  Loss:  1.337150235970815\n",
      "Train Iter:  7  Loss:  1.3459892443248205\n",
      "Train Iter:  8  Loss:  1.3590704202651978\n",
      "Train Iter:  9  Loss:  1.3313110669453938\n",
      "Train Iter:  10  Loss:  1.363100278377533\n",
      "Train Iter:  11  Loss:  1.3329642469232732\n",
      "Train Iter:  12  Loss:  1.336286296447118\n",
      "Train Iter:  13  Loss:  1.3293203849058886\n",
      "Train Iter:  14  Loss:  1.3231651101793562\n",
      "Train Iter:  15  Loss:  1.314679225285848\n",
      "Train Iter:  16  Loss:  1.3013405501842499\n",
      "Train Iter:  17  Loss:  1.3054039688671337\n",
      "Train Iter:  18  Loss:  1.319319897227817\n",
      "Train Iter:  19  Loss:  1.3166291650972868\n",
      "Train Iter:  20  Loss:  1.314605462551117\n",
      "Train Iter:  21  Loss:  1.3035819076356434\n",
      "Train Iter:  22  Loss:  1.3229095123030923\n",
      "Train Iter:  23  Loss:  1.3286495986192122\n",
      "Train Iter:  24  Loss:  1.3153467724720638\n",
      "Train Iter:  25  Loss:  1.3088133096694947\n",
      "Train Iter:  26  Loss:  1.3050680802418635\n",
      "Train Iter:  27  Loss:  1.3043346493332475\n",
      "Train Iter:  28  Loss:  1.3122724166938238\n",
      "Train Iter:  29  Loss:  1.2985533475875854\n",
      "Train Iter:  30  Loss:  1.2967176636060078\n",
      "Train Iter:  31  Loss:  1.297348318561431\n",
      "Train Iter:  32  Loss:  1.291446477174759\n",
      "Train Iter:  33  Loss:  1.3013622038292163\n",
      "Train Iter:  34  Loss:  1.305776431280024\n",
      "Train Iter:  35  Loss:  1.3008050407682146\n",
      "Train Iter:  36  Loss:  1.3097376624743144\n",
      "Train Iter:  37  Loss:  1.3070593395748653\n",
      "Train Iter:  38  Loss:  1.3036287709286338\n",
      "Train Iter:  39  Loss:  1.3072339968803601\n",
      "Train Iter:  40  Loss:  1.3040140479803086\n",
      "Train Iter:  41  Loss:  1.3083299282120495\n",
      "Train Iter:  42  Loss:  1.3114251210576011\n",
      "Train Iter:  43  Loss:  1.3130463971648105\n",
      "Train Iter:  44  Loss:  1.3158884129740975\n",
      "Train Iter:  45  Loss:  1.3079195088810391\n",
      "Train Iter:  46  Loss:  1.303987996733707\n",
      "Train Iter:  47  Loss:  1.3039284094851067\n",
      "Train Iter:  48  Loss:  1.3022683324913185\n",
      "Train Iter:  49  Loss:  1.3007006000499337\n",
      "Train Iter:  50  Loss:  1.302701984643936\n",
      "Train Iter:  51  Loss:  1.2999805878190434\n",
      "Train Iter:  52  Loss:  1.3017870084597514\n",
      "Train Iter:  53  Loss:  1.3027697722866851\n",
      "Train Iter:  54  Loss:  1.3018083936638303\n",
      "Train Iter:  55  Loss:  1.3056431586092168\n",
      "Train Iter:  56  Loss:  1.3089021752987589\n",
      "Train Iter:  57  Loss:  1.3094974432075233\n",
      "Train Iter:  58  Loss:  1.3106189257112042\n",
      "Train Iter:  59  Loss:  1.3059643739360873\n",
      "Train Iter:  60  Loss:  1.314518732825915\n",
      "Train Iter:  61  Loss:  1.3169069553984971\n",
      "Train Iter:  62  Loss:  1.3130065500736237\n",
      "Train Iter:  63  Loss:  1.30821279590092\n",
      "Train Iter:  64  Loss:  1.305182664655149\n",
      "Train Iter:  65  Loss:  1.3071133255958558\n",
      "Train Iter:  66  Loss:  1.3068375795176534\n",
      "Train Iter:  67  Loss:  1.3123909599745451\n",
      "Train Iter:  68  Loss:  1.3108675313346527\n",
      "Train Iter:  69  Loss:  1.3094180053558901\n",
      "Train Iter:  70  Loss:  1.3103509979588646\n",
      "Train Iter:  71  Loss:  1.3076784938154087\n",
      "Train Iter:  72  Loss:  1.3090884909033775\n",
      "Train Iter:  73  Loss:  1.308933141296857\n",
      "Train Iter:  74  Loss:  1.3061706673454594\n",
      "Train Iter:  75  Loss:  1.3017017483711242\n",
      "Train Iter:  76  Loss:  1.2986872455007152\n",
      "Train Iter:  77  Loss:  1.2969742277999976\n",
      "Train Iter:  78  Loss:  1.3014673353769841\n",
      "Train Iter:  79  Loss:  1.304263841502274\n",
      "Train Iter:  80  Loss:  1.303814274817705\n",
      "Train Iter:  81  Loss:  1.3023618525928922\n",
      "Train Iter:  82  Loss:  1.2997912219384822\n",
      "Train Iter:  83  Loss:  1.2973547161343586\n",
      "Train Iter:  84  Loss:  1.2944295555353165\n",
      "Train Iter:  85  Loss:  1.297219021881328\n",
      "Train Iter:  86  Loss:  1.290055047634036\n",
      "Train Iter:  87  Loss:  1.28900294742365\n",
      "Train Iter:  88  Loss:  1.2891610454429279\n",
      "Train Iter:  89  Loss:  1.2882915815610565\n",
      "Train Iter:  90  Loss:  1.287296344174279\n",
      "Train Iter:  91  Loss:  1.2868324518203735\n",
      "Train Iter:  92  Loss:  1.2865332611229108\n",
      "Train Iter:  93  Loss:  1.2895302080339002\n",
      "Train Iter:  94  Loss:  1.2876455555570887\n",
      "Train Iter:  95  Loss:  1.28802439790023\n",
      "Train Iter:  96  Loss:  1.288236889988184\n",
      "Train Iter:  97  Loss:  1.2897638328296621\n",
      "Train Iter:  98  Loss:  1.290267557513957\n",
      "Train Iter:  99  Loss:  1.2920379626630532\n",
      "Train Iter:  100  Loss:  1.2900558078289033\n",
      "Train Iter:  101  Loss:  1.2869861863627292\n",
      "Train Iter:  102  Loss:  1.284665640078339\n",
      "Train Iter:  103  Loss:  1.285048827962968\n",
      "Train Iter:  104  Loss:  1.282213018490718\n",
      "Train Iter:  105  Loss:  1.2815133968989054\n",
      "Train Iter:  106  Loss:  1.2816673877104274\n",
      "Train Iter:  107  Loss:  1.2813370105262114\n",
      "Train Iter:  108  Loss:  1.2824183415483545\n",
      "Train Iter:  109  Loss:  1.283951997756958\n",
      "Train Iter:  110  Loss:  1.2852542313662443\n",
      "Train Iter:  111  Loss:  1.2839213674132888\n",
      "Train Iter:  112  Loss:  1.2809372075966425\n",
      "Train Iter:  113  Loss:  1.2819036793919791\n",
      "Train Iter:  114  Loss:  1.2832598853529544\n",
      "Train Iter:  115  Loss:  1.2841219445933467\n",
      "Train Iter:  116  Loss:  1.286166151022089\n",
      "Train Iter:  117  Loss:  1.2857060656588302\n",
      "Train Iter:  118  Loss:  1.2851768267356742\n",
      "Train Iter:  119  Loss:  1.2853767160607987\n",
      "Train Iter:  120  Loss:  1.2877002318700155\n",
      "Train Iter:  121  Loss:  1.285702010816779\n",
      "Train Iter:  122  Loss:  1.2833383825958753\n",
      "Train Iter:  123  Loss:  1.2812442634163834\n",
      "Train Iter:  124  Loss:  1.2804018153298287\n",
      "Train Iter:  125  Loss:  1.2807513999938964\n",
      "Train Iter:  126  Loss:  1.2818652372511605\n",
      "Train Iter:  127  Loss:  1.2808145059375313\n",
      "Train Iter:  128  Loss:  1.278229161631316\n",
      "Train Iter:  129  Loss:  1.2790147699126901\n",
      "Train Iter:  130  Loss:  1.278789268548672\n",
      "Train Iter:  131  Loss:  1.2799796261860215\n",
      "Train Iter:  132  Loss:  1.281001755233967\n",
      "Train Iter:  133  Loss:  1.2816762623930336\n",
      "Train Iter:  134  Loss:  1.2794803732366704\n",
      "Train Iter:  135  Loss:  1.2781934027318602\n",
      "Train Iter:  136  Loss:  1.2785793669960077\n",
      "Train Iter:  137  Loss:  1.2765888884989884\n",
      "Train Iter:  138  Loss:  1.2762058537075485\n",
      "Train Iter:  139  Loss:  1.2787380016964973\n",
      "Train Iter:  140  Loss:  1.2774186692067555\n",
      "Train Iter:  141  Loss:  1.2758055752050792\n",
      "Train Iter:  142  Loss:  1.279688490528456\n",
      "Train Iter:  143  Loss:  1.2781324299065384\n",
      "Train Iter:  144  Loss:  1.2770954333245754\n",
      "Train Iter:  145  Loss:  1.2779054456743701\n",
      "Train Iter:  146  Loss:  1.2781382639930672\n",
      "Train Iter:  147  Loss:  1.2765368608390393\n",
      "Train Iter:  148  Loss:  1.277228510460338\n",
      "Train Iter:  149  Loss:  1.2759886560024032\n",
      "Train Iter:  150  Loss:  1.2772761444250742\n",
      "Train Iter:  151  Loss:  1.2781324303702803\n",
      "Train Iter:  152  Loss:  1.278685949742794\n",
      "Train Iter:  153  Loss:  1.2802951300845427\n",
      "Train Iter:  154  Loss:  1.2814929853012036\n",
      "Train Iter:  155  Loss:  1.2812364112946295\n",
      "Train Iter:  156  Loss:  1.2803439371860945\n",
      "Train Iter:  157  Loss:  1.280849484501371\n",
      "Train Iter:  158  Loss:  1.282023992342285\n",
      "Train Iter:  159  Loss:  1.280295782494095\n",
      "Train Iter:  160  Loss:  1.2820962261408568\n",
      "Train Iter:  161  Loss:  1.279632564657223\n",
      "Train Iter:  162  Loss:  1.2809560335712669\n",
      "Train Iter:  163  Loss:  1.280861645388457\n",
      "Train Iter:  164  Loss:  1.2816888173905814\n",
      "Train Iter:  165  Loss:  1.28020494778951\n",
      "Train Iter:  166  Loss:  1.2816412398614079\n",
      "Train Iter:  167  Loss:  1.281441859856337\n",
      "Train Iter:  168  Loss:  1.2836443313530512\n",
      "Train Iter:  169  Loss:  1.2836003783186511\n",
      "Train Iter:  170  Loss:  1.2854043427635642\n",
      "Train Iter:  171  Loss:  1.28654834819816\n",
      "Train Iter:  172  Loss:  1.2841675409743951\n",
      "Train Iter:  173  Loss:  1.2832037193237702\n",
      "Train Iter:  174  Loss:  1.2831607423294549\n",
      "Train Iter:  175  Loss:  1.2829678102902005\n",
      "Train Iter:  176  Loss:  1.2818455367603085\n",
      "Train Iter:  177  Loss:  1.2834618963764213\n",
      "Train Iter:  178  Loss:  1.2825315778844812\n",
      "Train Iter:  179  Loss:  1.2834812019790351\n",
      "Train Iter:  180  Loss:  1.2845226635535558\n",
      "Train Iter:  181  Loss:  1.2827410394974177\n",
      "Train Iter:  182  Loss:  1.285481578046149\n",
      "Train Iter:  183  Loss:  1.2845123393939493\n",
      "Train Iter:  184  Loss:  1.2851415976234104\n",
      "Train Iter:  185  Loss:  1.2853423163697526\n",
      "Train Iter:  186  Loss:  1.2848467794797753\n",
      "Val Iter: Loss:  1.021614909172058\n",
      "Epoch 12, Mean Train Loss: 1.2848467794797753, Val Loss (1 sample): 1.021614909172058\n",
      "Train Iter:  1  Loss:  1.0379085540771484\n",
      "Train Iter:  2  Loss:  1.0661240816116333\n",
      "Train Iter:  3  Loss:  1.0365336934725444\n",
      "Train Iter:  4  Loss:  1.141567662358284\n",
      "Train Iter:  5  Loss:  1.1255220770835876\n",
      "Train Iter:  6  Loss:  1.1520582139492035\n",
      "Train Iter:  7  Loss:  1.1964417610849654\n",
      "Train Iter:  8  Loss:  1.2003238126635551\n",
      "Train Iter:  9  Loss:  1.214306930700938\n",
      "Train Iter:  10  Loss:  1.2556202948093413\n",
      "Train Iter:  11  Loss:  1.2539033510468223\n",
      "Train Iter:  12  Loss:  1.2683650702238083\n",
      "Train Iter:  13  Loss:  1.27364516716737\n",
      "Train Iter:  14  Loss:  1.2679869149412428\n",
      "Train Iter:  15  Loss:  1.2784772515296936\n",
      "Train Iter:  16  Loss:  1.2660762183368206\n",
      "Train Iter:  17  Loss:  1.267138000796823\n",
      "Train Iter:  18  Loss:  1.2591054870022669\n",
      "Train Iter:  19  Loss:  1.250967995116585\n",
      "Train Iter:  20  Loss:  1.2431002706289291\n",
      "Train Iter:  21  Loss:  1.2522203042393638\n",
      "Train Iter:  22  Loss:  1.2560617463155226\n",
      "Train Iter:  23  Loss:  1.2509769330854001\n",
      "Train Iter:  24  Loss:  1.2360316539804141\n",
      "Train Iter:  25  Loss:  1.2183185958862304\n",
      "Train Iter:  26  Loss:  1.2187295785317054\n",
      "Train Iter:  27  Loss:  1.222639070616828\n",
      "Train Iter:  28  Loss:  1.223055056163243\n",
      "Train Iter:  29  Loss:  1.2338889919478317\n",
      "Train Iter:  30  Loss:  1.2338023026784262\n",
      "Train Iter:  31  Loss:  1.229591638811173\n",
      "Train Iter:  32  Loss:  1.2392272762954235\n",
      "Train Iter:  33  Loss:  1.236253590294809\n",
      "Train Iter:  34  Loss:  1.2385159590665031\n",
      "Train Iter:  35  Loss:  1.2312502843993052\n",
      "Train Iter:  36  Loss:  1.2433268907997344\n",
      "Train Iter:  37  Loss:  1.240429564102276\n",
      "Train Iter:  38  Loss:  1.2496611083808697\n",
      "Train Iter:  39  Loss:  1.2444668473341527\n",
      "Train Iter:  40  Loss:  1.2474920079112053\n",
      "Train Iter:  41  Loss:  1.2537858646090438\n",
      "Train Iter:  42  Loss:  1.2511193397499265\n",
      "Train Iter:  43  Loss:  1.246852077717005\n",
      "Train Iter:  44  Loss:  1.249435076659376\n",
      "Train Iter:  45  Loss:  1.2539554582701788\n",
      "Train Iter:  46  Loss:  1.2503653881342516\n",
      "Train Iter:  47  Loss:  1.2510570376477343\n",
      "Train Iter:  48  Loss:  1.2522673246761162\n",
      "Train Iter:  49  Loss:  1.2526397863212897\n",
      "Train Iter:  50  Loss:  1.251606184244156\n",
      "Train Iter:  51  Loss:  1.2538761145928328\n",
      "Train Iter:  52  Loss:  1.2488150825867286\n",
      "Train Iter:  53  Loss:  1.2453607298293203\n",
      "Train Iter:  54  Loss:  1.2381457487742107\n",
      "Train Iter:  55  Loss:  1.2383751088922674\n",
      "Train Iter:  56  Loss:  1.2370121777057648\n",
      "Train Iter:  57  Loss:  1.2402943163587337\n",
      "Train Iter:  58  Loss:  1.2356454185370742\n",
      "Train Iter:  59  Loss:  1.2410935618109622\n",
      "Train Iter:  60  Loss:  1.2335863918066026\n",
      "Train Iter:  61  Loss:  1.2297612887914064\n",
      "Train Iter:  62  Loss:  1.2369721983709643\n",
      "Train Iter:  63  Loss:  1.2322585639499484\n",
      "Train Iter:  64  Loss:  1.2385546807199717\n",
      "Train Iter:  65  Loss:  1.2359972972136277\n",
      "Train Iter:  66  Loss:  1.2432477004600293\n",
      "Train Iter:  67  Loss:  1.2505069419519226\n",
      "Train Iter:  68  Loss:  1.2526138144380905\n",
      "Train Iter:  69  Loss:  1.2483770208082337\n",
      "Train Iter:  70  Loss:  1.2501403536115374\n",
      "Train Iter:  71  Loss:  1.253720545432937\n",
      "Train Iter:  72  Loss:  1.251901497443517\n",
      "Train Iter:  73  Loss:  1.2503371810259885\n",
      "Train Iter:  74  Loss:  1.2526223820609015\n",
      "Train Iter:  75  Loss:  1.2621215883890788\n",
      "Train Iter:  76  Loss:  1.2607458186777014\n",
      "Train Iter:  77  Loss:  1.2591771398271834\n",
      "Train Iter:  78  Loss:  1.2563734604762151\n",
      "Train Iter:  79  Loss:  1.2586344872848898\n",
      "Train Iter:  80  Loss:  1.2596176460385322\n",
      "Train Iter:  81  Loss:  1.2566307680106457\n",
      "Train Iter:  82  Loss:  1.257468588468505\n",
      "Train Iter:  83  Loss:  1.2561076822051083\n",
      "Train Iter:  84  Loss:  1.2559393360501243\n",
      "Train Iter:  85  Loss:  1.260601721090429\n",
      "Train Iter:  86  Loss:  1.2558510386666586\n",
      "Train Iter:  87  Loss:  1.2567709643265297\n",
      "Train Iter:  88  Loss:  1.2559411945668133\n",
      "Train Iter:  89  Loss:  1.2564478108052457\n",
      "Train Iter:  90  Loss:  1.2579945034450954\n",
      "Train Iter:  91  Loss:  1.2567252012399526\n",
      "Train Iter:  92  Loss:  1.2565787279087564\n",
      "Train Iter:  93  Loss:  1.2555881674571703\n",
      "Train Iter:  94  Loss:  1.2564229014072013\n",
      "Train Iter:  95  Loss:  1.2632959880326924\n",
      "Train Iter:  96  Loss:  1.2578864178309839\n",
      "Train Iter:  97  Loss:  1.2544791870510454\n",
      "Train Iter:  98  Loss:  1.254087443254432\n",
      "Train Iter:  99  Loss:  1.2566328494235723\n",
      "Train Iter:  100  Loss:  1.255740250349045\n",
      "Train Iter:  101  Loss:  1.2557637290199204\n",
      "Train Iter:  102  Loss:  1.2587348818778992\n",
      "Train Iter:  103  Loss:  1.2602849909402791\n",
      "Train Iter:  104  Loss:  1.2603739477120912\n",
      "Train Iter:  105  Loss:  1.260829192116147\n",
      "Train Iter:  106  Loss:  1.2579965265292041\n",
      "Train Iter:  107  Loss:  1.258459437673337\n",
      "Train Iter:  108  Loss:  1.2652408944235907\n",
      "Train Iter:  109  Loss:  1.2650294478880155\n",
      "Train Iter:  110  Loss:  1.265589579668912\n",
      "Train Iter:  111  Loss:  1.2678483778291993\n",
      "Train Iter:  112  Loss:  1.265543272452695\n",
      "Train Iter:  113  Loss:  1.2668746488284222\n",
      "Train Iter:  114  Loss:  1.2693992100263898\n",
      "Train Iter:  115  Loss:  1.2691528537999028\n",
      "Train Iter:  116  Loss:  1.2674327626310546\n",
      "Train Iter:  117  Loss:  1.2688314048652976\n",
      "Train Iter:  118  Loss:  1.2731003003605341\n",
      "Train Iter:  119  Loss:  1.2751823753869833\n",
      "Train Iter:  120  Loss:  1.277024265130361\n",
      "Train Iter:  121  Loss:  1.2748230932172664\n",
      "Train Iter:  122  Loss:  1.2753611500145958\n",
      "Train Iter:  123  Loss:  1.2756027661687959\n",
      "Train Iter:  124  Loss:  1.2767837682077963\n",
      "Train Iter:  125  Loss:  1.2755885677337646\n",
      "Train Iter:  126  Loss:  1.2783864944700212\n",
      "Train Iter:  127  Loss:  1.277609205621434\n",
      "Train Iter:  128  Loss:  1.276875383220613\n",
      "Train Iter:  129  Loss:  1.2758699156517206\n",
      "Train Iter:  130  Loss:  1.2777590247300954\n",
      "Train Iter:  131  Loss:  1.2756404967708441\n",
      "Train Iter:  132  Loss:  1.2765756086869673\n",
      "Train Iter:  133  Loss:  1.2767066471558763\n",
      "Train Iter:  134  Loss:  1.278354915220346\n",
      "Train Iter:  135  Loss:  1.2806240099447745\n",
      "Train Iter:  136  Loss:  1.2845099515774672\n",
      "Train Iter:  137  Loss:  1.2878448006010403\n",
      "Train Iter:  138  Loss:  1.2889833813128264\n",
      "Train Iter:  139  Loss:  1.290238319541053\n",
      "Train Iter:  140  Loss:  1.2918888347489492\n",
      "Train Iter:  141  Loss:  1.290260910987854\n",
      "Train Iter:  142  Loss:  1.2901061194043764\n",
      "Train Iter:  143  Loss:  1.2909075805357286\n",
      "Train Iter:  144  Loss:  1.2899186569783423\n",
      "Train Iter:  145  Loss:  1.2905371715282572\n",
      "Train Iter:  146  Loss:  1.2912781548826662\n",
      "Train Iter:  147  Loss:  1.290030686222777\n",
      "Train Iter:  148  Loss:  1.288922546683131\n",
      "Train Iter:  149  Loss:  1.2931590736312355\n",
      "Train Iter:  150  Loss:  1.295337221622467\n",
      "Train Iter:  151  Loss:  1.295587071519814\n",
      "Train Iter:  152  Loss:  1.2938830389788276\n",
      "Train Iter:  153  Loss:  1.2923763172299254\n",
      "Train Iter:  154  Loss:  1.2920511659089622\n",
      "Train Iter:  155  Loss:  1.2901027960162008\n",
      "Train Iter:  156  Loss:  1.2894618927668302\n",
      "Train Iter:  157  Loss:  1.2881965929535544\n",
      "Train Iter:  158  Loss:  1.289379265866702\n",
      "Train Iter:  159  Loss:  1.289995013917767\n",
      "Train Iter:  160  Loss:  1.2915234204381705\n",
      "Train Iter:  161  Loss:  1.29045669003303\n",
      "Train Iter:  162  Loss:  1.291319308825481\n",
      "Train Iter:  163  Loss:  1.2886885177869738\n",
      "Train Iter:  164  Loss:  1.2864247702243852\n",
      "Train Iter:  165  Loss:  1.2858461694283918\n",
      "Train Iter:  166  Loss:  1.2860290842602051\n",
      "Train Iter:  167  Loss:  1.2865310511189307\n",
      "Train Iter:  168  Loss:  1.2875037118792534\n",
      "Train Iter:  169  Loss:  1.2877647703921302\n",
      "Train Iter:  170  Loss:  1.2863464884898241\n",
      "Train Iter:  171  Loss:  1.2867535384077775\n",
      "Train Iter:  172  Loss:  1.2869326947040336\n",
      "Train Iter:  173  Loss:  1.286024943597055\n",
      "Train Iter:  174  Loss:  1.285744945550787\n",
      "Train Iter:  175  Loss:  1.285371824332646\n",
      "Train Iter:  176  Loss:  1.2857439311390573\n",
      "Train Iter:  177  Loss:  1.288567591858449\n",
      "Train Iter:  178  Loss:  1.2887608874379919\n",
      "Train Iter:  179  Loss:  1.2900634487913973\n",
      "Train Iter:  180  Loss:  1.2899313320716221\n",
      "Train Iter:  181  Loss:  1.2925236768485433\n",
      "Train Iter:  182  Loss:  1.2925690907043415\n",
      "Train Iter:  183  Loss:  1.2932995841151378\n",
      "Train Iter:  184  Loss:  1.2976208734123602\n",
      "Train Iter:  185  Loss:  1.2982974725800591\n",
      "Train Iter:  186  Loss:  1.2978749291230274\n",
      "Val Iter: Loss:  1.4879215955734253\n",
      "Epoch 13, Mean Train Loss: 1.2978749291230274, Val Loss (1 sample): 1.4879215955734253\n",
      "Train Iter:  1  Loss:  1.0409311056137085\n",
      "Train Iter:  2  Loss:  1.2269152998924255\n",
      "Train Iter:  3  Loss:  1.1554454565048218\n",
      "Train Iter:  4  Loss:  1.1514086723327637\n",
      "Train Iter:  5  Loss:  1.159398078918457\n",
      "Train Iter:  6  Loss:  1.1743654211362202\n",
      "Train Iter:  7  Loss:  1.1598798377173287\n",
      "Train Iter:  8  Loss:  1.2287854999303818\n",
      "Train Iter:  9  Loss:  1.242224269443088\n",
      "Train Iter:  10  Loss:  1.2381547570228577\n",
      "Train Iter:  11  Loss:  1.2393636595119129\n",
      "Train Iter:  12  Loss:  1.2579899330933888\n",
      "Train Iter:  13  Loss:  1.2575719539935772\n",
      "Train Iter:  14  Loss:  1.2590311084474837\n",
      "Train Iter:  15  Loss:  1.2555548350016277\n",
      "Train Iter:  16  Loss:  1.2519212886691093\n",
      "Train Iter:  17  Loss:  1.252228112781749\n",
      "Train Iter:  18  Loss:  1.2502072387271457\n",
      "Train Iter:  19  Loss:  1.2428812604201467\n",
      "Train Iter:  20  Loss:  1.2497923016548156\n",
      "Train Iter:  21  Loss:  1.2459628809066046\n",
      "Train Iter:  22  Loss:  1.2407299334352666\n",
      "Train Iter:  23  Loss:  1.2516015094259512\n",
      "Train Iter:  24  Loss:  1.2438590476910274\n",
      "Train Iter:  25  Loss:  1.2505390453338623\n",
      "Train Iter:  26  Loss:  1.245629681990697\n",
      "Train Iter:  27  Loss:  1.2494835279606007\n",
      "Train Iter:  28  Loss:  1.2540240713528223\n",
      "Train Iter:  29  Loss:  1.2460190016647865\n",
      "Train Iter:  30  Loss:  1.2449658950169882\n",
      "Train Iter:  31  Loss:  1.2412825207556448\n",
      "Train Iter:  32  Loss:  1.2412178926169872\n",
      "Train Iter:  33  Loss:  1.2407390601707227\n",
      "Train Iter:  34  Loss:  1.232349758639055\n",
      "Train Iter:  35  Loss:  1.231391101224082\n",
      "Train Iter:  36  Loss:  1.2306830535332363\n",
      "Train Iter:  37  Loss:  1.2350123524665833\n",
      "Train Iter:  38  Loss:  1.2376748307755119\n",
      "Train Iter:  39  Loss:  1.2405885351009858\n",
      "Train Iter:  40  Loss:  1.2558739200234412\n",
      "Train Iter:  41  Loss:  1.2603069820055148\n",
      "Train Iter:  42  Loss:  1.2623172870704107\n",
      "Train Iter:  43  Loss:  1.2585924406384312\n",
      "Train Iter:  44  Loss:  1.258935726501725\n",
      "Train Iter:  45  Loss:  1.2554030378659566\n",
      "Train Iter:  46  Loss:  1.2535010822441266\n",
      "Train Iter:  47  Loss:  1.245943400454014\n",
      "Train Iter:  48  Loss:  1.2462637387216091\n",
      "Train Iter:  49  Loss:  1.247896202972957\n",
      "Train Iter:  50  Loss:  1.2472013056278228\n",
      "Train Iter:  51  Loss:  1.2447371284166973\n",
      "Train Iter:  52  Loss:  1.2444014904590754\n",
      "Train Iter:  53  Loss:  1.243484968284391\n",
      "Train Iter:  54  Loss:  1.2467465809097997\n",
      "Train Iter:  55  Loss:  1.2485964612527327\n",
      "Train Iter:  56  Loss:  1.2474708929657936\n",
      "Train Iter:  57  Loss:  1.2490419975498266\n",
      "Train Iter:  58  Loss:  1.251522990136311\n",
      "Train Iter:  59  Loss:  1.249861530328201\n",
      "Train Iter:  60  Loss:  1.2539928525686264\n",
      "Train Iter:  61  Loss:  1.2547177261993534\n",
      "Train Iter:  62  Loss:  1.2567170714178393\n",
      "Train Iter:  63  Loss:  1.2547536832945687\n",
      "Train Iter:  64  Loss:  1.2536888597533107\n",
      "Train Iter:  65  Loss:  1.2560530139849737\n",
      "Train Iter:  66  Loss:  1.2592190278298927\n",
      "Train Iter:  67  Loss:  1.2589334418524558\n",
      "Train Iter:  68  Loss:  1.254535916097024\n",
      "Train Iter:  69  Loss:  1.2458180025003958\n",
      "Train Iter:  70  Loss:  1.2456952154636383\n",
      "Train Iter:  71  Loss:  1.2416531661866417\n",
      "Train Iter:  72  Loss:  1.2380875357323222\n",
      "Train Iter:  73  Loss:  1.2395762245949\n",
      "Train Iter:  74  Loss:  1.2398074873395868\n",
      "Train Iter:  75  Loss:  1.238172449270884\n",
      "Train Iter:  76  Loss:  1.2374091297388077\n",
      "Train Iter:  77  Loss:  1.233501903422467\n",
      "Train Iter:  78  Loss:  1.2288254316036518\n",
      "Train Iter:  79  Loss:  1.2250480365149583\n",
      "Train Iter:  80  Loss:  1.2244978591799736\n",
      "Train Iter:  81  Loss:  1.2214728954397602\n",
      "Train Iter:  82  Loss:  1.2214313084032478\n",
      "Train Iter:  83  Loss:  1.2277736886438118\n",
      "Train Iter:  84  Loss:  1.227931342664219\n",
      "Train Iter:  85  Loss:  1.228488211772021\n",
      "Train Iter:  86  Loss:  1.2293431239072667\n",
      "Train Iter:  87  Loss:  1.2304316327489655\n",
      "Train Iter:  88  Loss:  1.2315842082554644\n",
      "Train Iter:  89  Loss:  1.2308633802981859\n",
      "Train Iter:  90  Loss:  1.231666851705975\n",
      "Train Iter:  91  Loss:  1.2278856405845056\n",
      "Train Iter:  92  Loss:  1.2283914179905602\n",
      "Train Iter:  93  Loss:  1.2300296996229438\n",
      "Train Iter:  94  Loss:  1.2260219201128533\n",
      "Train Iter:  95  Loss:  1.2267184621409366\n",
      "Train Iter:  96  Loss:  1.2254871676365535\n",
      "Train Iter:  97  Loss:  1.226008195238015\n",
      "Train Iter:  98  Loss:  1.2271139378450355\n",
      "Train Iter:  99  Loss:  1.2279342063749679\n",
      "Train Iter:  100  Loss:  1.230068951845169\n",
      "Train Iter:  101  Loss:  1.2311688897633317\n",
      "Train Iter:  102  Loss:  1.2306471233274423\n",
      "Train Iter:  103  Loss:  1.2317281440623755\n",
      "Train Iter:  104  Loss:  1.2312669135057008\n",
      "Train Iter:  105  Loss:  1.2301758323396956\n",
      "Train Iter:  106  Loss:  1.2337776501223725\n",
      "Train Iter:  107  Loss:  1.2351930364270076\n",
      "Train Iter:  108  Loss:  1.236489654691131\n",
      "Train Iter:  109  Loss:  1.2363449236668578\n",
      "Train Iter:  110  Loss:  1.2363032525235957\n",
      "Train Iter:  111  Loss:  1.2360928370071962\n",
      "Train Iter:  112  Loss:  1.238502646131175\n",
      "Train Iter:  113  Loss:  1.2411398170268642\n",
      "Train Iter:  114  Loss:  1.240659190897356\n",
      "Train Iter:  115  Loss:  1.2417027805162513\n",
      "Train Iter:  116  Loss:  1.2414561294276139\n",
      "Train Iter:  117  Loss:  1.2413498275300376\n",
      "Train Iter:  118  Loss:  1.2410913665415877\n",
      "Train Iter:  119  Loss:  1.24057016753349\n",
      "Train Iter:  120  Loss:  1.243404965599378\n",
      "Train Iter:  121  Loss:  1.2440345878443442\n",
      "Train Iter:  122  Loss:  1.2479098654184184\n",
      "Train Iter:  123  Loss:  1.246923040568344\n",
      "Train Iter:  124  Loss:  1.2462645423027776\n",
      "Train Iter:  125  Loss:  1.2465620594024658\n",
      "Train Iter:  126  Loss:  1.2472882857398382\n",
      "Train Iter:  127  Loss:  1.2448145198071097\n",
      "Train Iter:  128  Loss:  1.2458610674366355\n",
      "Train Iter:  129  Loss:  1.2470290346663127\n",
      "Train Iter:  130  Loss:  1.2481101558758663\n",
      "Train Iter:  131  Loss:  1.248168145427267\n",
      "Train Iter:  132  Loss:  1.2493320884126606\n",
      "Train Iter:  133  Loss:  1.2530622267185296\n",
      "Train Iter:  134  Loss:  1.2524761246211493\n",
      "Train Iter:  135  Loss:  1.253704184072989\n",
      "Train Iter:  136  Loss:  1.254611133652575\n",
      "Train Iter:  137  Loss:  1.2558678562623742\n",
      "Train Iter:  138  Loss:  1.2554845058399697\n",
      "Train Iter:  139  Loss:  1.2584516470380824\n",
      "Train Iter:  140  Loss:  1.2581197236265456\n",
      "Train Iter:  141  Loss:  1.2586050963570885\n",
      "Train Iter:  142  Loss:  1.2595763836108462\n",
      "Train Iter:  143  Loss:  1.2603970474296518\n",
      "Train Iter:  144  Loss:  1.2584558468725946\n",
      "Train Iter:  145  Loss:  1.2603544013253574\n",
      "Train Iter:  146  Loss:  1.259181317401259\n",
      "Train Iter:  147  Loss:  1.2577140087984047\n",
      "Train Iter:  148  Loss:  1.256794325403265\n",
      "Train Iter:  149  Loss:  1.2584021235472405\n",
      "Train Iter:  150  Loss:  1.2576434723536174\n",
      "Train Iter:  151  Loss:  1.2590374251864604\n",
      "Train Iter:  152  Loss:  1.2577327097717084\n",
      "Train Iter:  153  Loss:  1.2576385518304662\n",
      "Train Iter:  154  Loss:  1.2581948173510564\n",
      "Train Iter:  155  Loss:  1.255347769106588\n",
      "Train Iter:  156  Loss:  1.2561582758640633\n",
      "Train Iter:  157  Loss:  1.255695682802018\n",
      "Train Iter:  158  Loss:  1.253176139125341\n",
      "Train Iter:  159  Loss:  1.2524850525945987\n",
      "Train Iter:  160  Loss:  1.2504140947014093\n",
      "Train Iter:  161  Loss:  1.2504516984365002\n",
      "Train Iter:  162  Loss:  1.249385854712239\n",
      "Train Iter:  163  Loss:  1.2485387584914458\n",
      "Train Iter:  164  Loss:  1.2475090808257825\n",
      "Train Iter:  165  Loss:  1.2502327077316515\n",
      "Train Iter:  166  Loss:  1.2494454100189438\n",
      "Train Iter:  167  Loss:  1.2496973415334782\n",
      "Train Iter:  168  Loss:  1.2502470055506343\n",
      "Train Iter:  169  Loss:  1.249943596018842\n",
      "Train Iter:  170  Loss:  1.2498075888437383\n",
      "Train Iter:  171  Loss:  1.2501316359865735\n",
      "Train Iter:  172  Loss:  1.2540513287449992\n",
      "Train Iter:  173  Loss:  1.2540461062695938\n",
      "Train Iter:  174  Loss:  1.2534893857336593\n",
      "Train Iter:  175  Loss:  1.2520779524530683\n",
      "Train Iter:  176  Loss:  1.2519611841575666\n",
      "Train Iter:  177  Loss:  1.2522501969068063\n",
      "Train Iter:  178  Loss:  1.2543092848879567\n",
      "Train Iter:  179  Loss:  1.253890867672819\n",
      "Train Iter:  180  Loss:  1.254799105392562\n",
      "Train Iter:  181  Loss:  1.2579049514143505\n",
      "Train Iter:  182  Loss:  1.2564748784342965\n",
      "Train Iter:  183  Loss:  1.2563737145538538\n",
      "Train Iter:  184  Loss:  1.254460562830386\n",
      "Train Iter:  185  Loss:  1.256429183805311\n",
      "Train Iter:  186  Loss:  1.2567800597477985\n",
      "Val Iter: Loss:  0.7039147019386292\n",
      "Epoch 14, Mean Train Loss: 1.2567800597477985, Val Loss (1 sample): 0.7039147019386292\n",
      "Train Iter:  1  Loss:  1.6135761737823486\n",
      "Train Iter:  2  Loss:  1.3691390752792358\n",
      "Train Iter:  3  Loss:  1.3273125092188518\n",
      "Train Iter:  4  Loss:  1.2652432024478912\n",
      "Train Iter:  5  Loss:  1.3496683597564698\n",
      "Train Iter:  6  Loss:  1.3414511481920879\n",
      "Train Iter:  7  Loss:  1.2894496491977148\n",
      "Train Iter:  8  Loss:  1.3473056927323341\n",
      "Train Iter:  9  Loss:  1.3168941934903462\n",
      "Train Iter:  10  Loss:  1.3334111869335175\n",
      "Train Iter:  11  Loss:  1.3106031905521045\n",
      "Train Iter:  12  Loss:  1.294062688946724\n",
      "Train Iter:  13  Loss:  1.2982264894705553\n",
      "Train Iter:  14  Loss:  1.2847770069326674\n",
      "Train Iter:  15  Loss:  1.2858089168866476\n",
      "Train Iter:  16  Loss:  1.2532033175230026\n",
      "Train Iter:  17  Loss:  1.250975580776439\n",
      "Train Iter:  18  Loss:  1.2509073350164626\n",
      "Train Iter:  19  Loss:  1.2553292763860602\n",
      "Train Iter:  20  Loss:  1.2448972761631012\n",
      "Train Iter:  21  Loss:  1.242887093907311\n",
      "Train Iter:  22  Loss:  1.2373972860249607\n",
      "Train Iter:  23  Loss:  1.2426135332688042\n",
      "Train Iter:  24  Loss:  1.2426131268342335\n",
      "Train Iter:  25  Loss:  1.2366547060012818\n",
      "Train Iter:  26  Loss:  1.2323589783448439\n",
      "Train Iter:  27  Loss:  1.2329284217622545\n",
      "Train Iter:  28  Loss:  1.2245187312364578\n",
      "Train Iter:  29  Loss:  1.2227264087775658\n",
      "Train Iter:  30  Loss:  1.2264704604943593\n",
      "Train Iter:  31  Loss:  1.2348209331112523\n",
      "Train Iter:  32  Loss:  1.2333405707031488\n",
      "Train Iter:  33  Loss:  1.23400504841949\n",
      "Train Iter:  34  Loss:  1.2326709435266607\n",
      "Train Iter:  35  Loss:  1.2345765471458434\n",
      "Train Iter:  36  Loss:  1.2353821479611926\n",
      "Train Iter:  37  Loss:  1.2260173253110938\n",
      "Train Iter:  38  Loss:  1.2369312251869\n",
      "Train Iter:  39  Loss:  1.2365321669823084\n",
      "Train Iter:  40  Loss:  1.2336397394537926\n",
      "Train Iter:  41  Loss:  1.2352173168484757\n",
      "Train Iter:  42  Loss:  1.2329859719389962\n",
      "Train Iter:  43  Loss:  1.2437606842018838\n",
      "Train Iter:  44  Loss:  1.2482361481948332\n",
      "Train Iter:  45  Loss:  1.2516353567441305\n",
      "Train Iter:  46  Loss:  1.251275325598924\n",
      "Train Iter:  47  Loss:  1.244635395547177\n",
      "Train Iter:  48  Loss:  1.240476455539465\n",
      "Train Iter:  49  Loss:  1.2422774069163265\n",
      "Train Iter:  50  Loss:  1.2416470491886138\n",
      "Train Iter:  51  Loss:  1.2390467197287316\n",
      "Train Iter:  52  Loss:  1.2331702766510158\n",
      "Train Iter:  53  Loss:  1.2382760036666438\n",
      "Train Iter:  54  Loss:  1.2373329235447779\n",
      "Train Iter:  55  Loss:  1.23726557709954\n",
      "Train Iter:  56  Loss:  1.240057541855744\n",
      "Train Iter:  57  Loss:  1.2359689902841\n",
      "Train Iter:  58  Loss:  1.2323704927132046\n",
      "Train Iter:  59  Loss:  1.2364862338971283\n",
      "Train Iter:  60  Loss:  1.2375118960936864\n",
      "Train Iter:  61  Loss:  1.2314380807954757\n",
      "Train Iter:  62  Loss:  1.2381293187218327\n",
      "Train Iter:  63  Loss:  1.2428194256055922\n",
      "Train Iter:  64  Loss:  1.2400679411366582\n",
      "Train Iter:  65  Loss:  1.2417966466683608\n",
      "Train Iter:  66  Loss:  1.240743538646987\n",
      "Train Iter:  67  Loss:  1.2379648338502913\n",
      "Train Iter:  68  Loss:  1.2378301892210455\n",
      "Train Iter:  69  Loss:  1.2382132359172986\n",
      "Train Iter:  70  Loss:  1.2399527319840022\n",
      "Train Iter:  71  Loss:  1.235060366106705\n",
      "Train Iter:  72  Loss:  1.2371231185065374\n",
      "Train Iter:  73  Loss:  1.2346642997166881\n",
      "Train Iter:  74  Loss:  1.2372202615480166\n",
      "Train Iter:  75  Loss:  1.2420790926615397\n",
      "Train Iter:  76  Loss:  1.2424919275861037\n",
      "Train Iter:  77  Loss:  1.2424698804880117\n",
      "Train Iter:  78  Loss:  1.2416323056587806\n",
      "Train Iter:  79  Loss:  1.2441780280463304\n",
      "Train Iter:  80  Loss:  1.2444813176989555\n",
      "Train Iter:  81  Loss:  1.24994789082327\n",
      "Train Iter:  82  Loss:  1.2520731876536113\n",
      "Train Iter:  83  Loss:  1.253301392118615\n",
      "Train Iter:  84  Loss:  1.2526726538226718\n",
      "Train Iter:  85  Loss:  1.2542659240610459\n",
      "Train Iter:  86  Loss:  1.2523669184640396\n",
      "Train Iter:  87  Loss:  1.2498150803576942\n",
      "Train Iter:  88  Loss:  1.2508052126927809\n",
      "Train Iter:  89  Loss:  1.2494341148419326\n",
      "Train Iter:  90  Loss:  1.2501254200935363\n",
      "Train Iter:  91  Loss:  1.2484265461072817\n",
      "Train Iter:  92  Loss:  1.2454186742720397\n",
      "Train Iter:  93  Loss:  1.2472157055331814\n",
      "Train Iter:  94  Loss:  1.2470112075196935\n",
      "Train Iter:  95  Loss:  1.2493920276039525\n",
      "Train Iter:  96  Loss:  1.2456487913926442\n",
      "Train Iter:  97  Loss:  1.2446344894232209\n",
      "Train Iter:  98  Loss:  1.244741281684564\n",
      "Train Iter:  99  Loss:  1.249561551845435\n",
      "Train Iter:  100  Loss:  1.2540246737003327\n",
      "Train Iter:  101  Loss:  1.254704173248593\n",
      "Train Iter:  102  Loss:  1.2563054012317283\n",
      "Train Iter:  103  Loss:  1.2589358079780653\n",
      "Train Iter:  104  Loss:  1.2573527785447927\n",
      "Train Iter:  105  Loss:  1.2572143986111595\n",
      "Train Iter:  106  Loss:  1.2562906663372833\n",
      "Train Iter:  107  Loss:  1.2573710613161604\n",
      "Train Iter:  108  Loss:  1.2570920730078663\n",
      "Train Iter:  109  Loss:  1.253519527409055\n",
      "Train Iter:  110  Loss:  1.2498169644312425\n",
      "Train Iter:  111  Loss:  1.2466509041485485\n",
      "Train Iter:  112  Loss:  1.2483021255050386\n",
      "Train Iter:  113  Loss:  1.2511576614548674\n",
      "Train Iter:  114  Loss:  1.2518578182186997\n",
      "Train Iter:  115  Loss:  1.2498578206352566\n",
      "Train Iter:  116  Loss:  1.2510355227980121\n",
      "Train Iter:  117  Loss:  1.2520133175401607\n",
      "Train Iter:  118  Loss:  1.251891291747659\n",
      "Train Iter:  119  Loss:  1.252770284644696\n",
      "Train Iter:  120  Loss:  1.2492522671818733\n",
      "Train Iter:  121  Loss:  1.2486228189192528\n",
      "Train Iter:  122  Loss:  1.2479802384728291\n",
      "Train Iter:  123  Loss:  1.2497922826588639\n",
      "Train Iter:  124  Loss:  1.2487148772324286\n",
      "Train Iter:  125  Loss:  1.2478355507850647\n",
      "Train Iter:  126  Loss:  1.2462329339413416\n",
      "Train Iter:  127  Loss:  1.2481190853231534\n",
      "Train Iter:  128  Loss:  1.248262817505747\n",
      "Train Iter:  129  Loss:  1.2499492034431576\n",
      "Train Iter:  130  Loss:  1.2510986250180465\n",
      "Train Iter:  131  Loss:  1.2512936451052892\n",
      "Train Iter:  132  Loss:  1.2543122899351697\n",
      "Train Iter:  133  Loss:  1.2542223047493095\n",
      "Train Iter:  134  Loss:  1.2546749590937771\n",
      "Train Iter:  135  Loss:  1.2540317504494278\n",
      "Train Iter:  136  Loss:  1.253548087880892\n",
      "Train Iter:  137  Loss:  1.2559791564071265\n",
      "Train Iter:  138  Loss:  1.2554178864195726\n",
      "Train Iter:  139  Loss:  1.254256024206285\n",
      "Train Iter:  140  Loss:  1.254595199227333\n",
      "Train Iter:  141  Loss:  1.2535993888023051\n",
      "Train Iter:  142  Loss:  1.2546088976759306\n",
      "Train Iter:  143  Loss:  1.2539866583330648\n",
      "Train Iter:  144  Loss:  1.2530511530737083\n",
      "Train Iter:  145  Loss:  1.2538094787762082\n",
      "Train Iter:  146  Loss:  1.2526425821323917\n",
      "Train Iter:  147  Loss:  1.2542715190219231\n",
      "Train Iter:  148  Loss:  1.2556281053536646\n",
      "Train Iter:  149  Loss:  1.257192072852346\n",
      "Train Iter:  150  Loss:  1.255668997367223\n",
      "Train Iter:  151  Loss:  1.2554200046899302\n",
      "Train Iter:  152  Loss:  1.2560443835038888\n",
      "Train Iter:  153  Loss:  1.2562362006287169\n",
      "Train Iter:  154  Loss:  1.257605412176677\n",
      "Train Iter:  155  Loss:  1.2584263705438183\n",
      "Train Iter:  156  Loss:  1.2564585571869826\n",
      "Train Iter:  157  Loss:  1.255373438072812\n",
      "Train Iter:  158  Loss:  1.2553165241887299\n",
      "Train Iter:  159  Loss:  1.2559830366440539\n",
      "Train Iter:  160  Loss:  1.2545746710151433\n",
      "Train Iter:  161  Loss:  1.254204158827385\n",
      "Train Iter:  162  Loss:  1.253920105136471\n",
      "Train Iter:  163  Loss:  1.2553104012290393\n",
      "Train Iter:  164  Loss:  1.2563926893763426\n",
      "Train Iter:  165  Loss:  1.2553472457510053\n",
      "Train Iter:  166  Loss:  1.2575392855937222\n",
      "Train Iter:  167  Loss:  1.2566204153135152\n",
      "Train Iter:  168  Loss:  1.2597556827323777\n",
      "Train Iter:  169  Loss:  1.2601980207234444\n",
      "Train Iter:  170  Loss:  1.2608922912794\n",
      "Train Iter:  171  Loss:  1.2619510146609523\n",
      "Train Iter:  172  Loss:  1.2601742068695467\n",
      "Train Iter:  173  Loss:  1.2596950313948483\n",
      "Train Iter:  174  Loss:  1.2609950711672333\n",
      "Train Iter:  175  Loss:  1.2618304644312177\n",
      "Train Iter:  176  Loss:  1.2605171985924244\n",
      "Train Iter:  177  Loss:  1.2615717827937023\n",
      "Train Iter:  178  Loss:  1.261704242296433\n",
      "Train Iter:  179  Loss:  1.2616693537328496\n",
      "Train Iter:  180  Loss:  1.2622838248809178\n",
      "Train Iter:  181  Loss:  1.2625688870967422\n",
      "Train Iter:  182  Loss:  1.2628881128934712\n",
      "Train Iter:  183  Loss:  1.2632458030851812\n",
      "Train Iter:  184  Loss:  1.2652146424288335\n",
      "Train Iter:  185  Loss:  1.2653133060481097\n",
      "Train Iter:  186  Loss:  1.263958078879182\n",
      "Val Iter: Loss:  1.2334986925125122\n",
      "Epoch 15, Mean Train Loss: 1.263958078879182, Val Loss (1 sample): 1.2334986925125122\n",
      "Train Iter:  1  Loss:  1.2813704013824463\n",
      "Train Iter:  2  Loss:  1.2185500264167786\n",
      "Train Iter:  3  Loss:  1.2588553428649902\n",
      "Train Iter:  4  Loss:  1.265115201473236\n",
      "Train Iter:  5  Loss:  1.261437964439392\n",
      "Train Iter:  6  Loss:  1.2525248726209004\n",
      "Train Iter:  7  Loss:  1.2243563107081823\n",
      "Train Iter:  8  Loss:  1.2195689678192139\n",
      "Train Iter:  9  Loss:  1.2277697722117107\n",
      "Train Iter:  10  Loss:  1.2184033393859863\n",
      "Train Iter:  11  Loss:  1.20832406390797\n",
      "Train Iter:  12  Loss:  1.2248453895250957\n",
      "Train Iter:  13  Loss:  1.2477708137952364\n",
      "Train Iter:  14  Loss:  1.2716678380966187\n",
      "Train Iter:  15  Loss:  1.2605898141860963\n",
      "Train Iter:  16  Loss:  1.2691512629389763\n",
      "Train Iter:  17  Loss:  1.264369901488809\n",
      "Train Iter:  18  Loss:  1.2642899288071527\n",
      "Train Iter:  19  Loss:  1.2639813109448081\n",
      "Train Iter:  20  Loss:  1.255765187740326\n",
      "Train Iter:  21  Loss:  1.2485661336353846\n",
      "Train Iter:  22  Loss:  1.2611686316403476\n",
      "Train Iter:  23  Loss:  1.2691546160241831\n",
      "Train Iter:  24  Loss:  1.2680192788441975\n",
      "Train Iter:  25  Loss:  1.2591050863265991\n",
      "Train Iter:  26  Loss:  1.2621775773855357\n",
      "Train Iter:  27  Loss:  1.2543769942389593\n",
      "Train Iter:  28  Loss:  1.269989481994084\n",
      "Train Iter:  29  Loss:  1.2665241504537648\n",
      "Train Iter:  30  Loss:  1.2628763755162558\n",
      "Train Iter:  31  Loss:  1.2760640075129848\n",
      "Train Iter:  32  Loss:  1.274321623146534\n",
      "Train Iter:  33  Loss:  1.2761575236464993\n",
      "Train Iter:  34  Loss:  1.279706225675695\n",
      "Train Iter:  35  Loss:  1.2701179044587272\n",
      "Train Iter:  36  Loss:  1.2725772145721648\n",
      "Train Iter:  37  Loss:  1.2737266324661873\n",
      "Train Iter:  38  Loss:  1.2735980890299146\n",
      "Train Iter:  39  Loss:  1.2741704552601545\n",
      "Train Iter:  40  Loss:  1.2773816242814064\n",
      "Train Iter:  41  Loss:  1.2712856923661582\n",
      "Train Iter:  42  Loss:  1.2697549164295197\n",
      "Train Iter:  43  Loss:  1.2722531681837037\n",
      "Train Iter:  44  Loss:  1.2708330628546802\n",
      "Train Iter:  45  Loss:  1.265997302532196\n",
      "Train Iter:  46  Loss:  1.2641173536362855\n",
      "Train Iter:  47  Loss:  1.2680417996771791\n",
      "Train Iter:  48  Loss:  1.2650906778872013\n",
      "Train Iter:  49  Loss:  1.2663384785457534\n",
      "Train Iter:  50  Loss:  1.2655397474765777\n",
      "Train Iter:  51  Loss:  1.2655663548731337\n",
      "Train Iter:  52  Loss:  1.2656807589989443\n",
      "Train Iter:  53  Loss:  1.280657045121463\n",
      "Train Iter:  54  Loss:  1.2822450388360906\n",
      "Train Iter:  55  Loss:  1.2746167172085154\n",
      "Train Iter:  56  Loss:  1.2776099305067743\n",
      "Train Iter:  57  Loss:  1.2756516358308625\n",
      "Train Iter:  58  Loss:  1.2755808737771264\n",
      "Train Iter:  59  Loss:  1.2733456977343156\n",
      "Train Iter:  60  Loss:  1.2748470475276312\n",
      "Train Iter:  61  Loss:  1.273320701278624\n",
      "Train Iter:  62  Loss:  1.2737998952788692\n",
      "Train Iter:  63  Loss:  1.280439453465598\n",
      "Train Iter:  64  Loss:  1.2842361805960536\n",
      "Train Iter:  65  Loss:  1.282325934446775\n",
      "Train Iter:  66  Loss:  1.2825687934051861\n",
      "Train Iter:  67  Loss:  1.283125549110014\n",
      "Train Iter:  68  Loss:  1.2844341300866182\n",
      "Train Iter:  69  Loss:  1.290391795013262\n",
      "Train Iter:  70  Loss:  1.2909828875746046\n",
      "Train Iter:  71  Loss:  1.2924952448253901\n",
      "Train Iter:  72  Loss:  1.2959378767344687\n",
      "Train Iter:  73  Loss:  1.293146790706948\n",
      "Train Iter:  74  Loss:  1.2915747818109151\n",
      "Train Iter:  75  Loss:  1.285098175207774\n",
      "Train Iter:  76  Loss:  1.2874558701326972\n",
      "Train Iter:  77  Loss:  1.2840790833745683\n",
      "Train Iter:  78  Loss:  1.2838382514623494\n",
      "Train Iter:  79  Loss:  1.2842770218849182\n",
      "Train Iter:  80  Loss:  1.2831253759562968\n",
      "Train Iter:  81  Loss:  1.2821409253426541\n",
      "Train Iter:  82  Loss:  1.2836649337919748\n",
      "Train Iter:  83  Loss:  1.2873442338173648\n",
      "Train Iter:  84  Loss:  1.286022655311085\n",
      "Train Iter:  85  Loss:  1.285664103311651\n",
      "Train Iter:  86  Loss:  1.2892356203045956\n",
      "Train Iter:  87  Loss:  1.2898327023133465\n",
      "Train Iter:  88  Loss:  1.29039146209305\n",
      "Train Iter:  89  Loss:  1.2879678513226884\n",
      "Train Iter:  90  Loss:  1.2848113305038875\n",
      "Train Iter:  91  Loss:  1.2872638866141601\n",
      "Train Iter:  92  Loss:  1.2865557456793992\n",
      "Train Iter:  93  Loss:  1.2851722554493976\n",
      "Train Iter:  94  Loss:  1.2825998353197219\n",
      "Train Iter:  95  Loss:  1.2830299973487853\n",
      "Train Iter:  96  Loss:  1.2842237930744886\n",
      "Train Iter:  97  Loss:  1.2805130666064233\n",
      "Train Iter:  98  Loss:  1.2808629323025138\n",
      "Train Iter:  99  Loss:  1.280906657979946\n",
      "Train Iter:  100  Loss:  1.281832902431488\n",
      "Train Iter:  101  Loss:  1.2814757387236793\n",
      "Train Iter:  102  Loss:  1.2807954584850985\n",
      "Train Iter:  103  Loss:  1.280590764527182\n",
      "Train Iter:  104  Loss:  1.2802121260991464\n",
      "Train Iter:  105  Loss:  1.2786970513207572\n",
      "Train Iter:  106  Loss:  1.2763578385676977\n",
      "Train Iter:  107  Loss:  1.278740478453235\n",
      "Train Iter:  108  Loss:  1.275879595014784\n",
      "Train Iter:  109  Loss:  1.2774707713258375\n",
      "Train Iter:  110  Loss:  1.2749050097032026\n",
      "Train Iter:  111  Loss:  1.2744927621102549\n",
      "Train Iter:  112  Loss:  1.2724942096642085\n",
      "Train Iter:  113  Loss:  1.2737942564803941\n",
      "Train Iter:  114  Loss:  1.2763212563698751\n",
      "Train Iter:  115  Loss:  1.279320914848991\n",
      "Train Iter:  116  Loss:  1.2792035176836212\n",
      "Train Iter:  117  Loss:  1.2798082451535087\n",
      "Train Iter:  118  Loss:  1.2777632656743971\n",
      "Train Iter:  119  Loss:  1.2776041111024488\n",
      "Train Iter:  120  Loss:  1.2772019237279892\n",
      "Train Iter:  121  Loss:  1.2759857709742775\n",
      "Train Iter:  122  Loss:  1.273408309846628\n",
      "Train Iter:  123  Loss:  1.277007170324403\n",
      "Train Iter:  124  Loss:  1.2766182783149904\n",
      "Train Iter:  125  Loss:  1.2755526309013367\n",
      "Train Iter:  126  Loss:  1.2773992556428153\n",
      "Train Iter:  127  Loss:  1.2764288060308442\n",
      "Train Iter:  128  Loss:  1.2759283944033086\n",
      "Train Iter:  129  Loss:  1.274234726909519\n",
      "Train Iter:  130  Loss:  1.273370971587988\n",
      "Train Iter:  131  Loss:  1.2748144887785875\n",
      "Train Iter:  132  Loss:  1.2756573896516452\n",
      "Train Iter:  133  Loss:  1.2768109718659766\n",
      "Train Iter:  134  Loss:  1.2783586280559427\n",
      "Train Iter:  135  Loss:  1.2777856910670244\n",
      "Train Iter:  136  Loss:  1.2760566161835896\n",
      "Train Iter:  137  Loss:  1.2759025823460879\n",
      "Train Iter:  138  Loss:  1.2731655533762947\n",
      "Train Iter:  139  Loss:  1.2729873888784176\n",
      "Train Iter:  140  Loss:  1.2708571902343204\n",
      "Train Iter:  141  Loss:  1.2701686764439792\n",
      "Train Iter:  142  Loss:  1.2742542070402225\n",
      "Train Iter:  143  Loss:  1.2764082480143835\n",
      "Train Iter:  144  Loss:  1.2752270930343204\n",
      "Train Iter:  145  Loss:  1.274705456043112\n",
      "Train Iter:  146  Loss:  1.2758072892280474\n",
      "Train Iter:  147  Loss:  1.277955485039017\n",
      "Train Iter:  148  Loss:  1.276418583618628\n",
      "Train Iter:  149  Loss:  1.276441204467876\n",
      "Train Iter:  150  Loss:  1.2785080099105834\n",
      "Train Iter:  151  Loss:  1.2788417410376847\n",
      "Train Iter:  152  Loss:  1.2798042305205997\n",
      "Train Iter:  153  Loss:  1.279502990978216\n",
      "Train Iter:  154  Loss:  1.2788139767461009\n",
      "Train Iter:  155  Loss:  1.276882533104189\n",
      "Train Iter:  156  Loss:  1.2762771462782836\n",
      "Train Iter:  157  Loss:  1.2776742065028779\n",
      "Train Iter:  158  Loss:  1.2784717867646036\n",
      "Train Iter:  159  Loss:  1.278266042283496\n",
      "Train Iter:  160  Loss:  1.2775423973798752\n",
      "Train Iter:  161  Loss:  1.2779006995029332\n",
      "Train Iter:  162  Loss:  1.2773013490217704\n",
      "Train Iter:  163  Loss:  1.276465204595788\n",
      "Train Iter:  164  Loss:  1.2751369948794202\n",
      "Train Iter:  165  Loss:  1.276979111180161\n",
      "Train Iter:  166  Loss:  1.2775610649442097\n",
      "Train Iter:  167  Loss:  1.2762580596044393\n",
      "Train Iter:  168  Loss:  1.2749667174759365\n",
      "Train Iter:  169  Loss:  1.2720643990843958\n",
      "Train Iter:  170  Loss:  1.2725776802091038\n",
      "Train Iter:  171  Loss:  1.2701597367113793\n",
      "Train Iter:  172  Loss:  1.2702454356260078\n",
      "Train Iter:  173  Loss:  1.2702945084930155\n",
      "Train Iter:  174  Loss:  1.2713563497039093\n",
      "Train Iter:  175  Loss:  1.2730151666913714\n",
      "Train Iter:  176  Loss:  1.272488216107542\n",
      "Train Iter:  177  Loss:  1.2720577757237321\n",
      "Train Iter:  178  Loss:  1.2743460321694278\n",
      "Train Iter:  179  Loss:  1.273433021326971\n",
      "Train Iter:  180  Loss:  1.273399371571011\n",
      "Train Iter:  181  Loss:  1.2734788293996568\n",
      "Train Iter:  182  Loss:  1.2759516835212708\n",
      "Train Iter:  183  Loss:  1.277157870146746\n",
      "Train Iter:  184  Loss:  1.2774249062589977\n",
      "Train Iter:  185  Loss:  1.2787055647051013\n",
      "Train Iter:  186  Loss:  1.2768217309187817\n",
      "Val Iter: Loss:  0.9614443182945251\n",
      "Epoch 16, Mean Train Loss: 1.2768217309187817, Val Loss (1 sample): 0.9614443182945251\n",
      "Train Iter:  1  Loss:  1.3321974277496338\n",
      "Train Iter:  2  Loss:  1.2333496809005737\n",
      "Train Iter:  3  Loss:  1.130423088868459\n",
      "Train Iter:  4  Loss:  1.1368193477392197\n",
      "Train Iter:  5  Loss:  1.1576770424842835\n",
      "Train Iter:  6  Loss:  1.1489018897215526\n",
      "Train Iter:  7  Loss:  1.1753833208765303\n",
      "Train Iter:  8  Loss:  1.183133564889431\n",
      "Train Iter:  9  Loss:  1.145137561692132\n",
      "Train Iter:  10  Loss:  1.1424747467041017\n",
      "Train Iter:  11  Loss:  1.1351992325349287\n",
      "Train Iter:  12  Loss:  1.135809342066447\n",
      "Train Iter:  13  Loss:  1.1574791761545034\n",
      "Train Iter:  14  Loss:  1.1761429565293449\n",
      "Train Iter:  15  Loss:  1.183189868927002\n",
      "Train Iter:  16  Loss:  1.1697266437113285\n",
      "Train Iter:  17  Loss:  1.1709441472502315\n",
      "Train Iter:  18  Loss:  1.1708013382222917\n",
      "Train Iter:  19  Loss:  1.1938271114700718\n",
      "Train Iter:  20  Loss:  1.1908391147851944\n",
      "Train Iter:  21  Loss:  1.2052866589455378\n",
      "Train Iter:  22  Loss:  1.2091801139441403\n",
      "Train Iter:  23  Loss:  1.2128818579342053\n",
      "Train Iter:  24  Loss:  1.210406554241975\n",
      "Train Iter:  25  Loss:  1.2120575547218322\n",
      "Train Iter:  26  Loss:  1.2225481294668639\n",
      "Train Iter:  27  Loss:  1.2161264971450523\n",
      "Train Iter:  28  Loss:  1.2055806581463133\n",
      "Train Iter:  29  Loss:  1.2081352862818489\n",
      "Train Iter:  30  Loss:  1.2121804813543955\n",
      "Train Iter:  31  Loss:  1.233630686037002\n",
      "Train Iter:  32  Loss:  1.229477958753705\n",
      "Train Iter:  33  Loss:  1.2352541811538464\n",
      "Train Iter:  34  Loss:  1.2396303713321686\n",
      "Train Iter:  35  Loss:  1.2417623264448983\n",
      "Train Iter:  36  Loss:  1.2370874600278006\n",
      "Train Iter:  37  Loss:  1.2343413362631928\n",
      "Train Iter:  38  Loss:  1.2369736448714608\n",
      "Train Iter:  39  Loss:  1.2402808956610851\n",
      "Train Iter:  40  Loss:  1.236166076362133\n",
      "Train Iter:  41  Loss:  1.232030393146887\n",
      "Train Iter:  42  Loss:  1.2378008578504835\n",
      "Train Iter:  43  Loss:  1.244803058546643\n",
      "Train Iter:  44  Loss:  1.2543042396957225\n",
      "Train Iter:  45  Loss:  1.251619072755178\n",
      "Train Iter:  46  Loss:  1.2510765855726989\n",
      "Train Iter:  47  Loss:  1.2517985828379368\n",
      "Train Iter:  48  Loss:  1.2474508446951706\n",
      "Train Iter:  49  Loss:  1.250699251281972\n",
      "Train Iter:  50  Loss:  1.2490752232074738\n",
      "Train Iter:  51  Loss:  1.246392407838036\n",
      "Train Iter:  52  Loss:  1.2529575469402165\n",
      "Train Iter:  53  Loss:  1.2513603298169262\n",
      "Train Iter:  54  Loss:  1.2532384649471\n",
      "Train Iter:  55  Loss:  1.2617382602258163\n",
      "Train Iter:  56  Loss:  1.2632617663059915\n",
      "Train Iter:  57  Loss:  1.2717954432755185\n",
      "Train Iter:  58  Loss:  1.2766379785948787\n",
      "Train Iter:  59  Loss:  1.2789492071685145\n",
      "Train Iter:  60  Loss:  1.2787209620078406\n",
      "Train Iter:  61  Loss:  1.2841894812271244\n",
      "Train Iter:  62  Loss:  1.2810255894737859\n",
      "Train Iter:  63  Loss:  1.2787654655320304\n",
      "Train Iter:  64  Loss:  1.2803179854527116\n",
      "Train Iter:  65  Loss:  1.2815793835199796\n",
      "Train Iter:  66  Loss:  1.2849449163133448\n",
      "Train Iter:  67  Loss:  1.283825976635093\n",
      "Train Iter:  68  Loss:  1.2842549339813345\n",
      "Train Iter:  69  Loss:  1.281898520131042\n",
      "Train Iter:  70  Loss:  1.285563725233078\n",
      "Train Iter:  71  Loss:  1.2832129060382573\n",
      "Train Iter:  72  Loss:  1.2846116398771603\n",
      "Train Iter:  73  Loss:  1.2858401169515636\n",
      "Train Iter:  74  Loss:  1.2879428307752352\n",
      "Train Iter:  75  Loss:  1.2874846005439757\n",
      "Train Iter:  76  Loss:  1.2875552326440811\n",
      "Train Iter:  77  Loss:  1.2880006386088085\n",
      "Train Iter:  78  Loss:  1.2931607113434718\n",
      "Train Iter:  79  Loss:  1.2958897721918323\n",
      "Train Iter:  80  Loss:  1.296165766566992\n",
      "Train Iter:  81  Loss:  1.2905174677754625\n",
      "Train Iter:  82  Loss:  1.2893606649666298\n",
      "Train Iter:  83  Loss:  1.2907474349780255\n",
      "Train Iter:  84  Loss:  1.2913775947831927\n",
      "Train Iter:  85  Loss:  1.2931162672884324\n",
      "Train Iter:  86  Loss:  1.2896045654319053\n",
      "Train Iter:  87  Loss:  1.2919051537568542\n",
      "Train Iter:  88  Loss:  1.2916958792643114\n",
      "Train Iter:  89  Loss:  1.2964120433571633\n",
      "Train Iter:  90  Loss:  1.3011493351724412\n",
      "Train Iter:  91  Loss:  1.301485734981495\n",
      "Train Iter:  92  Loss:  1.300448100204053\n",
      "Train Iter:  93  Loss:  1.3008879166777416\n",
      "Train Iter:  94  Loss:  1.3005638794696077\n",
      "Train Iter:  95  Loss:  1.2976785898208618\n",
      "Train Iter:  96  Loss:  1.3013883084058762\n",
      "Train Iter:  97  Loss:  1.295728721569494\n",
      "Train Iter:  98  Loss:  1.2962150719701027\n",
      "Train Iter:  99  Loss:  1.2939680489626797\n",
      "Train Iter:  100  Loss:  1.2925094318389894\n",
      "Train Iter:  101  Loss:  1.290244995957554\n",
      "Train Iter:  102  Loss:  1.2885805099618202\n",
      "Train Iter:  103  Loss:  1.290597353166747\n",
      "Train Iter:  104  Loss:  1.289863415635549\n",
      "Train Iter:  105  Loss:  1.2882056735810778\n",
      "Train Iter:  106  Loss:  1.2864519458896708\n",
      "Train Iter:  107  Loss:  1.28814873182885\n",
      "Train Iter:  108  Loss:  1.2912927943247337\n",
      "Train Iter:  109  Loss:  1.294484341910126\n",
      "Train Iter:  110  Loss:  1.2948903972452337\n",
      "Train Iter:  111  Loss:  1.2943861301954802\n",
      "Train Iter:  112  Loss:  1.2953490519097872\n",
      "Train Iter:  113  Loss:  1.2962398708394143\n",
      "Train Iter:  114  Loss:  1.294397092702096\n",
      "Train Iter:  115  Loss:  1.2925787158634352\n",
      "Train Iter:  116  Loss:  1.2918175428078091\n",
      "Train Iter:  117  Loss:  1.290097251916543\n",
      "Train Iter:  118  Loss:  1.289834668070583\n",
      "Train Iter:  119  Loss:  1.2950563160311275\n",
      "Train Iter:  120  Loss:  1.2953855673472086\n",
      "Train Iter:  121  Loss:  1.2968123057657037\n",
      "Train Iter:  122  Loss:  1.2946990190959367\n",
      "Train Iter:  123  Loss:  1.2932825321104469\n",
      "Train Iter:  124  Loss:  1.29299989246553\n",
      "Train Iter:  125  Loss:  1.2918111705780029\n",
      "Train Iter:  126  Loss:  1.2900519796780177\n",
      "Train Iter:  127  Loss:  1.2907963277786736\n",
      "Train Iter:  128  Loss:  1.2891353014856577\n",
      "Train Iter:  129  Loss:  1.2932099270266155\n",
      "Train Iter:  130  Loss:  1.2908525554033425\n",
      "Train Iter:  131  Loss:  1.2906535877526262\n",
      "Train Iter:  132  Loss:  1.291508667848327\n",
      "Train Iter:  133  Loss:  1.292051160694065\n",
      "Train Iter:  134  Loss:  1.291060735485447\n",
      "Train Iter:  135  Loss:  1.2914204310487818\n",
      "Train Iter:  136  Loss:  1.292868728585103\n",
      "Train Iter:  137  Loss:  1.2949482746367906\n",
      "Train Iter:  138  Loss:  1.2947723023269488\n",
      "Train Iter:  139  Loss:  1.2961655790857274\n",
      "Train Iter:  140  Loss:  1.2960504084825515\n",
      "Train Iter:  141  Loss:  1.2981204948526748\n",
      "Train Iter:  142  Loss:  1.2978589832782745\n",
      "Train Iter:  143  Loss:  1.2961884541111393\n",
      "Train Iter:  144  Loss:  1.2943486732741196\n",
      "Train Iter:  145  Loss:  1.2937568898858696\n",
      "Train Iter:  146  Loss:  1.2936617385034692\n",
      "Train Iter:  147  Loss:  1.2916707088347195\n",
      "Train Iter:  148  Loss:  1.2910867909321915\n",
      "Train Iter:  149  Loss:  1.2920298084316637\n",
      "Train Iter:  150  Loss:  1.294946753581365\n",
      "Train Iter:  151  Loss:  1.293602182770407\n",
      "Train Iter:  152  Loss:  1.2917946306498427\n",
      "Train Iter:  153  Loss:  1.2918986514502881\n",
      "Train Iter:  154  Loss:  1.293991490230932\n",
      "Train Iter:  155  Loss:  1.2958030112328067\n",
      "Train Iter:  156  Loss:  1.2939711155799718\n",
      "Train Iter:  157  Loss:  1.292751891977468\n",
      "Train Iter:  158  Loss:  1.2932201371162753\n",
      "Train Iter:  159  Loss:  1.292557066716488\n",
      "Train Iter:  160  Loss:  1.291686574742198\n",
      "Train Iter:  161  Loss:  1.2917064520883264\n",
      "Train Iter:  162  Loss:  1.2917599350581934\n",
      "Train Iter:  163  Loss:  1.2919859589974574\n",
      "Train Iter:  164  Loss:  1.2925911556656768\n",
      "Train Iter:  165  Loss:  1.2935307043971438\n",
      "Train Iter:  166  Loss:  1.2935554101524582\n",
      "Train Iter:  167  Loss:  1.2946400396124331\n",
      "Train Iter:  168  Loss:  1.292794862673396\n",
      "Train Iter:  169  Loss:  1.292710112396782\n",
      "Train Iter:  170  Loss:  1.2917442132444943\n",
      "Train Iter:  171  Loss:  1.294834177396451\n",
      "Train Iter:  172  Loss:  1.294076257666876\n",
      "Train Iter:  173  Loss:  1.2938861743563173\n",
      "Train Iter:  174  Loss:  1.2936247594055088\n",
      "Train Iter:  175  Loss:  1.2924627467564174\n",
      "Train Iter:  176  Loss:  1.2933191243897786\n",
      "Train Iter:  177  Loss:  1.2921166507537756\n",
      "Train Iter:  178  Loss:  1.2919596426942375\n",
      "Train Iter:  179  Loss:  1.293568850895546\n",
      "Train Iter:  180  Loss:  1.2942408084869386\n",
      "Train Iter:  181  Loss:  1.2958500846314824\n",
      "Train Iter:  182  Loss:  1.2947059935265846\n",
      "Train Iter:  183  Loss:  1.2951524752736743\n",
      "Train Iter:  184  Loss:  1.2967931550482046\n",
      "Train Iter:  185  Loss:  1.2951963424682618\n",
      "Train Iter:  186  Loss:  1.2966429059223463\n",
      "Val Iter: Loss:  1.361361026763916\n",
      "Epoch 17, Mean Train Loss: 1.2966429059223463, Val Loss (1 sample): 1.361361026763916\n",
      "Train Iter:  1  Loss:  1.275854468345642\n",
      "Train Iter:  2  Loss:  1.3335411548614502\n",
      "Train Iter:  3  Loss:  1.2782184680302937\n",
      "Train Iter:  4  Loss:  1.2502054572105408\n",
      "Train Iter:  5  Loss:  1.247330403327942\n",
      "Train Iter:  6  Loss:  1.2353413899739583\n",
      "Train Iter:  7  Loss:  1.231124418122428\n",
      "Train Iter:  8  Loss:  1.2163692116737366\n",
      "Train Iter:  9  Loss:  1.1957532564798992\n",
      "Train Iter:  10  Loss:  1.189539861679077\n",
      "Train Iter:  11  Loss:  1.179731303995306\n",
      "Train Iter:  12  Loss:  1.1707399586836498\n",
      "Train Iter:  13  Loss:  1.1684460915051973\n",
      "Train Iter:  14  Loss:  1.176589846611023\n",
      "Train Iter:  15  Loss:  1.2012826601664226\n",
      "Train Iter:  16  Loss:  1.1952259689569473\n",
      "Train Iter:  17  Loss:  1.2130827272639555\n",
      "Train Iter:  18  Loss:  1.2233043048116896\n",
      "Train Iter:  19  Loss:  1.225737496426231\n",
      "Train Iter:  20  Loss:  1.2273309171199798\n",
      "Train Iter:  21  Loss:  1.217185133979434\n",
      "Train Iter:  22  Loss:  1.2200061895630576\n",
      "Train Iter:  23  Loss:  1.2170201073522153\n",
      "Train Iter:  24  Loss:  1.20928455889225\n",
      "Train Iter:  25  Loss:  1.2178212308883667\n",
      "Train Iter:  26  Loss:  1.2364038091439467\n",
      "Train Iter:  27  Loss:  1.2452879658451788\n",
      "Train Iter:  28  Loss:  1.2472788648945945\n",
      "Train Iter:  29  Loss:  1.2464982764474277\n",
      "Train Iter:  30  Loss:  1.2469305038452148\n",
      "Train Iter:  31  Loss:  1.2417028219469133\n",
      "Train Iter:  32  Loss:  1.2405147403478622\n",
      "Train Iter:  33  Loss:  1.243276509371671\n",
      "Train Iter:  34  Loss:  1.2599976483513327\n",
      "Train Iter:  35  Loss:  1.2582032101494924\n",
      "Train Iter:  36  Loss:  1.2607381873660617\n",
      "Train Iter:  37  Loss:  1.255492645340997\n",
      "Train Iter:  38  Loss:  1.254935057539689\n",
      "Train Iter:  39  Loss:  1.2597772372074616\n",
      "Train Iter:  40  Loss:  1.2620564848184586\n",
      "Train Iter:  41  Loss:  1.2612449076117538\n",
      "Train Iter:  42  Loss:  1.2576198748179845\n",
      "Train Iter:  43  Loss:  1.2608358582784964\n",
      "Train Iter:  44  Loss:  1.255790802565488\n",
      "Train Iter:  45  Loss:  1.2594793902503119\n",
      "Train Iter:  46  Loss:  1.254479239816251\n",
      "Train Iter:  47  Loss:  1.2536677756208054\n",
      "Train Iter:  48  Loss:  1.2461252460877101\n",
      "Train Iter:  49  Loss:  1.2426086810170387\n",
      "Train Iter:  50  Loss:  1.2451265597343444\n",
      "Train Iter:  51  Loss:  1.2483061739042693\n",
      "Train Iter:  52  Loss:  1.2468968217189496\n",
      "Train Iter:  53  Loss:  1.2454920984664053\n",
      "Train Iter:  54  Loss:  1.2427098839371293\n",
      "Train Iter:  55  Loss:  1.2415049531243063\n",
      "Train Iter:  56  Loss:  1.2422170809337072\n",
      "Train Iter:  57  Loss:  1.2388860070914554\n",
      "Train Iter:  58  Loss:  1.2415320215554073\n",
      "Train Iter:  59  Loss:  1.2427822250430867\n",
      "Train Iter:  60  Loss:  1.2407893896102906\n",
      "Train Iter:  61  Loss:  1.2423354543623377\n",
      "Train Iter:  62  Loss:  1.238692049057253\n",
      "Train Iter:  63  Loss:  1.2421690679731823\n",
      "Train Iter:  64  Loss:  1.2401589397341013\n",
      "Train Iter:  65  Loss:  1.2368080671016985\n",
      "Train Iter:  66  Loss:  1.235127461679054\n",
      "Train Iter:  67  Loss:  1.2346120396656777\n",
      "Train Iter:  68  Loss:  1.2402509783997255\n",
      "Train Iter:  69  Loss:  1.240809288577757\n",
      "Train Iter:  70  Loss:  1.2412915229797363\n",
      "Train Iter:  71  Loss:  1.2465712990559323\n",
      "Train Iter:  72  Loss:  1.2519350267118878\n",
      "Train Iter:  73  Loss:  1.2537838530867067\n",
      "Train Iter:  74  Loss:  1.2561408651841652\n",
      "Train Iter:  75  Loss:  1.2536725473403931\n",
      "Train Iter:  76  Loss:  1.252525801721372\n",
      "Train Iter:  77  Loss:  1.260212467862414\n",
      "Train Iter:  78  Loss:  1.2599181425877106\n",
      "Train Iter:  79  Loss:  1.2597840813141834\n",
      "Train Iter:  80  Loss:  1.2616488680243492\n",
      "Train Iter:  81  Loss:  1.2613477795212358\n",
      "Train Iter:  82  Loss:  1.260682768938018\n",
      "Train Iter:  83  Loss:  1.2559283763529308\n",
      "Train Iter:  84  Loss:  1.2543829331795375\n",
      "Train Iter:  85  Loss:  1.251450880134807\n",
      "Train Iter:  86  Loss:  1.2513606305732283\n",
      "Train Iter:  87  Loss:  1.2490100867446812\n",
      "Train Iter:  88  Loss:  1.2515004812316461\n",
      "Train Iter:  89  Loss:  1.252327012881804\n",
      "Train Iter:  90  Loss:  1.2528614170021481\n",
      "Train Iter:  91  Loss:  1.2520202928847008\n",
      "Train Iter:  92  Loss:  1.2551304626723994\n",
      "Train Iter:  93  Loss:  1.256728325479774\n",
      "Train Iter:  94  Loss:  1.2585103404014668\n",
      "Train Iter:  95  Loss:  1.25959779526058\n",
      "Train Iter:  96  Loss:  1.2586870696395636\n",
      "Train Iter:  97  Loss:  1.2615430631588416\n",
      "Train Iter:  98  Loss:  1.2613055164716682\n",
      "Train Iter:  99  Loss:  1.2558212280273438\n",
      "Train Iter:  100  Loss:  1.2557829225063324\n",
      "Train Iter:  101  Loss:  1.2569842043489512\n",
      "Train Iter:  102  Loss:  1.2548507942872889\n",
      "Train Iter:  103  Loss:  1.2511169927791483\n",
      "Train Iter:  104  Loss:  1.248572582235703\n",
      "Train Iter:  105  Loss:  1.2445436403864907\n",
      "Train Iter:  106  Loss:  1.2443942242073562\n",
      "Train Iter:  107  Loss:  1.2462805136341915\n",
      "Train Iter:  108  Loss:  1.2502978107443563\n",
      "Train Iter:  109  Loss:  1.2515072532750051\n",
      "Train Iter:  110  Loss:  1.250926594842564\n",
      "Train Iter:  111  Loss:  1.2545112580866427\n",
      "Train Iter:  112  Loss:  1.253534335643053\n",
      "Train Iter:  113  Loss:  1.2523679527561222\n",
      "Train Iter:  114  Loss:  1.251113645340267\n",
      "Train Iter:  115  Loss:  1.2525147982265639\n",
      "Train Iter:  116  Loss:  1.2567890178540657\n",
      "Train Iter:  117  Loss:  1.2534252532526977\n",
      "Train Iter:  118  Loss:  1.252494896367445\n",
      "Train Iter:  119  Loss:  1.255342340769888\n",
      "Train Iter:  120  Loss:  1.253141634662946\n",
      "Train Iter:  121  Loss:  1.2527052952238351\n",
      "Train Iter:  122  Loss:  1.2529603975718138\n",
      "Train Iter:  123  Loss:  1.2538904775448931\n",
      "Train Iter:  124  Loss:  1.2509552451872057\n",
      "Train Iter:  125  Loss:  1.2501124238967896\n",
      "Train Iter:  126  Loss:  1.2481757109127347\n",
      "Train Iter:  127  Loss:  1.250438195513928\n",
      "Train Iter:  128  Loss:  1.2484371238388121\n",
      "Train Iter:  129  Loss:  1.247505754925484\n",
      "Train Iter:  130  Loss:  1.2499380125449253\n",
      "Train Iter:  131  Loss:  1.2478759589086053\n",
      "Train Iter:  132  Loss:  1.2488538505453053\n",
      "Train Iter:  133  Loss:  1.2510654496071034\n",
      "Train Iter:  134  Loss:  1.2555381059646606\n",
      "Train Iter:  135  Loss:  1.2561350478066338\n",
      "Train Iter:  136  Loss:  1.2545941997976864\n",
      "Train Iter:  137  Loss:  1.2547970079157473\n",
      "Train Iter:  138  Loss:  1.2542719936025315\n",
      "Train Iter:  139  Loss:  1.2541965966602024\n",
      "Train Iter:  140  Loss:  1.2522317435060228\n",
      "Train Iter:  141  Loss:  1.2527981176443979\n",
      "Train Iter:  142  Loss:  1.2537829028048986\n",
      "Train Iter:  143  Loss:  1.254975381431046\n",
      "Train Iter:  144  Loss:  1.253644444876247\n",
      "Train Iter:  145  Loss:  1.2535112644064015\n",
      "Train Iter:  146  Loss:  1.254311540355421\n",
      "Train Iter:  147  Loss:  1.2549319153740293\n",
      "Train Iter:  148  Loss:  1.2556719731640171\n",
      "Train Iter:  149  Loss:  1.2550090143344546\n",
      "Train Iter:  150  Loss:  1.255467992623647\n",
      "Train Iter:  151  Loss:  1.2583176244963084\n",
      "Train Iter:  152  Loss:  1.259739315039233\n",
      "Train Iter:  153  Loss:  1.2589485138849494\n",
      "Train Iter:  154  Loss:  1.2579783007696077\n",
      "Train Iter:  155  Loss:  1.2568758849174746\n",
      "Train Iter:  156  Loss:  1.2583811955574231\n",
      "Train Iter:  157  Loss:  1.2617180605602871\n",
      "Train Iter:  158  Loss:  1.2610694811313967\n",
      "Train Iter:  159  Loss:  1.2614756672637268\n",
      "Train Iter:  160  Loss:  1.2616527482867241\n",
      "Train Iter:  161  Loss:  1.2592483525690825\n",
      "Train Iter:  162  Loss:  1.258439517683453\n",
      "Train Iter:  163  Loss:  1.258987487825148\n",
      "Train Iter:  164  Loss:  1.2623365466914527\n",
      "Train Iter:  165  Loss:  1.2613971005786548\n",
      "Train Iter:  166  Loss:  1.261680095669735\n",
      "Train Iter:  167  Loss:  1.2611731701268407\n",
      "Train Iter:  168  Loss:  1.2590245813840912\n",
      "Train Iter:  169  Loss:  1.2577952347563568\n",
      "Train Iter:  170  Loss:  1.2566127913839678\n",
      "Train Iter:  171  Loss:  1.258046663644021\n",
      "Train Iter:  172  Loss:  1.257138505578041\n",
      "Train Iter:  173  Loss:  1.2573472127059981\n",
      "Train Iter:  174  Loss:  1.2583458913468766\n",
      "Train Iter:  175  Loss:  1.257608747141702\n",
      "Train Iter:  176  Loss:  1.2569505609571934\n",
      "Train Iter:  177  Loss:  1.2569813071671179\n",
      "Train Iter:  178  Loss:  1.2567256792877497\n",
      "Train Iter:  179  Loss:  1.2568349162293546\n",
      "Train Iter:  180  Loss:  1.2555589930878746\n",
      "Train Iter:  181  Loss:  1.2556997152323222\n",
      "Train Iter:  182  Loss:  1.2563336416260227\n",
      "Train Iter:  183  Loss:  1.2574155933218576\n",
      "Train Iter:  184  Loss:  1.2575763532001039\n",
      "Train Iter:  185  Loss:  1.2577174260809614\n",
      "Train Iter:  186  Loss:  1.2568038442442495\n",
      "Val Iter: Loss:  1.0084353685379028\n",
      "Epoch 18, Mean Train Loss: 1.2568038442442495, Val Loss (1 sample): 1.0084353685379028\n",
      "Train Iter:  1  Loss:  1.281955599784851\n",
      "Train Iter:  2  Loss:  1.2192733883857727\n",
      "Train Iter:  3  Loss:  1.2584826151529949\n",
      "Train Iter:  4  Loss:  1.2419412434101105\n",
      "Train Iter:  5  Loss:  1.2722025156021117\n",
      "Train Iter:  6  Loss:  1.3280453284581502\n",
      "Train Iter:  7  Loss:  1.3233052151543754\n",
      "Train Iter:  8  Loss:  1.2950332164764404\n",
      "Train Iter:  9  Loss:  1.2680886321597629\n",
      "Train Iter:  10  Loss:  1.2397084355354309\n",
      "Train Iter:  11  Loss:  1.2400894923643633\n",
      "Train Iter:  12  Loss:  1.2344805002212524\n",
      "Train Iter:  13  Loss:  1.245593080153832\n",
      "Train Iter:  14  Loss:  1.2495217067854745\n",
      "Train Iter:  15  Loss:  1.2444283962249756\n",
      "Train Iter:  16  Loss:  1.2362142652273178\n",
      "Train Iter:  17  Loss:  1.2321013352450203\n",
      "Train Iter:  18  Loss:  1.2190742095311482\n",
      "Train Iter:  19  Loss:  1.2111360085637946\n",
      "Train Iter:  20  Loss:  1.2415759861469269\n",
      "Train Iter:  21  Loss:  1.2431998479933966\n",
      "Train Iter:  22  Loss:  1.2386714111674915\n",
      "Train Iter:  23  Loss:  1.235791538072669\n",
      "Train Iter:  24  Loss:  1.2330270210901897\n",
      "Train Iter:  25  Loss:  1.2253176879882812\n",
      "Train Iter:  26  Loss:  1.2348104394399202\n",
      "Train Iter:  27  Loss:  1.2291044526629977\n",
      "Train Iter:  28  Loss:  1.2163038083485194\n",
      "Train Iter:  29  Loss:  1.2171438644672263\n",
      "Train Iter:  30  Loss:  1.2149959365526835\n",
      "Train Iter:  31  Loss:  1.2161949565333705\n",
      "Train Iter:  32  Loss:  1.217238336801529\n",
      "Train Iter:  33  Loss:  1.21247999234633\n",
      "Train Iter:  34  Loss:  1.2207140606992386\n",
      "Train Iter:  35  Loss:  1.2235428673880442\n",
      "Train Iter:  36  Loss:  1.209313389327791\n",
      "Train Iter:  37  Loss:  1.2049722897039878\n",
      "Train Iter:  38  Loss:  1.216111879599722\n",
      "Train Iter:  39  Loss:  1.2162631077644153\n",
      "Train Iter:  40  Loss:  1.2123527467250823\n",
      "Train Iter:  41  Loss:  1.2047868516387008\n",
      "Train Iter:  42  Loss:  1.2068221554869698\n",
      "Train Iter:  43  Loss:  1.2128839894782666\n",
      "Train Iter:  44  Loss:  1.2163437455892563\n",
      "Train Iter:  45  Loss:  1.2151836382018195\n",
      "Train Iter:  46  Loss:  1.2115000602991686\n",
      "Train Iter:  47  Loss:  1.2176558121721794\n",
      "Train Iter:  48  Loss:  1.2168142137428124\n",
      "Train Iter:  49  Loss:  1.219148965514436\n",
      "Train Iter:  50  Loss:  1.2165174210071563\n",
      "Train Iter:  51  Loss:  1.215194467236014\n",
      "Train Iter:  52  Loss:  1.2155461070629268\n",
      "Train Iter:  53  Loss:  1.2190091171354618\n",
      "Train Iter:  54  Loss:  1.222514608392009\n",
      "Train Iter:  55  Loss:  1.2221365115859293\n",
      "Train Iter:  56  Loss:  1.2201831394008227\n",
      "Train Iter:  57  Loss:  1.2248251427683914\n",
      "Train Iter:  58  Loss:  1.2210396148007492\n",
      "Train Iter:  59  Loss:  1.2231885871644748\n",
      "Train Iter:  60  Loss:  1.2276023199160895\n",
      "Train Iter:  61  Loss:  1.2268812470748776\n",
      "Train Iter:  62  Loss:  1.2243521953782728\n",
      "Train Iter:  63  Loss:  1.2214462501662118\n",
      "Train Iter:  64  Loss:  1.221922685392201\n",
      "Train Iter:  65  Loss:  1.2267741542596085\n",
      "Train Iter:  66  Loss:  1.2238656002463717\n",
      "Train Iter:  67  Loss:  1.2234645788349323\n",
      "Train Iter:  68  Loss:  1.2196534249712438\n",
      "Train Iter:  69  Loss:  1.2234878963318423\n",
      "Train Iter:  70  Loss:  1.2279032971177781\n",
      "Train Iter:  71  Loss:  1.2254881464259726\n",
      "Train Iter:  72  Loss:  1.2241842970252037\n",
      "Train Iter:  73  Loss:  1.2287770828155622\n",
      "Train Iter:  74  Loss:  1.226085061157072\n",
      "Train Iter:  75  Loss:  1.2270084897677103\n",
      "Train Iter:  76  Loss:  1.2258410853774924\n",
      "Train Iter:  77  Loss:  1.2285871946966493\n",
      "Train Iter:  78  Loss:  1.2290845276453557\n",
      "Train Iter:  79  Loss:  1.2297213869758798\n",
      "Train Iter:  80  Loss:  1.235469850152731\n",
      "Train Iter:  81  Loss:  1.2356131790596763\n",
      "Train Iter:  82  Loss:  1.2342522020747022\n",
      "Train Iter:  83  Loss:  1.22996557836073\n",
      "Train Iter:  84  Loss:  1.2282585054636002\n",
      "Train Iter:  85  Loss:  1.2266014667118297\n",
      "Train Iter:  86  Loss:  1.2247908676779546\n",
      "Train Iter:  87  Loss:  1.2276698179628658\n",
      "Train Iter:  88  Loss:  1.2295698435469107\n",
      "Train Iter:  89  Loss:  1.2294958220439012\n",
      "Train Iter:  90  Loss:  1.2316753645737966\n",
      "Train Iter:  91  Loss:  1.2343116271626817\n",
      "Train Iter:  92  Loss:  1.233980211874713\n",
      "Train Iter:  93  Loss:  1.2347436162733263\n",
      "Train Iter:  94  Loss:  1.234168455321738\n",
      "Train Iter:  95  Loss:  1.2345130198880245\n",
      "Train Iter:  96  Loss:  1.2334487171222766\n",
      "Train Iter:  97  Loss:  1.238666656705522\n",
      "Train Iter:  98  Loss:  1.2351933766384513\n",
      "Train Iter:  99  Loss:  1.2374776529543328\n",
      "Train Iter:  100  Loss:  1.2375191509723664\n",
      "Train Iter:  101  Loss:  1.2332462715630483\n",
      "Train Iter:  102  Loss:  1.2332305353061825\n",
      "Train Iter:  103  Loss:  1.2305774312574886\n",
      "Train Iter:  104  Loss:  1.2309108829269042\n",
      "Train Iter:  105  Loss:  1.232565148103805\n",
      "Train Iter:  106  Loss:  1.2327213607869059\n",
      "Train Iter:  107  Loss:  1.2339999458500157\n",
      "Train Iter:  108  Loss:  1.2331615846466135\n",
      "Train Iter:  109  Loss:  1.2341646114620595\n",
      "Train Iter:  110  Loss:  1.2348458512262865\n",
      "Train Iter:  111  Loss:  1.2343925393379487\n",
      "Train Iter:  112  Loss:  1.2368943143103803\n",
      "Train Iter:  113  Loss:  1.2336839959684727\n",
      "Train Iter:  114  Loss:  1.2332842240208073\n",
      "Train Iter:  115  Loss:  1.2334403115770092\n",
      "Train Iter:  116  Loss:  1.2304223190093864\n",
      "Train Iter:  117  Loss:  1.2288600593550592\n",
      "Train Iter:  118  Loss:  1.227325369745998\n",
      "Train Iter:  119  Loss:  1.2285089993677218\n",
      "Train Iter:  120  Loss:  1.2319556017716726\n",
      "Train Iter:  121  Loss:  1.2322462994205066\n",
      "Train Iter:  122  Loss:  1.2306500429012737\n",
      "Train Iter:  123  Loss:  1.2309575381317759\n",
      "Train Iter:  124  Loss:  1.2318919964375035\n",
      "Train Iter:  125  Loss:  1.2337932691574096\n",
      "Train Iter:  126  Loss:  1.2347196624392556\n",
      "Train Iter:  127  Loss:  1.2342084843342698\n",
      "Train Iter:  128  Loss:  1.2339293211698532\n",
      "Train Iter:  129  Loss:  1.235283005145169\n",
      "Train Iter:  130  Loss:  1.2356009620886583\n",
      "Train Iter:  131  Loss:  1.2357301839435373\n",
      "Train Iter:  132  Loss:  1.2358830345399452\n",
      "Train Iter:  133  Loss:  1.2341515112640267\n",
      "Train Iter:  134  Loss:  1.2378062739301083\n",
      "Train Iter:  135  Loss:  1.2384403476008663\n",
      "Train Iter:  136  Loss:  1.237436880083645\n",
      "Train Iter:  137  Loss:  1.2373915466949017\n",
      "Train Iter:  138  Loss:  1.2353029808272487\n",
      "Train Iter:  139  Loss:  1.2359360177739918\n",
      "Train Iter:  140  Loss:  1.2364647588559559\n",
      "Train Iter:  141  Loss:  1.234760843270214\n",
      "Train Iter:  142  Loss:  1.2358616691240123\n",
      "Train Iter:  143  Loss:  1.2381616689108468\n",
      "Train Iter:  144  Loss:  1.2393425471252866\n",
      "Train Iter:  145  Loss:  1.2415144410626642\n",
      "Train Iter:  146  Loss:  1.239536609143427\n",
      "Train Iter:  147  Loss:  1.2407323669414132\n",
      "Train Iter:  148  Loss:  1.2425222851940103\n",
      "Train Iter:  149  Loss:  1.2435232356890735\n",
      "Train Iter:  150  Loss:  1.2470539677143098\n",
      "Train Iter:  151  Loss:  1.2462613894449954\n",
      "Train Iter:  152  Loss:  1.243935231707598\n",
      "Train Iter:  153  Loss:  1.2414787941508822\n",
      "Train Iter:  154  Loss:  1.240142544755688\n",
      "Train Iter:  155  Loss:  1.2386450709835175\n",
      "Train Iter:  156  Loss:  1.2366025482232754\n",
      "Train Iter:  157  Loss:  1.2369042828584174\n",
      "Train Iter:  158  Loss:  1.2367727307579186\n",
      "Train Iter:  159  Loss:  1.2381054339168955\n",
      "Train Iter:  160  Loss:  1.2392479445785285\n",
      "Train Iter:  161  Loss:  1.238771034693866\n",
      "Train Iter:  162  Loss:  1.237580864885707\n",
      "Train Iter:  163  Loss:  1.2389033802447875\n",
      "Train Iter:  164  Loss:  1.2389257764670907\n",
      "Train Iter:  165  Loss:  1.238390701828581\n",
      "Train Iter:  166  Loss:  1.2384078908397491\n",
      "Train Iter:  167  Loss:  1.2396728717638348\n",
      "Train Iter:  168  Loss:  1.2400403395295143\n",
      "Train Iter:  169  Loss:  1.2428712961236401\n",
      "Train Iter:  170  Loss:  1.2421729624271394\n",
      "Train Iter:  171  Loss:  1.2395477678343567\n",
      "Train Iter:  172  Loss:  1.2366766118726065\n",
      "Train Iter:  173  Loss:  1.2370649162744511\n",
      "Train Iter:  174  Loss:  1.2373306552569072\n",
      "Train Iter:  175  Loss:  1.237888525554112\n",
      "Train Iter:  176  Loss:  1.2409476664933292\n",
      "Train Iter:  177  Loss:  1.2415970765938193\n",
      "Train Iter:  178  Loss:  1.241166323758243\n",
      "Train Iter:  179  Loss:  1.240558993882973\n",
      "Train Iter:  180  Loss:  1.2424351255098978\n",
      "Train Iter:  181  Loss:  1.2411002908622362\n",
      "Train Iter:  182  Loss:  1.242200290763771\n",
      "Train Iter:  183  Loss:  1.245533429208349\n",
      "Train Iter:  184  Loss:  1.2453002249417098\n",
      "Train Iter:  185  Loss:  1.2456577771418804\n",
      "Train Iter:  186  Loss:  1.2451413869857788\n",
      "Val Iter: Loss:  1.3823691606521606\n",
      "Epoch 19, Mean Train Loss: 1.2451413869857788, Val Loss (1 sample): 1.3823691606521606\n",
      "Train Iter:  1  Loss:  1.3654296398162842\n",
      "Train Iter:  2  Loss:  1.230049967765808\n",
      "Train Iter:  3  Loss:  1.3984259764353435\n",
      "Train Iter:  4  Loss:  1.4142021238803864\n",
      "Train Iter:  5  Loss:  1.4408554077148437\n",
      "Train Iter:  6  Loss:  1.3681028286616008\n",
      "Train Iter:  7  Loss:  1.3628919465201241\n",
      "Train Iter:  8  Loss:  1.3290278762578964\n",
      "Train Iter:  9  Loss:  1.361271169450548\n",
      "Train Iter:  10  Loss:  1.340265142917633\n",
      "Train Iter:  11  Loss:  1.3167561184276233\n",
      "Train Iter:  12  Loss:  1.3007308145364125\n",
      "Train Iter:  13  Loss:  1.289943502499507\n",
      "Train Iter:  14  Loss:  1.2876381703785487\n",
      "Train Iter:  15  Loss:  1.270142428080241\n",
      "Train Iter:  16  Loss:  1.267469361424446\n",
      "Train Iter:  17  Loss:  1.2947483693852144\n",
      "Train Iter:  18  Loss:  1.2898155318366156\n",
      "Train Iter:  19  Loss:  1.296756254999261\n",
      "Train Iter:  20  Loss:  1.2991632103919983\n",
      "Train Iter:  21  Loss:  1.2857883771260579\n",
      "Train Iter:  22  Loss:  1.2779727144674822\n",
      "Train Iter:  23  Loss:  1.2900577835414722\n",
      "Train Iter:  24  Loss:  1.2880109051863353\n",
      "Train Iter:  25  Loss:  1.289566626548767\n",
      "Train Iter:  26  Loss:  1.2969756401502168\n",
      "Train Iter:  27  Loss:  1.3035775202291984\n",
      "Train Iter:  28  Loss:  1.2963393117700304\n",
      "Train Iter:  29  Loss:  1.2899010222533653\n",
      "Train Iter:  30  Loss:  1.288193690776825\n",
      "Train Iter:  31  Loss:  1.2761928208412663\n",
      "Train Iter:  32  Loss:  1.2750801462680101\n",
      "Train Iter:  33  Loss:  1.2805264050310308\n",
      "Train Iter:  34  Loss:  1.2801690013969647\n",
      "Train Iter:  35  Loss:  1.2747665933200292\n",
      "Train Iter:  36  Loss:  1.2687385297483869\n",
      "Train Iter:  37  Loss:  1.2702074872480857\n",
      "Train Iter:  38  Loss:  1.2630674870390641\n",
      "Train Iter:  39  Loss:  1.2662343275852692\n",
      "Train Iter:  40  Loss:  1.2731998711824417\n",
      "Train Iter:  41  Loss:  1.2689180054315707\n",
      "Train Iter:  42  Loss:  1.2625868490764074\n",
      "Train Iter:  43  Loss:  1.260132490202438\n",
      "Train Iter:  44  Loss:  1.2608628435568376\n",
      "Train Iter:  45  Loss:  1.2619119538201227\n",
      "Train Iter:  46  Loss:  1.263701565887617\n",
      "Train Iter:  47  Loss:  1.2632351555722825\n",
      "Train Iter:  48  Loss:  1.2611943110823631\n",
      "Train Iter:  49  Loss:  1.2661740682563003\n",
      "Train Iter:  50  Loss:  1.272091534137726\n",
      "Train Iter:  51  Loss:  1.2689586246714872\n",
      "Train Iter:  52  Loss:  1.2680557645284212\n",
      "Train Iter:  53  Loss:  1.2615989997701824\n",
      "Train Iter:  54  Loss:  1.2588046800207209\n",
      "Train Iter:  55  Loss:  1.2559625333005733\n",
      "Train Iter:  56  Loss:  1.2557330269898688\n",
      "Train Iter:  57  Loss:  1.2606388424572192\n",
      "Train Iter:  58  Loss:  1.2656042010619724\n",
      "Train Iter:  59  Loss:  1.2621301160020344\n",
      "Train Iter:  60  Loss:  1.2582689573367436\n",
      "Train Iter:  61  Loss:  1.2655291997018407\n",
      "Train Iter:  62  Loss:  1.2684968642650112\n",
      "Train Iter:  63  Loss:  1.2638351009005593\n",
      "Train Iter:  64  Loss:  1.264734698459506\n",
      "Train Iter:  65  Loss:  1.2637374217693622\n",
      "Train Iter:  66  Loss:  1.2645395705194185\n",
      "Train Iter:  67  Loss:  1.2616356362157792\n",
      "Train Iter:  68  Loss:  1.258395461475148\n",
      "Train Iter:  69  Loss:  1.2555621637814287\n",
      "Train Iter:  70  Loss:  1.2537618807383946\n",
      "Train Iter:  71  Loss:  1.2575699829719436\n",
      "Train Iter:  72  Loss:  1.2603441774845123\n",
      "Train Iter:  73  Loss:  1.2594876420007992\n",
      "Train Iter:  74  Loss:  1.2565922882105853\n",
      "Train Iter:  75  Loss:  1.2562479241689046\n",
      "Train Iter:  76  Loss:  1.2547355482452793\n",
      "Train Iter:  77  Loss:  1.252914885421852\n",
      "Train Iter:  78  Loss:  1.2538868861320691\n",
      "Train Iter:  79  Loss:  1.257055237323423\n",
      "Train Iter:  80  Loss:  1.258047179877758\n",
      "Train Iter:  81  Loss:  1.2509130357224265\n",
      "Train Iter:  82  Loss:  1.2532649316438815\n",
      "Train Iter:  83  Loss:  1.2580748624112232\n",
      "Train Iter:  84  Loss:  1.2593993941942851\n",
      "Train Iter:  85  Loss:  1.2547299328972312\n",
      "Train Iter:  86  Loss:  1.2570307947868524\n",
      "Train Iter:  87  Loss:  1.2576501986076092\n",
      "Train Iter:  88  Loss:  1.2587242681871762\n",
      "Train Iter:  89  Loss:  1.2604606298918135\n",
      "Train Iter:  90  Loss:  1.2600564373864067\n",
      "Train Iter:  91  Loss:  1.2581826736638835\n",
      "Train Iter:  92  Loss:  1.26017241763032\n",
      "Train Iter:  93  Loss:  1.2594596211628248\n",
      "Train Iter:  94  Loss:  1.259322910866839\n",
      "Train Iter:  95  Loss:  1.260605109365363\n",
      "Train Iter:  96  Loss:  1.2585619054734707\n",
      "Train Iter:  97  Loss:  1.2577248258689016\n",
      "Train Iter:  98  Loss:  1.2569077574476903\n",
      "Train Iter:  99  Loss:  1.2589783933427598\n",
      "Train Iter:  100  Loss:  1.2611732268333435\n",
      "Train Iter:  101  Loss:  1.2656081466391536\n",
      "Train Iter:  102  Loss:  1.267131268978119\n",
      "Train Iter:  103  Loss:  1.2636571607543428\n",
      "Train Iter:  104  Loss:  1.2625845378408065\n",
      "Train Iter:  105  Loss:  1.2621826949573698\n",
      "Train Iter:  106  Loss:  1.2611015185994923\n",
      "Train Iter:  107  Loss:  1.2600440304970073\n",
      "Train Iter:  108  Loss:  1.2601818635507867\n",
      "Train Iter:  109  Loss:  1.2607630562344823\n",
      "Train Iter:  110  Loss:  1.2594992209564555\n",
      "Train Iter:  111  Loss:  1.25651789087433\n",
      "Train Iter:  112  Loss:  1.2558025244091238\n",
      "Train Iter:  113  Loss:  1.2553642265564573\n",
      "Train Iter:  114  Loss:  1.2535446511026014\n",
      "Train Iter:  115  Loss:  1.2537784830383631\n",
      "Train Iter:  116  Loss:  1.2527669673335964\n",
      "Train Iter:  117  Loss:  1.254574994246165\n",
      "Train Iter:  118  Loss:  1.2562682603375386\n",
      "Train Iter:  119  Loss:  1.254550119908918\n",
      "Train Iter:  120  Loss:  1.2544750864307086\n",
      "Train Iter:  121  Loss:  1.2549159196782704\n",
      "Train Iter:  122  Loss:  1.253730856492871\n",
      "Train Iter:  123  Loss:  1.2511706017866366\n",
      "Train Iter:  124  Loss:  1.2537052991890139\n",
      "Train Iter:  125  Loss:  1.2527166819572448\n",
      "Train Iter:  126  Loss:  1.2529008923068878\n",
      "Train Iter:  127  Loss:  1.2533153825857508\n",
      "Train Iter:  128  Loss:  1.2552470755763352\n",
      "Train Iter:  129  Loss:  1.2555642437565235\n",
      "Train Iter:  130  Loss:  1.2549839849655444\n",
      "Train Iter:  131  Loss:  1.2538999514725373\n",
      "Train Iter:  132  Loss:  1.2537597539750012\n",
      "Train Iter:  133  Loss:  1.2506290696617355\n",
      "Train Iter:  134  Loss:  1.2532240085637392\n",
      "Train Iter:  135  Loss:  1.254635096920861\n",
      "Train Iter:  136  Loss:  1.25320135451415\n",
      "Train Iter:  137  Loss:  1.2567230858942018\n",
      "Train Iter:  138  Loss:  1.2568683948205865\n",
      "Train Iter:  139  Loss:  1.2585045369408971\n",
      "Train Iter:  140  Loss:  1.2575873302561895\n",
      "Train Iter:  141  Loss:  1.2592247635760205\n",
      "Train Iter:  142  Loss:  1.2591510175819127\n",
      "Train Iter:  143  Loss:  1.260187319525472\n",
      "Train Iter:  144  Loss:  1.2613600794639852\n",
      "Train Iter:  145  Loss:  1.2592749295563532\n",
      "Train Iter:  146  Loss:  1.2604870424695211\n",
      "Train Iter:  147  Loss:  1.2596704120538673\n",
      "Train Iter:  148  Loss:  1.2610251706194233\n",
      "Train Iter:  149  Loss:  1.260935587770987\n",
      "Train Iter:  150  Loss:  1.2600906614462535\n",
      "Train Iter:  151  Loss:  1.2598470224449967\n",
      "Train Iter:  152  Loss:  1.257901462285142\n",
      "Train Iter:  153  Loss:  1.2598754409091923\n",
      "Train Iter:  154  Loss:  1.261166599663821\n",
      "Train Iter:  155  Loss:  1.260076742787515\n",
      "Train Iter:  156  Loss:  1.2597001966757653\n",
      "Train Iter:  157  Loss:  1.258112443480522\n",
      "Train Iter:  158  Loss:  1.2585684076140198\n",
      "Train Iter:  159  Loss:  1.2589060562961507\n",
      "Train Iter:  160  Loss:  1.2567775532603265\n",
      "Train Iter:  161  Loss:  1.25573552321203\n",
      "Train Iter:  162  Loss:  1.2564662650779441\n",
      "Train Iter:  163  Loss:  1.2556358034625374\n",
      "Train Iter:  164  Loss:  1.2577168367257932\n",
      "Train Iter:  165  Loss:  1.2586630445538145\n",
      "Train Iter:  166  Loss:  1.2595051124871495\n",
      "Train Iter:  167  Loss:  1.260048128887565\n",
      "Train Iter:  168  Loss:  1.260719826533681\n",
      "Train Iter:  169  Loss:  1.2607291605345596\n",
      "Train Iter:  170  Loss:  1.2630295101334066\n",
      "Train Iter:  171  Loss:  1.2628932319886503\n",
      "Train Iter:  172  Loss:  1.2627223515233328\n",
      "Train Iter:  173  Loss:  1.2621894061909935\n",
      "Train Iter:  174  Loss:  1.2634593563518306\n",
      "Train Iter:  175  Loss:  1.2623754957744053\n",
      "Train Iter:  176  Loss:  1.2626795402982018\n",
      "Train Iter:  177  Loss:  1.2631229067926353\n",
      "Train Iter:  178  Loss:  1.263711135708884\n",
      "Train Iter:  179  Loss:  1.2623027682970356\n",
      "Train Iter:  180  Loss:  1.2605156070656247\n",
      "Train Iter:  181  Loss:  1.2619745105669644\n",
      "Train Iter:  182  Loss:  1.2614269269691718\n",
      "Train Iter:  183  Loss:  1.261996936928379\n",
      "Train Iter:  184  Loss:  1.259478498088277\n",
      "Train Iter:  185  Loss:  1.2602613935599456\n",
      "Train Iter:  186  Loss:  1.2622740355230146\n",
      "Val Iter: Loss:  1.5136836767196655\n",
      "Epoch 20, Mean Train Loss: 1.2622740355230146, Val Loss (1 sample): 1.5136836767196655\n",
      "Train Iter:  1  Loss:  1.192690134048462\n",
      "Train Iter:  2  Loss:  1.2783539295196533\n",
      "Train Iter:  3  Loss:  1.213547984759013\n",
      "Train Iter:  4  Loss:  1.1913280487060547\n",
      "Train Iter:  5  Loss:  1.2188451051712037\n",
      "Train Iter:  6  Loss:  1.218834638595581\n",
      "Train Iter:  7  Loss:  1.257544892174857\n",
      "Train Iter:  8  Loss:  1.2399705946445465\n",
      "Train Iter:  9  Loss:  1.2633202738232083\n",
      "Train Iter:  10  Loss:  1.3035349130630494\n",
      "Train Iter:  11  Loss:  1.3292573473670266\n",
      "Train Iter:  12  Loss:  1.358277012904485\n",
      "Train Iter:  13  Loss:  1.3577416585041926\n",
      "Train Iter:  14  Loss:  1.3421531404767717\n",
      "Train Iter:  15  Loss:  1.3156199177106223\n",
      "Train Iter:  16  Loss:  1.2967723496258259\n",
      "Train Iter:  17  Loss:  1.2927085757255554\n",
      "Train Iter:  18  Loss:  1.3008981711334653\n",
      "Train Iter:  19  Loss:  1.3081332162806862\n",
      "Train Iter:  20  Loss:  1.3271643668413162\n",
      "Train Iter:  21  Loss:  1.30934096518017\n",
      "Train Iter:  22  Loss:  1.3276640014214949\n",
      "Train Iter:  23  Loss:  1.3215705311816672\n",
      "Train Iter:  24  Loss:  1.3102851510047913\n",
      "Train Iter:  25  Loss:  1.3199389362335205\n",
      "Train Iter:  26  Loss:  1.310730021733504\n",
      "Train Iter:  27  Loss:  1.302605982179995\n",
      "Train Iter:  28  Loss:  1.3128684163093567\n",
      "Train Iter:  29  Loss:  1.3083598572632362\n",
      "Train Iter:  30  Loss:  1.310133445262909\n",
      "Train Iter:  31  Loss:  1.313215132682554\n",
      "Train Iter:  32  Loss:  1.3089402988553047\n",
      "Train Iter:  33  Loss:  1.3060745506575613\n",
      "Train Iter:  34  Loss:  1.2962527064716114\n",
      "Train Iter:  35  Loss:  1.2983493634632655\n",
      "Train Iter:  36  Loss:  1.2938054303328197\n",
      "Train Iter:  37  Loss:  1.287473752691939\n",
      "Train Iter:  38  Loss:  1.2881659363445483\n",
      "Train Iter:  39  Loss:  1.292481624163114\n",
      "Train Iter:  40  Loss:  1.2899603307247163\n",
      "Train Iter:  41  Loss:  1.2854456581720493\n",
      "Train Iter:  42  Loss:  1.2851448541595822\n",
      "Train Iter:  43  Loss:  1.2885828877604284\n",
      "Train Iter:  44  Loss:  1.2858504517511888\n",
      "Train Iter:  45  Loss:  1.2853872511121962\n",
      "Train Iter:  46  Loss:  1.2794874025427776\n",
      "Train Iter:  47  Loss:  1.2717012205022447\n",
      "Train Iter:  48  Loss:  1.2777506870528061\n",
      "Train Iter:  49  Loss:  1.2775775376631289\n",
      "Train Iter:  50  Loss:  1.277190009355545\n",
      "Train Iter:  51  Loss:  1.2815147159146327\n",
      "Train Iter:  52  Loss:  1.2772461416629644\n",
      "Train Iter:  53  Loss:  1.2812913082680613\n",
      "Train Iter:  54  Loss:  1.282149291700787\n",
      "Train Iter:  55  Loss:  1.2825181039896878\n",
      "Train Iter:  56  Loss:  1.2837679290345736\n",
      "Train Iter:  57  Loss:  1.2859189123438115\n",
      "Train Iter:  58  Loss:  1.285315066576004\n",
      "Train Iter:  59  Loss:  1.2811148944547621\n",
      "Train Iter:  60  Loss:  1.2840612202882766\n",
      "Train Iter:  61  Loss:  1.2864462440131141\n",
      "Train Iter:  62  Loss:  1.2907371857473928\n",
      "Train Iter:  63  Loss:  1.292945130476876\n",
      "Train Iter:  64  Loss:  1.292929102666676\n",
      "Train Iter:  65  Loss:  1.2885592047984784\n",
      "Train Iter:  66  Loss:  1.2837180097897847\n",
      "Train Iter:  67  Loss:  1.2861362962580438\n",
      "Train Iter:  68  Loss:  1.294900827548083\n",
      "Train Iter:  69  Loss:  1.2968259583348813\n",
      "Train Iter:  70  Loss:  1.2957751767975942\n",
      "Train Iter:  71  Loss:  1.2918840260572837\n",
      "Train Iter:  72  Loss:  1.2940712455246184\n",
      "Train Iter:  73  Loss:  1.2956534428139255\n",
      "Train Iter:  74  Loss:  1.291967823698714\n",
      "Train Iter:  75  Loss:  1.2933499463399252\n",
      "Train Iter:  76  Loss:  1.2971106516687494\n",
      "Train Iter:  77  Loss:  1.297533400647052\n",
      "Train Iter:  78  Loss:  1.2992619306613238\n",
      "Train Iter:  79  Loss:  1.2977292507509641\n",
      "Train Iter:  80  Loss:  1.2963378325104713\n",
      "Train Iter:  81  Loss:  1.2979718461448764\n",
      "Train Iter:  82  Loss:  1.2944799312731115\n",
      "Train Iter:  83  Loss:  1.2930267560912903\n",
      "Train Iter:  84  Loss:  1.2911989632106962\n",
      "Train Iter:  85  Loss:  1.2923267378526575\n",
      "Train Iter:  86  Loss:  1.2912626405094945\n",
      "Train Iter:  87  Loss:  1.2904024288572113\n",
      "Train Iter:  88  Loss:  1.2886892597783695\n",
      "Train Iter:  89  Loss:  1.2883671897180964\n",
      "Train Iter:  90  Loss:  1.2913845207956103\n",
      "Train Iter:  91  Loss:  1.2891532628090827\n",
      "Train Iter:  92  Loss:  1.2899188166079314\n",
      "Train Iter:  93  Loss:  1.2916319716361262\n",
      "Train Iter:  94  Loss:  1.288537414784127\n",
      "Train Iter:  95  Loss:  1.2901132847133436\n",
      "Train Iter:  96  Loss:  1.2899132122596104\n",
      "Train Iter:  97  Loss:  1.2878133193733765\n",
      "Train Iter:  98  Loss:  1.287349253284688\n",
      "Train Iter:  99  Loss:  1.2850743760966292\n",
      "Train Iter:  100  Loss:  1.2837278592586516\n",
      "Train Iter:  101  Loss:  1.2842904730598526\n",
      "Train Iter:  102  Loss:  1.285912944990046\n",
      "Train Iter:  103  Loss:  1.2842552094783597\n",
      "Train Iter:  104  Loss:  1.2823771960460222\n",
      "Train Iter:  105  Loss:  1.2812038625989641\n",
      "Train Iter:  106  Loss:  1.2808013074802902\n",
      "Train Iter:  107  Loss:  1.2819457399510892\n",
      "Train Iter:  108  Loss:  1.2817750815992002\n",
      "Train Iter:  109  Loss:  1.279590954474353\n",
      "Train Iter:  110  Loss:  1.279222180626609\n",
      "Train Iter:  111  Loss:  1.2799021639265455\n",
      "Train Iter:  112  Loss:  1.2826194635459356\n",
      "Train Iter:  113  Loss:  1.2807722471456613\n",
      "Train Iter:  114  Loss:  1.2803697366463511\n",
      "Train Iter:  115  Loss:  1.2797892601593681\n",
      "Train Iter:  116  Loss:  1.2797655874285205\n",
      "Train Iter:  117  Loss:  1.2780940583628466\n",
      "Train Iter:  118  Loss:  1.277014070648258\n",
      "Train Iter:  119  Loss:  1.2755916078551477\n",
      "Train Iter:  120  Loss:  1.2736735035975775\n",
      "Train Iter:  121  Loss:  1.2750734957781704\n",
      "Train Iter:  122  Loss:  1.273597652794885\n",
      "Train Iter:  123  Loss:  1.2759018293241176\n",
      "Train Iter:  124  Loss:  1.2742867787038126\n",
      "Train Iter:  125  Loss:  1.2752293462753297\n",
      "Train Iter:  126  Loss:  1.2749073051270985\n",
      "Train Iter:  127  Loss:  1.2742301516645536\n",
      "Train Iter:  128  Loss:  1.2733864709734917\n",
      "Train Iter:  129  Loss:  1.2738406149915946\n",
      "Train Iter:  130  Loss:  1.2770054129453805\n",
      "Train Iter:  131  Loss:  1.2769377495496328\n",
      "Train Iter:  132  Loss:  1.2766496087565566\n",
      "Train Iter:  133  Loss:  1.2754888713807988\n",
      "Train Iter:  134  Loss:  1.2781880373385415\n",
      "Train Iter:  135  Loss:  1.279851785412541\n",
      "Train Iter:  136  Loss:  1.279215874917367\n",
      "Train Iter:  137  Loss:  1.2771813260377758\n",
      "Train Iter:  138  Loss:  1.2756033988966458\n",
      "Train Iter:  139  Loss:  1.2753899106018836\n",
      "Train Iter:  140  Loss:  1.2754698864051275\n",
      "Train Iter:  141  Loss:  1.2766955127107336\n",
      "Train Iter:  142  Loss:  1.278599971616772\n",
      "Train Iter:  143  Loss:  1.277454687998845\n",
      "Train Iter:  144  Loss:  1.2780711601177852\n",
      "Train Iter:  145  Loss:  1.279113296804757\n",
      "Train Iter:  146  Loss:  1.2774705298959392\n",
      "Train Iter:  147  Loss:  1.2769544692266555\n",
      "Train Iter:  148  Loss:  1.2768068434418858\n",
      "Train Iter:  149  Loss:  1.275806050172588\n",
      "Train Iter:  150  Loss:  1.275540690422058\n",
      "Train Iter:  151  Loss:  1.2757866824699553\n",
      "Train Iter:  152  Loss:  1.2767063669468228\n",
      "Train Iter:  153  Loss:  1.2768189642164443\n",
      "Train Iter:  154  Loss:  1.2770394079096905\n",
      "Train Iter:  155  Loss:  1.2761942486609181\n",
      "Train Iter:  156  Loss:  1.2760992722633557\n",
      "Train Iter:  157  Loss:  1.2756448300780765\n",
      "Train Iter:  158  Loss:  1.2756671799889094\n",
      "Train Iter:  159  Loss:  1.2791530253752224\n",
      "Train Iter:  160  Loss:  1.2777910187840462\n",
      "Train Iter:  161  Loss:  1.2775280756979996\n",
      "Train Iter:  162  Loss:  1.2774263281881073\n",
      "Train Iter:  163  Loss:  1.2768228163748432\n",
      "Train Iter:  164  Loss:  1.2778254282183763\n",
      "Train Iter:  165  Loss:  1.2782851537068685\n",
      "Train Iter:  166  Loss:  1.2780957006546387\n",
      "Train Iter:  167  Loss:  1.2783161751524417\n",
      "Train Iter:  168  Loss:  1.279344196120898\n",
      "Train Iter:  169  Loss:  1.2779438432151748\n",
      "Train Iter:  170  Loss:  1.277502587262322\n",
      "Train Iter:  171  Loss:  1.276837506489447\n",
      "Train Iter:  172  Loss:  1.276478428480237\n",
      "Train Iter:  173  Loss:  1.279992390230212\n",
      "Train Iter:  174  Loss:  1.278869781000861\n",
      "Train Iter:  175  Loss:  1.2787516035352435\n",
      "Train Iter:  176  Loss:  1.2816490043293347\n",
      "Train Iter:  177  Loss:  1.2825375202685425\n",
      "Train Iter:  178  Loss:  1.2845080474789223\n",
      "Train Iter:  179  Loss:  1.285447371738583\n",
      "Train Iter:  180  Loss:  1.284919234779146\n",
      "Train Iter:  181  Loss:  1.2870776692806687\n",
      "Train Iter:  182  Loss:  1.2864255086406247\n",
      "Train Iter:  183  Loss:  1.2856132202461117\n",
      "Train Iter:  184  Loss:  1.2855724083340687\n",
      "Train Iter:  185  Loss:  1.2844888706465025\n",
      "Train Iter:  186  Loss:  1.2830012024089854\n",
      "Val Iter: Loss:  2.5524165630340576\n",
      "Epoch 21, Mean Train Loss: 1.2830012024089854, Val Loss (1 sample): 2.5524165630340576\n",
      "Train Iter:  1  Loss:  1.1758135557174683\n",
      "Train Iter:  2  Loss:  1.1453648805618286\n",
      "Train Iter:  3  Loss:  1.0494225025177002\n",
      "Train Iter:  4  Loss:  1.1156943142414093\n",
      "Train Iter:  5  Loss:  1.0759823203086853\n",
      "Train Iter:  6  Loss:  1.0793454547723134\n",
      "Train Iter:  7  Loss:  1.1238813144820077\n",
      "Train Iter:  8  Loss:  1.1293384060263634\n",
      "Train Iter:  9  Loss:  1.1506930126084223\n",
      "Train Iter:  10  Loss:  1.1423390924930572\n",
      "Train Iter:  11  Loss:  1.1581412282857029\n",
      "Train Iter:  12  Loss:  1.1604926536480586\n",
      "Train Iter:  13  Loss:  1.168225714793572\n",
      "Train Iter:  14  Loss:  1.1689344431672777\n",
      "Train Iter:  15  Loss:  1.1645172556241354\n",
      "Train Iter:  16  Loss:  1.1595087014138699\n",
      "Train Iter:  17  Loss:  1.152493234942941\n",
      "Train Iter:  18  Loss:  1.167669650581148\n",
      "Train Iter:  19  Loss:  1.162863182394128\n",
      "Train Iter:  20  Loss:  1.165709462761879\n",
      "Train Iter:  21  Loss:  1.155413309733073\n",
      "Train Iter:  22  Loss:  1.1539207155054265\n",
      "Train Iter:  23  Loss:  1.148826407349628\n",
      "Train Iter:  24  Loss:  1.1531051446994145\n",
      "Train Iter:  25  Loss:  1.1477753686904908\n",
      "Train Iter:  26  Loss:  1.1579937430528493\n",
      "Train Iter:  27  Loss:  1.1699835547694453\n",
      "Train Iter:  28  Loss:  1.17334959762437\n",
      "Train Iter:  29  Loss:  1.1811871323092231\n",
      "Train Iter:  30  Loss:  1.174023578564326\n",
      "Train Iter:  31  Loss:  1.1816019608128456\n",
      "Train Iter:  32  Loss:  1.1878013741225004\n",
      "Train Iter:  33  Loss:  1.1797643520615317\n",
      "Train Iter:  34  Loss:  1.1753914654254913\n",
      "Train Iter:  35  Loss:  1.1904870799609593\n",
      "Train Iter:  36  Loss:  1.2060026708576415\n",
      "Train Iter:  37  Loss:  1.2064322152653255\n",
      "Train Iter:  38  Loss:  1.2048750974630054\n",
      "Train Iter:  39  Loss:  1.2148295778494616\n",
      "Train Iter:  40  Loss:  1.213434399664402\n",
      "Train Iter:  41  Loss:  1.2217828864004554\n",
      "Train Iter:  42  Loss:  1.2189782531488509\n",
      "Train Iter:  43  Loss:  1.217048053131547\n",
      "Train Iter:  44  Loss:  1.2171773382208564\n",
      "Train Iter:  45  Loss:  1.2219072593583\n",
      "Train Iter:  46  Loss:  1.21637503867564\n",
      "Train Iter:  47  Loss:  1.2244554136661774\n",
      "Train Iter:  48  Loss:  1.227773026873668\n",
      "Train Iter:  49  Loss:  1.228348161493029\n",
      "Train Iter:  50  Loss:  1.227157965898514\n",
      "Train Iter:  51  Loss:  1.2209149973065245\n",
      "Train Iter:  52  Loss:  1.2141935573174403\n",
      "Train Iter:  53  Loss:  1.2121021455188967\n",
      "Train Iter:  54  Loss:  1.2091163440986916\n",
      "Train Iter:  55  Loss:  1.2036311745643615\n",
      "Train Iter:  56  Loss:  1.2015789246984891\n",
      "Train Iter:  57  Loss:  1.1962518284195347\n",
      "Train Iter:  58  Loss:  1.198060464242409\n",
      "Train Iter:  59  Loss:  1.1991503127550676\n",
      "Train Iter:  60  Loss:  1.1968726962804794\n",
      "Train Iter:  61  Loss:  1.20416273347667\n",
      "Train Iter:  62  Loss:  1.2044986726776246\n",
      "Train Iter:  63  Loss:  1.202785076603057\n",
      "Train Iter:  64  Loss:  1.205845563672483\n",
      "Train Iter:  65  Loss:  1.2025210756521958\n",
      "Train Iter:  66  Loss:  1.2027557519349186\n",
      "Train Iter:  67  Loss:  1.1992721949050675\n",
      "Train Iter:  68  Loss:  1.2019689240876366\n",
      "Train Iter:  69  Loss:  1.2021124121071636\n",
      "Train Iter:  70  Loss:  1.2019710506711687\n",
      "Train Iter:  71  Loss:  1.2086304359033073\n",
      "Train Iter:  72  Loss:  1.211970676978429\n",
      "Train Iter:  73  Loss:  1.210607843856289\n",
      "Train Iter:  74  Loss:  1.2195902982273616\n",
      "Train Iter:  75  Loss:  1.2178004503250122\n",
      "Train Iter:  76  Loss:  1.2192760313812054\n",
      "Train Iter:  77  Loss:  1.217925418506969\n",
      "Train Iter:  78  Loss:  1.219838912670429\n",
      "Train Iter:  79  Loss:  1.2202487218229077\n",
      "Train Iter:  80  Loss:  1.219039298593998\n",
      "Train Iter:  81  Loss:  1.2232722176445856\n",
      "Train Iter:  82  Loss:  1.2238480099817601\n",
      "Train Iter:  83  Loss:  1.2193486625889698\n",
      "Train Iter:  84  Loss:  1.2215688406001954\n",
      "Train Iter:  85  Loss:  1.2181672376744888\n",
      "Train Iter:  86  Loss:  1.2149881741335227\n",
      "Train Iter:  87  Loss:  1.216269470494369\n",
      "Train Iter:  88  Loss:  1.221343406899409\n",
      "Train Iter:  89  Loss:  1.2233272194862366\n",
      "Train Iter:  90  Loss:  1.2214586860603756\n",
      "Train Iter:  91  Loss:  1.2212863184593536\n",
      "Train Iter:  92  Loss:  1.2178836523190788\n",
      "Train Iter:  93  Loss:  1.2160460929716788\n",
      "Train Iter:  94  Loss:  1.2108360294331895\n",
      "Train Iter:  95  Loss:  1.2150613778515866\n",
      "Train Iter:  96  Loss:  1.2124806977808475\n",
      "Train Iter:  97  Loss:  1.2152525542937602\n",
      "Train Iter:  98  Loss:  1.2132978317688923\n",
      "Train Iter:  99  Loss:  1.2137482322827735\n",
      "Train Iter:  100  Loss:  1.213726238012314\n",
      "Train Iter:  101  Loss:  1.2144522171209353\n",
      "Train Iter:  102  Loss:  1.2174352652886336\n",
      "Train Iter:  103  Loss:  1.2196791715992308\n",
      "Train Iter:  104  Loss:  1.2203241942020564\n",
      "Train Iter:  105  Loss:  1.2212797096797399\n",
      "Train Iter:  106  Loss:  1.2191575315763366\n",
      "Train Iter:  107  Loss:  1.221765064747534\n",
      "Train Iter:  108  Loss:  1.2289545834064484\n",
      "Train Iter:  109  Loss:  1.2321269195014184\n",
      "Train Iter:  110  Loss:  1.2294759517366236\n",
      "Train Iter:  111  Loss:  1.2332448535137348\n",
      "Train Iter:  112  Loss:  1.2318470142781734\n",
      "Train Iter:  113  Loss:  1.2315128255734402\n",
      "Train Iter:  114  Loss:  1.2314916226947517\n",
      "Train Iter:  115  Loss:  1.2308694554411848\n",
      "Train Iter:  116  Loss:  1.2306624265580342\n",
      "Train Iter:  117  Loss:  1.2301293657376215\n",
      "Train Iter:  118  Loss:  1.2313973302558316\n",
      "Train Iter:  119  Loss:  1.2331915968606453\n",
      "Train Iter:  120  Loss:  1.2362467790643374\n",
      "Train Iter:  121  Loss:  1.2366337741702056\n",
      "Train Iter:  122  Loss:  1.2366045679225297\n",
      "Train Iter:  123  Loss:  1.236741458981987\n",
      "Train Iter:  124  Loss:  1.236682103045525\n",
      "Train Iter:  125  Loss:  1.2341865735054016\n",
      "Train Iter:  126  Loss:  1.2340082663392264\n",
      "Train Iter:  127  Loss:  1.2326005073044244\n",
      "Train Iter:  128  Loss:  1.2340461541898549\n",
      "Train Iter:  129  Loss:  1.2316254215647084\n",
      "Train Iter:  130  Loss:  1.2331262382177206\n",
      "Train Iter:  131  Loss:  1.2335656731183293\n",
      "Train Iter:  132  Loss:  1.2324355194966\n",
      "Train Iter:  133  Loss:  1.2313020995685033\n",
      "Train Iter:  134  Loss:  1.2322225459476015\n",
      "Train Iter:  135  Loss:  1.2345900142634356\n",
      "Train Iter:  136  Loss:  1.2334488538258217\n",
      "Train Iter:  137  Loss:  1.2335754619897716\n",
      "Train Iter:  138  Loss:  1.23383532702059\n",
      "Train Iter:  139  Loss:  1.2341713180644907\n",
      "Train Iter:  140  Loss:  1.2336941203900746\n",
      "Train Iter:  141  Loss:  1.2355404885102672\n",
      "Train Iter:  142  Loss:  1.2348511945193923\n",
      "Train Iter:  143  Loss:  1.2400162024097843\n",
      "Train Iter:  144  Loss:  1.2408525343570445\n",
      "Train Iter:  145  Loss:  1.2409374142515248\n",
      "Train Iter:  146  Loss:  1.2396288673355156\n",
      "Train Iter:  147  Loss:  1.2402614951133728\n",
      "Train Iter:  148  Loss:  1.243309325865797\n",
      "Train Iter:  149  Loss:  1.244314170923809\n",
      "Train Iter:  150  Loss:  1.2440607138474782\n",
      "Train Iter:  151  Loss:  1.2436220713009107\n",
      "Train Iter:  152  Loss:  1.2435875993810201\n",
      "Train Iter:  153  Loss:  1.2439656175819098\n",
      "Train Iter:  154  Loss:  1.244486841675523\n",
      "Train Iter:  155  Loss:  1.247769138505382\n",
      "Train Iter:  156  Loss:  1.2464703218295023\n",
      "Train Iter:  157  Loss:  1.2456948905234124\n",
      "Train Iter:  158  Loss:  1.24557101236114\n",
      "Train Iter:  159  Loss:  1.246147153137615\n",
      "Train Iter:  160  Loss:  1.2458416696637868\n",
      "Train Iter:  161  Loss:  1.244601869805259\n",
      "Train Iter:  162  Loss:  1.2467264816348935\n",
      "Train Iter:  163  Loss:  1.2469681380716569\n",
      "Train Iter:  164  Loss:  1.2465615414264726\n",
      "Train Iter:  165  Loss:  1.246283656539339\n",
      "Train Iter:  166  Loss:  1.246550925883902\n",
      "Train Iter:  167  Loss:  1.246584344409897\n",
      "Train Iter:  168  Loss:  1.2478585608658337\n",
      "Train Iter:  169  Loss:  1.2454862062042282\n",
      "Train Iter:  170  Loss:  1.244348006739336\n",
      "Train Iter:  171  Loss:  1.2439952771566067\n",
      "Train Iter:  172  Loss:  1.2440724930790967\n",
      "Train Iter:  173  Loss:  1.245865340522259\n",
      "Train Iter:  174  Loss:  1.2484135165296752\n",
      "Train Iter:  175  Loss:  1.2487171823637826\n",
      "Train Iter:  176  Loss:  1.2490627809681676\n",
      "Train Iter:  177  Loss:  1.247777885299618\n",
      "Train Iter:  178  Loss:  1.2493820776430409\n",
      "Train Iter:  179  Loss:  1.250005906853596\n",
      "Train Iter:  180  Loss:  1.250597326291932\n",
      "Train Iter:  181  Loss:  1.250161729794181\n",
      "Train Iter:  182  Loss:  1.2468531816215305\n",
      "Train Iter:  183  Loss:  1.2477888725494426\n",
      "Train Iter:  184  Loss:  1.24672628323669\n",
      "Train Iter:  185  Loss:  1.2483499439986976\n",
      "Train Iter:  186  Loss:  1.2494671117233973\n",
      "Val Iter: Loss:  2.5920605659484863\n",
      "Epoch 22, Mean Train Loss: 1.2494671117233973, Val Loss (1 sample): 2.5920605659484863\n",
      "Train Iter:  1  Loss:  1.6075546741485596\n",
      "Train Iter:  2  Loss:  1.5619471073150635\n",
      "Train Iter:  3  Loss:  1.5303579966227214\n",
      "Train Iter:  4  Loss:  1.5380060374736786\n",
      "Train Iter:  5  Loss:  1.4896355152130127\n",
      "Train Iter:  6  Loss:  1.4536166191101074\n",
      "Train Iter:  7  Loss:  1.426752073424203\n",
      "Train Iter:  8  Loss:  1.4070887565612793\n",
      "Train Iter:  9  Loss:  1.4105281829833984\n",
      "Train Iter:  10  Loss:  1.4288895964622497\n",
      "Train Iter:  11  Loss:  1.3931508281014182\n",
      "Train Iter:  12  Loss:  1.408137361208598\n",
      "Train Iter:  13  Loss:  1.4226908133580134\n",
      "Train Iter:  14  Loss:  1.4331791996955872\n",
      "Train Iter:  15  Loss:  1.4520506223042806\n",
      "Train Iter:  16  Loss:  1.4470571428537369\n",
      "Train Iter:  17  Loss:  1.4466616897022022\n",
      "Train Iter:  18  Loss:  1.440990752644009\n",
      "Train Iter:  19  Loss:  1.4313554450085288\n",
      "Train Iter:  20  Loss:  1.407201874256134\n",
      "Train Iter:  21  Loss:  1.4216470093954177\n",
      "Train Iter:  22  Loss:  1.4130329218777744\n",
      "Train Iter:  23  Loss:  1.4000549523726753\n",
      "Train Iter:  24  Loss:  1.3990079015493393\n",
      "Train Iter:  25  Loss:  1.4007848405838013\n",
      "Train Iter:  26  Loss:  1.41748341688743\n",
      "Train Iter:  27  Loss:  1.4019401492895904\n",
      "Train Iter:  28  Loss:  1.403913465993745\n",
      "Train Iter:  29  Loss:  1.4014764304818779\n",
      "Train Iter:  30  Loss:  1.401287720600764\n",
      "Train Iter:  31  Loss:  1.3962603172948282\n",
      "Train Iter:  32  Loss:  1.3915648106485605\n",
      "Train Iter:  33  Loss:  1.3802134286273608\n",
      "Train Iter:  34  Loss:  1.3797196623157053\n",
      "Train Iter:  35  Loss:  1.374219572544098\n",
      "Train Iter:  36  Loss:  1.3676858130428526\n",
      "Train Iter:  37  Loss:  1.365954410385441\n",
      "Train Iter:  38  Loss:  1.3628647562704588\n",
      "Train Iter:  39  Loss:  1.3641768892606099\n",
      "Train Iter:  40  Loss:  1.3593914613127709\n",
      "Train Iter:  41  Loss:  1.3604876806096333\n",
      "Train Iter:  42  Loss:  1.3664906777086712\n",
      "Train Iter:  43  Loss:  1.3618554029353829\n",
      "Train Iter:  44  Loss:  1.3624584471637553\n",
      "Train Iter:  45  Loss:  1.3602722790506152\n",
      "Train Iter:  46  Loss:  1.3623377356840216\n",
      "Train Iter:  47  Loss:  1.3615510324214368\n",
      "Train Iter:  48  Loss:  1.3566245002051194\n",
      "Train Iter:  49  Loss:  1.3583528959021276\n",
      "Train Iter:  50  Loss:  1.3619656932353974\n",
      "Train Iter:  51  Loss:  1.35976534614376\n",
      "Train Iter:  52  Loss:  1.363756958108682\n",
      "Train Iter:  53  Loss:  1.3666390416757115\n",
      "Train Iter:  54  Loss:  1.36715308714796\n",
      "Train Iter:  55  Loss:  1.3629867109385403\n",
      "Train Iter:  56  Loss:  1.3612834438681602\n",
      "Train Iter:  57  Loss:  1.3571980511933042\n",
      "Train Iter:  58  Loss:  1.3556059383112808\n",
      "Train Iter:  59  Loss:  1.35706801838794\n",
      "Train Iter:  60  Loss:  1.3525375376145046\n",
      "Train Iter:  61  Loss:  1.3482324266042867\n",
      "Train Iter:  62  Loss:  1.3511784759259993\n",
      "Train Iter:  63  Loss:  1.349228526864733\n",
      "Train Iter:  64  Loss:  1.3467799639329314\n",
      "Train Iter:  65  Loss:  1.3487176409134498\n",
      "Train Iter:  66  Loss:  1.3448071506890384\n",
      "Train Iter:  67  Loss:  1.3428958538752884\n",
      "Train Iter:  68  Loss:  1.3394850536304361\n",
      "Train Iter:  69  Loss:  1.3356991133828093\n",
      "Train Iter:  70  Loss:  1.3343498510973795\n",
      "Train Iter:  71  Loss:  1.329786499621163\n",
      "Train Iter:  72  Loss:  1.3249980948037572\n",
      "Train Iter:  73  Loss:  1.3225109013792586\n",
      "Train Iter:  74  Loss:  1.326585348393466\n",
      "Train Iter:  75  Loss:  1.3243351737658182\n",
      "Train Iter:  76  Loss:  1.3225162162592536\n",
      "Train Iter:  77  Loss:  1.3215764412632236\n",
      "Train Iter:  78  Loss:  1.3202706644168267\n",
      "Train Iter:  79  Loss:  1.318980898283705\n",
      "Train Iter:  80  Loss:  1.318524280935526\n",
      "Train Iter:  81  Loss:  1.3157716422905157\n",
      "Train Iter:  82  Loss:  1.318545723106803\n",
      "Train Iter:  83  Loss:  1.3153954047754586\n",
      "Train Iter:  84  Loss:  1.318317127369699\n",
      "Train Iter:  85  Loss:  1.318245627599604\n",
      "Train Iter:  86  Loss:  1.313357606183651\n",
      "Train Iter:  87  Loss:  1.3149124705928497\n",
      "Train Iter:  88  Loss:  1.316281303086064\n",
      "Train Iter:  89  Loss:  1.31706169281113\n",
      "Train Iter:  90  Loss:  1.316545252667533\n",
      "Train Iter:  91  Loss:  1.3169418514429867\n",
      "Train Iter:  92  Loss:  1.312155874527019\n",
      "Train Iter:  93  Loss:  1.3125977009855292\n",
      "Train Iter:  94  Loss:  1.3109448812109359\n",
      "Train Iter:  95  Loss:  1.3099813166417573\n",
      "Train Iter:  96  Loss:  1.3102463278919458\n",
      "Train Iter:  97  Loss:  1.3128274587011828\n",
      "Train Iter:  98  Loss:  1.3114250284068438\n",
      "Train Iter:  99  Loss:  1.309921560263393\n",
      "Train Iter:  100  Loss:  1.3087774246931076\n",
      "Train Iter:  101  Loss:  1.3060230435711322\n",
      "Train Iter:  102  Loss:  1.3015665873592974\n",
      "Train Iter:  103  Loss:  1.3031445550687104\n",
      "Train Iter:  104  Loss:  1.300850622355938\n",
      "Train Iter:  105  Loss:  1.3016449775014605\n",
      "Train Iter:  106  Loss:  1.3022885080778375\n",
      "Train Iter:  107  Loss:  1.298925384739849\n",
      "Train Iter:  108  Loss:  1.2976797338989046\n",
      "Train Iter:  109  Loss:  1.297673888709567\n",
      "Train Iter:  110  Loss:  1.2975388846614144\n",
      "Train Iter:  111  Loss:  1.2977058699539117\n",
      "Train Iter:  112  Loss:  1.2993608271437032\n",
      "Train Iter:  113  Loss:  1.2993681552135838\n",
      "Train Iter:  114  Loss:  1.297528633423019\n",
      "Train Iter:  115  Loss:  1.2977114941762842\n",
      "Train Iter:  116  Loss:  1.3018416705830345\n",
      "Train Iter:  117  Loss:  1.3006290946251307\n",
      "Train Iter:  118  Loss:  1.2994082797381838\n",
      "Train Iter:  119  Loss:  1.2989062386400558\n",
      "Train Iter:  120  Loss:  1.297380618751049\n",
      "Train Iter:  121  Loss:  1.2955759354859344\n",
      "Train Iter:  122  Loss:  1.2964246873972847\n",
      "Train Iter:  123  Loss:  1.297987292937147\n",
      "Train Iter:  124  Loss:  1.298619361173722\n",
      "Train Iter:  125  Loss:  1.29875519323349\n",
      "Train Iter:  126  Loss:  1.2955053694664487\n",
      "Train Iter:  127  Loss:  1.2958757023173055\n",
      "Train Iter:  128  Loss:  1.298248809762299\n",
      "Train Iter:  129  Loss:  1.2974663141161897\n",
      "Train Iter:  130  Loss:  1.2972750232769892\n",
      "Train Iter:  131  Loss:  1.2948589270351498\n",
      "Train Iter:  132  Loss:  1.2962796471335671\n",
      "Train Iter:  133  Loss:  1.2986796646189869\n",
      "Train Iter:  134  Loss:  1.2990100784088248\n",
      "Train Iter:  135  Loss:  1.3001344883883441\n",
      "Train Iter:  136  Loss:  1.2980787149247002\n",
      "Train Iter:  137  Loss:  1.2986272712693596\n",
      "Train Iter:  138  Loss:  1.2973604651464932\n",
      "Train Iter:  139  Loss:  1.2976665488249963\n",
      "Train Iter:  140  Loss:  1.297151485511235\n",
      "Train Iter:  141  Loss:  1.2981236200806097\n",
      "Train Iter:  142  Loss:  1.2975284029060685\n",
      "Train Iter:  143  Loss:  1.2951579714988495\n",
      "Train Iter:  144  Loss:  1.2956033303505845\n",
      "Train Iter:  145  Loss:  1.2956486418329436\n",
      "Train Iter:  146  Loss:  1.2935327883452585\n",
      "Train Iter:  147  Loss:  1.2927769304943733\n",
      "Train Iter:  148  Loss:  1.292068767789248\n",
      "Train Iter:  149  Loss:  1.291328843407983\n",
      "Train Iter:  150  Loss:  1.292000230550766\n",
      "Train Iter:  151  Loss:  1.292374445507858\n",
      "Train Iter:  152  Loss:  1.2935368901020603\n",
      "Train Iter:  153  Loss:  1.293012243859908\n",
      "Train Iter:  154  Loss:  1.2929456818413425\n",
      "Train Iter:  155  Loss:  1.2924878485741154\n",
      "Train Iter:  156  Loss:  1.2938097329475942\n",
      "Train Iter:  157  Loss:  1.2946304397977841\n",
      "Train Iter:  158  Loss:  1.2949012702778926\n",
      "Train Iter:  159  Loss:  1.2942321926542797\n",
      "Train Iter:  160  Loss:  1.2917683746665716\n",
      "Train Iter:  161  Loss:  1.2897052202165498\n",
      "Train Iter:  162  Loss:  1.2891383509577057\n",
      "Train Iter:  163  Loss:  1.2903821036859524\n",
      "Train Iter:  164  Loss:  1.2899601633955793\n",
      "Train Iter:  165  Loss:  1.2895591880335953\n",
      "Train Iter:  166  Loss:  1.288599890398692\n",
      "Train Iter:  167  Loss:  1.2863992224196474\n",
      "Train Iter:  168  Loss:  1.2869838093008314\n",
      "Train Iter:  169  Loss:  1.2876893225506212\n",
      "Train Iter:  170  Loss:  1.287259968589334\n",
      "Train Iter:  171  Loss:  1.2876340945561726\n",
      "Train Iter:  172  Loss:  1.287967052570609\n",
      "Train Iter:  173  Loss:  1.2894880661385597\n",
      "Train Iter:  174  Loss:  1.2876897817370536\n",
      "Train Iter:  175  Loss:  1.2889877809797015\n",
      "Train Iter:  176  Loss:  1.2887011400677941\n",
      "Train Iter:  177  Loss:  1.287718621350951\n",
      "Train Iter:  178  Loss:  1.2887589087646998\n",
      "Train Iter:  179  Loss:  1.2901070670708599\n",
      "Train Iter:  180  Loss:  1.2907217257552677\n",
      "Train Iter:  181  Loss:  1.2892750429185056\n",
      "Train Iter:  182  Loss:  1.28973983670329\n",
      "Train Iter:  183  Loss:  1.2932462347009794\n",
      "Train Iter:  184  Loss:  1.29137126226788\n",
      "Train Iter:  185  Loss:  1.2903869019972312\n",
      "Train Iter:  186  Loss:  1.290643350731942\n",
      "Val Iter: Loss:  1.172142505645752\n",
      "Epoch 23, Mean Train Loss: 1.290643350731942, Val Loss (1 sample): 1.172142505645752\n",
      "Train Iter:  1  Loss:  1.2444560527801514\n",
      "Train Iter:  2  Loss:  1.093216598033905\n",
      "Train Iter:  3  Loss:  1.2372955878575642\n",
      "Train Iter:  4  Loss:  1.2771855890750885\n",
      "Train Iter:  5  Loss:  1.2503530740737916\n",
      "Train Iter:  6  Loss:  1.22727632522583\n",
      "Train Iter:  7  Loss:  1.2270317077636719\n",
      "Train Iter:  8  Loss:  1.2350273430347443\n",
      "Train Iter:  9  Loss:  1.2160190211402044\n",
      "Train Iter:  10  Loss:  1.2444072008132934\n",
      "Train Iter:  11  Loss:  1.240137143568559\n",
      "Train Iter:  12  Loss:  1.2300707201162975\n",
      "Train Iter:  13  Loss:  1.2386234815304096\n",
      "Train Iter:  14  Loss:  1.2475684114864893\n",
      "Train Iter:  15  Loss:  1.2384241024653118\n",
      "Train Iter:  16  Loss:  1.2355166152119637\n",
      "Train Iter:  17  Loss:  1.226218532113468\n",
      "Train Iter:  18  Loss:  1.2379735310872395\n",
      "Train Iter:  19  Loss:  1.251904807592693\n",
      "Train Iter:  20  Loss:  1.2619794607162476\n",
      "Train Iter:  21  Loss:  1.2596282164255779\n",
      "Train Iter:  22  Loss:  1.2611497640609741\n",
      "Train Iter:  23  Loss:  1.2641881807990696\n",
      "Train Iter:  24  Loss:  1.2571928550799687\n",
      "Train Iter:  25  Loss:  1.2593322324752807\n",
      "Train Iter:  26  Loss:  1.2652534613242516\n",
      "Train Iter:  27  Loss:  1.2639847199122112\n",
      "Train Iter:  28  Loss:  1.2494542279413767\n",
      "Train Iter:  29  Loss:  1.247370758960987\n",
      "Train Iter:  30  Loss:  1.2413298070430756\n",
      "Train Iter:  31  Loss:  1.2374640222518676\n",
      "Train Iter:  32  Loss:  1.2396559584885836\n",
      "Train Iter:  33  Loss:  1.239901009834174\n",
      "Train Iter:  34  Loss:  1.2324016269515543\n",
      "Train Iter:  35  Loss:  1.2237841861588614\n",
      "Train Iter:  36  Loss:  1.2165983832544751\n",
      "Train Iter:  37  Loss:  1.2095774251061517\n",
      "Train Iter:  38  Loss:  1.207721942349484\n",
      "Train Iter:  39  Loss:  1.20766609448653\n",
      "Train Iter:  40  Loss:  1.2243293970823288\n",
      "Train Iter:  41  Loss:  1.2216618468121785\n",
      "Train Iter:  42  Loss:  1.227761013167245\n",
      "Train Iter:  43  Loss:  1.2309041522270026\n",
      "Train Iter:  44  Loss:  1.2317887084050612\n",
      "Train Iter:  45  Loss:  1.2322835206985474\n",
      "Train Iter:  46  Loss:  1.2275778703067615\n",
      "Train Iter:  47  Loss:  1.2243701676104932\n",
      "Train Iter:  48  Loss:  1.2262099509437878\n",
      "Train Iter:  49  Loss:  1.2347904760010389\n",
      "Train Iter:  50  Loss:  1.2331695985794067\n",
      "Train Iter:  51  Loss:  1.2354686236849017\n",
      "Train Iter:  52  Loss:  1.2350921241136699\n",
      "Train Iter:  53  Loss:  1.235597216858054\n",
      "Train Iter:  54  Loss:  1.2345150841606989\n",
      "Train Iter:  55  Loss:  1.2338230219754305\n",
      "Train Iter:  56  Loss:  1.2341971120664053\n",
      "Train Iter:  57  Loss:  1.228166977564494\n",
      "Train Iter:  58  Loss:  1.2213807157401382\n",
      "Train Iter:  59  Loss:  1.2179967862064556\n",
      "Train Iter:  60  Loss:  1.2183634907007217\n",
      "Train Iter:  61  Loss:  1.218546567393131\n",
      "Train Iter:  62  Loss:  1.2180881971313107\n",
      "Train Iter:  63  Loss:  1.2208485120818728\n",
      "Train Iter:  64  Loss:  1.220764615572989\n",
      "Train Iter:  65  Loss:  1.225584302498744\n",
      "Train Iter:  66  Loss:  1.2249147955215338\n",
      "Train Iter:  67  Loss:  1.2272318254655867\n",
      "Train Iter:  68  Loss:  1.2284273601630156\n",
      "Train Iter:  69  Loss:  1.2269712125045666\n",
      "Train Iter:  70  Loss:  1.2272963770798275\n",
      "Train Iter:  71  Loss:  1.226271969331822\n",
      "Train Iter:  72  Loss:  1.222816483842002\n",
      "Train Iter:  73  Loss:  1.2232101698444313\n",
      "Train Iter:  74  Loss:  1.228150253360336\n",
      "Train Iter:  75  Loss:  1.2329893652598063\n",
      "Train Iter:  76  Loss:  1.2360616781209643\n",
      "Train Iter:  77  Loss:  1.2362373030030882\n",
      "Train Iter:  78  Loss:  1.2340772365912414\n",
      "Train Iter:  79  Loss:  1.235352943215189\n",
      "Train Iter:  80  Loss:  1.2353682950139047\n",
      "Train Iter:  81  Loss:  1.2323857538494063\n",
      "Train Iter:  82  Loss:  1.2336328763787339\n",
      "Train Iter:  83  Loss:  1.2287493161408298\n",
      "Train Iter:  84  Loss:  1.23594403054033\n",
      "Train Iter:  85  Loss:  1.233338579710792\n",
      "Train Iter:  86  Loss:  1.2345731805923372\n",
      "Train Iter:  87  Loss:  1.2322516078236458\n",
      "Train Iter:  88  Loss:  1.236479983411052\n",
      "Train Iter:  89  Loss:  1.2363489270210266\n",
      "Train Iter:  90  Loss:  1.2377052631643084\n",
      "Train Iter:  91  Loss:  1.2384453664769184\n",
      "Train Iter:  92  Loss:  1.2393526804188024\n",
      "Train Iter:  93  Loss:  1.2385802929119398\n",
      "Train Iter:  94  Loss:  1.236863615030938\n",
      "Train Iter:  95  Loss:  1.239708125591278\n",
      "Train Iter:  96  Loss:  1.2422677210221689\n",
      "Train Iter:  97  Loss:  1.2452975616012651\n",
      "Train Iter:  98  Loss:  1.2524927884948498\n",
      "Train Iter:  99  Loss:  1.2504522372977902\n",
      "Train Iter:  100  Loss:  1.2497342628240586\n",
      "Train Iter:  101  Loss:  1.2538223437743612\n",
      "Train Iter:  102  Loss:  1.255687990024978\n",
      "Train Iter:  103  Loss:  1.2600308616184495\n",
      "Train Iter:  104  Loss:  1.260713677566785\n",
      "Train Iter:  105  Loss:  1.2599632529985336\n",
      "Train Iter:  106  Loss:  1.2616661533994495\n",
      "Train Iter:  107  Loss:  1.2637787405575547\n",
      "Train Iter:  108  Loss:  1.2621947742170758\n",
      "Train Iter:  109  Loss:  1.2600500075095291\n",
      "Train Iter:  110  Loss:  1.2627171868627722\n",
      "Train Iter:  111  Loss:  1.2643116642762948\n",
      "Train Iter:  112  Loss:  1.2639901100524835\n",
      "Train Iter:  113  Loss:  1.263815373973509\n",
      "Train Iter:  114  Loss:  1.263202013153779\n",
      "Train Iter:  115  Loss:  1.2637617593226225\n",
      "Train Iter:  116  Loss:  1.2622060852831807\n",
      "Train Iter:  117  Loss:  1.2597941603416052\n",
      "Train Iter:  118  Loss:  1.2626219538308807\n",
      "Train Iter:  119  Loss:  1.261722231612486\n",
      "Train Iter:  120  Loss:  1.262113870680332\n",
      "Train Iter:  121  Loss:  1.2610215357512482\n",
      "Train Iter:  122  Loss:  1.262800405748555\n",
      "Train Iter:  123  Loss:  1.2650449338967238\n",
      "Train Iter:  124  Loss:  1.266250739655187\n",
      "Train Iter:  125  Loss:  1.264426061153412\n",
      "Train Iter:  126  Loss:  1.2628110566782573\n",
      "Train Iter:  127  Loss:  1.2613359516061198\n",
      "Train Iter:  128  Loss:  1.2593926661647856\n",
      "Train Iter:  129  Loss:  1.2594058093174483\n",
      "Train Iter:  130  Loss:  1.2572993370202872\n",
      "Train Iter:  131  Loss:  1.2580779326781062\n",
      "Train Iter:  132  Loss:  1.2596213898875497\n",
      "Train Iter:  133  Loss:  1.255184116668271\n",
      "Train Iter:  134  Loss:  1.2553353376352965\n",
      "Train Iter:  135  Loss:  1.2545844576976917\n",
      "Train Iter:  136  Loss:  1.254987809149658\n",
      "Train Iter:  137  Loss:  1.2564780333616439\n",
      "Train Iter:  138  Loss:  1.256686241298482\n",
      "Train Iter:  139  Loss:  1.2586384484236188\n",
      "Train Iter:  140  Loss:  1.2558036071913583\n",
      "Train Iter:  141  Loss:  1.2560738629483161\n",
      "Train Iter:  142  Loss:  1.2558192274939846\n",
      "Train Iter:  143  Loss:  1.254638226715835\n",
      "Train Iter:  144  Loss:  1.2517878719502025\n",
      "Train Iter:  145  Loss:  1.2509271432613505\n",
      "Train Iter:  146  Loss:  1.2506216233723784\n",
      "Train Iter:  147  Loss:  1.249044156398903\n",
      "Train Iter:  148  Loss:  1.249732396087131\n",
      "Train Iter:  149  Loss:  1.2477049699565708\n",
      "Train Iter:  150  Loss:  1.2471850395202637\n",
      "Train Iter:  151  Loss:  1.2468717895596233\n",
      "Train Iter:  152  Loss:  1.2461121074463193\n",
      "Train Iter:  153  Loss:  1.246543430814556\n",
      "Train Iter:  154  Loss:  1.2457384242639913\n",
      "Train Iter:  155  Loss:  1.2429941542686955\n",
      "Train Iter:  156  Loss:  1.2461964468925426\n",
      "Train Iter:  157  Loss:  1.2453133167734571\n",
      "Train Iter:  158  Loss:  1.2457545599605464\n",
      "Train Iter:  159  Loss:  1.2438652136790678\n",
      "Train Iter:  160  Loss:  1.2435786593705416\n",
      "Train Iter:  161  Loss:  1.2456274295445555\n",
      "Train Iter:  162  Loss:  1.2466292752895827\n",
      "Train Iter:  163  Loss:  1.246938129509885\n",
      "Train Iter:  164  Loss:  1.24460110286387\n",
      "Train Iter:  165  Loss:  1.2429361235011707\n",
      "Train Iter:  166  Loss:  1.2418242549321739\n",
      "Train Iter:  167  Loss:  1.2425936367697346\n",
      "Train Iter:  168  Loss:  1.2434129047961462\n",
      "Train Iter:  169  Loss:  1.242543703000221\n",
      "Train Iter:  170  Loss:  1.2434204683584325\n",
      "Train Iter:  171  Loss:  1.2454954406671357\n",
      "Train Iter:  172  Loss:  1.2435463680777439\n",
      "Train Iter:  173  Loss:  1.2424301246687166\n",
      "Train Iter:  174  Loss:  1.2450055849963222\n",
      "Train Iter:  175  Loss:  1.2462375906535557\n",
      "Train Iter:  176  Loss:  1.2484615255485882\n",
      "Train Iter:  177  Loss:  1.247771325757948\n",
      "Train Iter:  178  Loss:  1.24843899989396\n",
      "Train Iter:  179  Loss:  1.2468506890302264\n",
      "Train Iter:  180  Loss:  1.2469972709814707\n",
      "Train Iter:  181  Loss:  1.2469677345528787\n",
      "Train Iter:  182  Loss:  1.2446674223129566\n",
      "Train Iter:  183  Loss:  1.2443627610884078\n",
      "Train Iter:  184  Loss:  1.243177125311416\n",
      "Train Iter:  185  Loss:  1.2440594044891564\n",
      "Train Iter:  186  Loss:  1.2455060549320713\n",
      "Val Iter: Loss:  0.8284696340560913\n",
      "Epoch 24, Mean Train Loss: 1.2455060549320713, Val Loss (1 sample): 0.8284696340560913\n",
      "Train Iter:  1  Loss:  1.33000910282135\n",
      "Train Iter:  2  Loss:  1.3842222094535828\n",
      "Train Iter:  3  Loss:  1.3367795546849568\n",
      "Train Iter:  4  Loss:  1.2965432107448578\n",
      "Train Iter:  5  Loss:  1.2825833320617677\n",
      "Train Iter:  6  Loss:  1.2921704451243083\n",
      "Train Iter:  7  Loss:  1.2457705651010786\n",
      "Train Iter:  8  Loss:  1.2545097842812538\n",
      "Train Iter:  9  Loss:  1.257445262538062\n",
      "Train Iter:  10  Loss:  1.3060332238674164\n",
      "Train Iter:  11  Loss:  1.2820827798409895\n",
      "Train Iter:  12  Loss:  1.2600200325250626\n",
      "Train Iter:  13  Loss:  1.2473664421301622\n",
      "Train Iter:  14  Loss:  1.2369697306837355\n",
      "Train Iter:  15  Loss:  1.2496419390042623\n",
      "Train Iter:  16  Loss:  1.2436545826494694\n",
      "Train Iter:  17  Loss:  1.2448611785383785\n",
      "Train Iter:  18  Loss:  1.2374740805890825\n",
      "Train Iter:  19  Loss:  1.230131823765604\n",
      "Train Iter:  20  Loss:  1.227994468808174\n",
      "Train Iter:  21  Loss:  1.2281090560413541\n",
      "Train Iter:  22  Loss:  1.2355736792087555\n",
      "Train Iter:  23  Loss:  1.223885090454765\n",
      "Train Iter:  24  Loss:  1.235593522588412\n",
      "Train Iter:  25  Loss:  1.253214373588562\n",
      "Train Iter:  26  Loss:  1.2574895391097436\n",
      "Train Iter:  27  Loss:  1.2556535888601232\n",
      "Train Iter:  28  Loss:  1.2606712877750397\n",
      "Train Iter:  29  Loss:  1.2632982895292084\n",
      "Train Iter:  30  Loss:  1.2670908649762471\n",
      "Train Iter:  31  Loss:  1.261997349800602\n",
      "Train Iter:  32  Loss:  1.2548102028667927\n",
      "Train Iter:  33  Loss:  1.2554559382525357\n",
      "Train Iter:  34  Loss:  1.2449179291725159\n",
      "Train Iter:  35  Loss:  1.2459631851741246\n",
      "Train Iter:  36  Loss:  1.2492748631371393\n",
      "Train Iter:  37  Loss:  1.2510962969547994\n",
      "Train Iter:  38  Loss:  1.2412222482656177\n",
      "Train Iter:  39  Loss:  1.2467689193212068\n",
      "Train Iter:  40  Loss:  1.2478705450892449\n",
      "Train Iter:  41  Loss:  1.245506573014143\n",
      "Train Iter:  42  Loss:  1.2386770645777385\n",
      "Train Iter:  43  Loss:  1.251728099445964\n",
      "Train Iter:  44  Loss:  1.248789456757632\n",
      "Train Iter:  45  Loss:  1.2452949656380548\n",
      "Train Iter:  46  Loss:  1.2430178030677463\n",
      "Train Iter:  47  Loss:  1.2416313404732562\n",
      "Train Iter:  48  Loss:  1.2470064361890156\n",
      "Train Iter:  49  Loss:  1.244146390837066\n",
      "Train Iter:  50  Loss:  1.246264157295227\n",
      "Train Iter:  51  Loss:  1.2518772658179789\n",
      "Train Iter:  52  Loss:  1.2552224603983073\n",
      "Train Iter:  53  Loss:  1.2516709588608652\n",
      "Train Iter:  54  Loss:  1.2454019221994612\n",
      "Train Iter:  55  Loss:  1.2475044044581327\n",
      "Train Iter:  56  Loss:  1.249333140041147\n",
      "Train Iter:  57  Loss:  1.2465916464203282\n",
      "Train Iter:  58  Loss:  1.247723940117606\n",
      "Train Iter:  59  Loss:  1.2461343104556455\n",
      "Train Iter:  60  Loss:  1.2457599292198818\n",
      "Train Iter:  61  Loss:  1.2406332053121973\n",
      "Train Iter:  62  Loss:  1.2376694938828867\n",
      "Train Iter:  63  Loss:  1.2350108954641554\n",
      "Train Iter:  64  Loss:  1.2325722919777036\n",
      "Train Iter:  65  Loss:  1.2332133687459506\n",
      "Train Iter:  66  Loss:  1.2332330391262516\n",
      "Train Iter:  67  Loss:  1.2349194065848392\n",
      "Train Iter:  68  Loss:  1.2286835724816603\n",
      "Train Iter:  69  Loss:  1.2302601432454758\n",
      "Train Iter:  70  Loss:  1.2261530033179693\n",
      "Train Iter:  71  Loss:  1.232135981741086\n",
      "Train Iter:  72  Loss:  1.2327144054902925\n",
      "Train Iter:  73  Loss:  1.2320446404692245\n",
      "Train Iter:  74  Loss:  1.23234200074866\n",
      "Train Iter:  75  Loss:  1.232208912372589\n",
      "Train Iter:  76  Loss:  1.2349039151480323\n",
      "Train Iter:  77  Loss:  1.2347558001419165\n",
      "Train Iter:  78  Loss:  1.2314953888073945\n",
      "Train Iter:  79  Loss:  1.2347182239158243\n",
      "Train Iter:  80  Loss:  1.2369325287640094\n",
      "Train Iter:  81  Loss:  1.235608560803496\n",
      "Train Iter:  82  Loss:  1.2358354904302737\n",
      "Train Iter:  83  Loss:  1.233988680753363\n",
      "Train Iter:  84  Loss:  1.2338681980257942\n",
      "Train Iter:  85  Loss:  1.2329270664383383\n",
      "Train Iter:  86  Loss:  1.2306805467882822\n",
      "Train Iter:  87  Loss:  1.2296845604633462\n",
      "Train Iter:  88  Loss:  1.232941633598371\n",
      "Train Iter:  89  Loss:  1.232816907127252\n",
      "Train Iter:  90  Loss:  1.2305225670337676\n",
      "Train Iter:  91  Loss:  1.227388159259335\n",
      "Train Iter:  92  Loss:  1.2291518929211989\n",
      "Train Iter:  93  Loss:  1.2289921378576627\n",
      "Train Iter:  94  Loss:  1.2284413132261722\n",
      "Train Iter:  95  Loss:  1.2314486591439497\n",
      "Train Iter:  96  Loss:  1.235308364033699\n",
      "Train Iter:  97  Loss:  1.235252144410438\n",
      "Train Iter:  98  Loss:  1.2383273110097768\n",
      "Train Iter:  99  Loss:  1.2414703838752978\n",
      "Train Iter:  100  Loss:  1.2391963338851928\n",
      "Train Iter:  101  Loss:  1.2419687804609243\n",
      "Train Iter:  102  Loss:  1.2436422682275958\n",
      "Train Iter:  103  Loss:  1.241403255647826\n",
      "Train Iter:  104  Loss:  1.242660036453834\n",
      "Train Iter:  105  Loss:  1.2427433570226034\n",
      "Train Iter:  106  Loss:  1.24519434065189\n",
      "Train Iter:  107  Loss:  1.2435763494990697\n",
      "Train Iter:  108  Loss:  1.2424062468387462\n",
      "Train Iter:  109  Loss:  1.2427958545334843\n",
      "Train Iter:  110  Loss:  1.2472769065336748\n",
      "Train Iter:  111  Loss:  1.2477888631391096\n",
      "Train Iter:  112  Loss:  1.2484932882445199\n",
      "Train Iter:  113  Loss:  1.2499142663668743\n",
      "Train Iter:  114  Loss:  1.2489948753725018\n",
      "Train Iter:  115  Loss:  1.2498659859532895\n",
      "Train Iter:  116  Loss:  1.249023249437069\n",
      "Train Iter:  117  Loss:  1.2501572054675503\n",
      "Train Iter:  118  Loss:  1.2513899530394603\n",
      "Train Iter:  119  Loss:  1.2502990169685428\n",
      "Train Iter:  120  Loss:  1.2508946657180786\n",
      "Train Iter:  121  Loss:  1.2502875663032216\n",
      "Train Iter:  122  Loss:  1.2503634476270833\n",
      "Train Iter:  123  Loss:  1.2515081361057312\n",
      "Train Iter:  124  Loss:  1.251461084811918\n",
      "Train Iter:  125  Loss:  1.2523397464752197\n",
      "Train Iter:  126  Loss:  1.2543605338959467\n",
      "Train Iter:  127  Loss:  1.2557278665031975\n",
      "Train Iter:  128  Loss:  1.2583133466541767\n",
      "Train Iter:  129  Loss:  1.2572944219722304\n",
      "Train Iter:  130  Loss:  1.2556708528445317\n",
      "Train Iter:  131  Loss:  1.2535583763632157\n",
      "Train Iter:  132  Loss:  1.2572114991419243\n",
      "Train Iter:  133  Loss:  1.257043699572857\n",
      "Train Iter:  134  Loss:  1.2571296006885928\n",
      "Train Iter:  135  Loss:  1.2586947087888365\n",
      "Train Iter:  136  Loss:  1.258300427128287\n",
      "Train Iter:  137  Loss:  1.2575504597086107\n",
      "Train Iter:  138  Loss:  1.254729540883631\n",
      "Train Iter:  139  Loss:  1.2537511797260037\n",
      "Train Iter:  140  Loss:  1.2551046043634415\n",
      "Train Iter:  141  Loss:  1.2533699244472152\n",
      "Train Iter:  142  Loss:  1.2528047859668732\n",
      "Train Iter:  143  Loss:  1.2509640984601909\n",
      "Train Iter:  144  Loss:  1.250462821374337\n",
      "Train Iter:  145  Loss:  1.2487135640506086\n",
      "Train Iter:  146  Loss:  1.2476122118022344\n",
      "Train Iter:  147  Loss:  1.2463788183367983\n",
      "Train Iter:  148  Loss:  1.2479706841546137\n",
      "Train Iter:  149  Loss:  1.2507624546153433\n",
      "Train Iter:  150  Loss:  1.2496112354596456\n",
      "Train Iter:  151  Loss:  1.249520589973753\n",
      "Train Iter:  152  Loss:  1.2500308673632772\n",
      "Train Iter:  153  Loss:  1.251195827340768\n",
      "Train Iter:  154  Loss:  1.250131211497567\n",
      "Train Iter:  155  Loss:  1.2496020247859339\n",
      "Train Iter:  156  Loss:  1.2522935095505836\n",
      "Train Iter:  157  Loss:  1.2522981667974193\n",
      "Train Iter:  158  Loss:  1.2548580094228816\n",
      "Train Iter:  159  Loss:  1.2542917338557213\n",
      "Train Iter:  160  Loss:  1.2549323797225953\n",
      "Train Iter:  161  Loss:  1.2542829439506769\n",
      "Train Iter:  162  Loss:  1.2534310015631311\n",
      "Train Iter:  163  Loss:  1.253194737288118\n",
      "Train Iter:  164  Loss:  1.2570674666544286\n",
      "Train Iter:  165  Loss:  1.257000402248267\n",
      "Train Iter:  166  Loss:  1.255679001291114\n",
      "Train Iter:  167  Loss:  1.254152756965089\n",
      "Train Iter:  168  Loss:  1.2538584619760513\n",
      "Train Iter:  169  Loss:  1.2547189392281708\n",
      "Train Iter:  170  Loss:  1.2554670838748707\n",
      "Train Iter:  171  Loss:  1.2545290223339147\n",
      "Train Iter:  172  Loss:  1.254301640876504\n",
      "Train Iter:  173  Loss:  1.254649353165158\n",
      "Train Iter:  174  Loss:  1.2557867272146817\n",
      "Train Iter:  175  Loss:  1.2545772763660976\n",
      "Train Iter:  176  Loss:  1.2545149373737248\n",
      "Train Iter:  177  Loss:  1.253919828409529\n",
      "Train Iter:  178  Loss:  1.2535284094596177\n",
      "Train Iter:  179  Loss:  1.253000333322493\n",
      "Train Iter:  180  Loss:  1.252213588025835\n",
      "Train Iter:  181  Loss:  1.2540851824850008\n",
      "Train Iter:  182  Loss:  1.2533070006213345\n",
      "Train Iter:  183  Loss:  1.254820093431108\n",
      "Train Iter:  184  Loss:  1.2556146124134893\n",
      "Train Iter:  185  Loss:  1.2559574075647302\n",
      "Train Iter:  186  Loss:  1.257009748489626\n",
      "Val Iter: Loss:  1.6551059484481812\n",
      "Epoch 25, Mean Train Loss: 1.257009748489626, Val Loss (1 sample): 1.6551059484481812\n",
      "Train Iter:  1  Loss:  1.238594651222229\n",
      "Train Iter:  2  Loss:  1.2592264413833618\n",
      "Train Iter:  3  Loss:  1.2932581504185994\n",
      "Train Iter:  4  Loss:  1.1716213822364807\n",
      "Train Iter:  5  Loss:  1.2596470594406128\n",
      "Train Iter:  6  Loss:  1.2757776975631714\n",
      "Train Iter:  7  Loss:  1.2452092000416346\n",
      "Train Iter:  8  Loss:  1.2866219282150269\n",
      "Train Iter:  9  Loss:  1.2762537002563477\n",
      "Train Iter:  10  Loss:  1.2583133220672607\n",
      "Train Iter:  11  Loss:  1.2452629912983288\n",
      "Train Iter:  12  Loss:  1.2446707685788472\n",
      "Train Iter:  13  Loss:  1.2661449175614576\n",
      "Train Iter:  14  Loss:  1.2722565957478114\n",
      "Train Iter:  15  Loss:  1.2621788819630941\n",
      "Train Iter:  16  Loss:  1.2637228071689606\n",
      "Train Iter:  17  Loss:  1.2751641904606539\n",
      "Train Iter:  18  Loss:  1.2745922141604953\n",
      "Train Iter:  19  Loss:  1.2715816560544466\n",
      "Train Iter:  20  Loss:  1.2578685283660889\n",
      "Train Iter:  21  Loss:  1.2395638199079604\n",
      "Train Iter:  22  Loss:  1.2439838349819183\n",
      "Train Iter:  23  Loss:  1.2502105210138403\n",
      "Train Iter:  24  Loss:  1.2566704427202542\n",
      "Train Iter:  25  Loss:  1.2640328621864318\n",
      "Train Iter:  26  Loss:  1.2602222217963293\n",
      "Train Iter:  27  Loss:  1.2574958161071494\n",
      "Train Iter:  28  Loss:  1.257400135908808\n",
      "Train Iter:  29  Loss:  1.2672540093290394\n",
      "Train Iter:  30  Loss:  1.2623503903547924\n",
      "Train Iter:  31  Loss:  1.267210323964396\n",
      "Train Iter:  32  Loss:  1.2776708509773016\n",
      "Train Iter:  33  Loss:  1.2838865103143635\n",
      "Train Iter:  34  Loss:  1.2799215369364794\n",
      "Train Iter:  35  Loss:  1.2849948695727758\n",
      "Train Iter:  36  Loss:  1.2875573088725407\n",
      "Train Iter:  37  Loss:  1.2796746508495227\n",
      "Train Iter:  38  Loss:  1.278433261733306\n",
      "Train Iter:  39  Loss:  1.2798988284208836\n",
      "Train Iter:  40  Loss:  1.2875059679150582\n",
      "Train Iter:  41  Loss:  1.285875141620636\n",
      "Train Iter:  42  Loss:  1.2882193795272283\n",
      "Train Iter:  43  Loss:  1.2816296136656473\n",
      "Train Iter:  44  Loss:  1.2835397977720608\n",
      "Train Iter:  45  Loss:  1.2808066831694709\n",
      "Train Iter:  46  Loss:  1.275692360556644\n",
      "Train Iter:  47  Loss:  1.277323084942838\n",
      "Train Iter:  48  Loss:  1.2792157319684823\n",
      "Train Iter:  49  Loss:  1.2800012522814226\n",
      "Train Iter:  50  Loss:  1.2809672701358794\n",
      "Train Iter:  51  Loss:  1.2796676123843473\n",
      "Train Iter:  52  Loss:  1.2776749260150468\n",
      "Train Iter:  53  Loss:  1.2811757067464433\n",
      "Train Iter:  54  Loss:  1.2829799464455358\n",
      "Train Iter:  55  Loss:  1.2834529042243958\n",
      "Train Iter:  56  Loss:  1.283337038542543\n",
      "Train Iter:  57  Loss:  1.2876362455518622\n",
      "Train Iter:  58  Loss:  1.2848849943999587\n",
      "Train Iter:  59  Loss:  1.2816476771387004\n",
      "Train Iter:  60  Loss:  1.2806039343277613\n",
      "Train Iter:  61  Loss:  1.2769551208761871\n",
      "Train Iter:  62  Loss:  1.2727523224969064\n",
      "Train Iter:  63  Loss:  1.2723452732676552\n",
      "Train Iter:  64  Loss:  1.2772847963497043\n",
      "Train Iter:  65  Loss:  1.27741435949619\n",
      "Train Iter:  66  Loss:  1.2809626884532697\n",
      "Train Iter:  67  Loss:  1.278772700188765\n",
      "Train Iter:  68  Loss:  1.2791590892216738\n",
      "Train Iter:  69  Loss:  1.2781609841015027\n",
      "Train Iter:  70  Loss:  1.2736943210874285\n",
      "Train Iter:  71  Loss:  1.2717739091792577\n",
      "Train Iter:  72  Loss:  1.2712623510095808\n",
      "Train Iter:  73  Loss:  1.267496260878158\n",
      "Train Iter:  74  Loss:  1.2663315711794674\n",
      "Train Iter:  75  Loss:  1.2677034552892048\n",
      "Train Iter:  76  Loss:  1.2681920104905178\n",
      "Train Iter:  77  Loss:  1.2681230569814708\n",
      "Train Iter:  78  Loss:  1.261628542955105\n",
      "Train Iter:  79  Loss:  1.2589203081553495\n",
      "Train Iter:  80  Loss:  1.2589870162308217\n",
      "Train Iter:  81  Loss:  1.2624559262652455\n",
      "Train Iter:  82  Loss:  1.2632613669081432\n",
      "Train Iter:  83  Loss:  1.2608300239206798\n",
      "Train Iter:  84  Loss:  1.256429367122196\n",
      "Train Iter:  85  Loss:  1.2570806292926564\n",
      "Train Iter:  86  Loss:  1.25872172034064\n",
      "Train Iter:  87  Loss:  1.2579981376384866\n",
      "Train Iter:  88  Loss:  1.256113576618108\n",
      "Train Iter:  89  Loss:  1.2509714365005493\n",
      "Train Iter:  90  Loss:  1.2477539201577506\n",
      "Train Iter:  91  Loss:  1.2478946178824037\n",
      "Train Iter:  92  Loss:  1.2481004965046179\n",
      "Train Iter:  93  Loss:  1.249037544573507\n",
      "Train Iter:  94  Loss:  1.2476552020996174\n",
      "Train Iter:  95  Loss:  1.2482151665185628\n",
      "Train Iter:  96  Loss:  1.2530568422128756\n",
      "Train Iter:  97  Loss:  1.2526527993457834\n",
      "Train Iter:  98  Loss:  1.2501840269079014\n",
      "Train Iter:  99  Loss:  1.2519870870041125\n",
      "Train Iter:  100  Loss:  1.2521957367658616\n",
      "Train Iter:  101  Loss:  1.2569519771207678\n",
      "Train Iter:  102  Loss:  1.2594767125213848\n",
      "Train Iter:  103  Loss:  1.2593944414148053\n",
      "Train Iter:  104  Loss:  1.26188647460479\n",
      "Train Iter:  105  Loss:  1.2620482563972473\n",
      "Train Iter:  106  Loss:  1.2606414382187825\n",
      "Train Iter:  107  Loss:  1.2598726610156978\n",
      "Train Iter:  108  Loss:  1.2577746913388923\n",
      "Train Iter:  109  Loss:  1.260333958568923\n",
      "Train Iter:  110  Loss:  1.2589194335720755\n",
      "Train Iter:  111  Loss:  1.2581292804297026\n",
      "Train Iter:  112  Loss:  1.2566439929817403\n",
      "Train Iter:  113  Loss:  1.2543675155766243\n",
      "Train Iter:  114  Loss:  1.2517367930788743\n",
      "Train Iter:  115  Loss:  1.2519789120425349\n",
      "Train Iter:  116  Loss:  1.2561353614618038\n",
      "Train Iter:  117  Loss:  1.2543294730349484\n",
      "Train Iter:  118  Loss:  1.255619176868665\n",
      "Train Iter:  119  Loss:  1.253123732174144\n",
      "Train Iter:  120  Loss:  1.251700468858083\n",
      "Train Iter:  121  Loss:  1.2516985087355306\n",
      "Train Iter:  122  Loss:  1.2520000494894434\n",
      "Train Iter:  123  Loss:  1.2497131359286424\n",
      "Train Iter:  124  Loss:  1.2513122001001913\n",
      "Train Iter:  125  Loss:  1.251469425201416\n",
      "Train Iter:  126  Loss:  1.250447098224882\n",
      "Train Iter:  127  Loss:  1.2518550063681415\n",
      "Train Iter:  128  Loss:  1.2521224850788713\n",
      "Train Iter:  129  Loss:  1.25283018285914\n",
      "Train Iter:  130  Loss:  1.25060675740242\n",
      "Train Iter:  131  Loss:  1.2507385902732384\n",
      "Train Iter:  132  Loss:  1.2520756870508194\n",
      "Train Iter:  133  Loss:  1.2500308998545309\n",
      "Train Iter:  134  Loss:  1.2523629313974238\n",
      "Train Iter:  135  Loss:  1.2525466119801556\n",
      "Train Iter:  136  Loss:  1.2511318067417425\n",
      "Train Iter:  137  Loss:  1.2508844802849484\n",
      "Train Iter:  138  Loss:  1.2515023778314176\n",
      "Train Iter:  139  Loss:  1.2530867937657473\n",
      "Train Iter:  140  Loss:  1.254190814920834\n",
      "Train Iter:  141  Loss:  1.255153035018461\n",
      "Train Iter:  142  Loss:  1.254181958000425\n",
      "Train Iter:  143  Loss:  1.2553949585327735\n",
      "Train Iter:  144  Loss:  1.2535708877775404\n",
      "Train Iter:  145  Loss:  1.25532027770733\n",
      "Train Iter:  146  Loss:  1.2572129736207935\n",
      "Train Iter:  147  Loss:  1.2600363590279404\n",
      "Train Iter:  148  Loss:  1.2603061690523818\n",
      "Train Iter:  149  Loss:  1.2586450920809036\n",
      "Train Iter:  150  Loss:  1.2591769528388976\n",
      "Train Iter:  151  Loss:  1.2601402105874573\n",
      "Train Iter:  152  Loss:  1.259949555522517\n",
      "Train Iter:  153  Loss:  1.2599632506277048\n",
      "Train Iter:  154  Loss:  1.2598129975331294\n",
      "Train Iter:  155  Loss:  1.2597067509928057\n",
      "Train Iter:  156  Loss:  1.2586772059782958\n",
      "Train Iter:  157  Loss:  1.2602839257307112\n",
      "Train Iter:  158  Loss:  1.260308767421336\n",
      "Train Iter:  159  Loss:  1.2623805542412043\n",
      "Train Iter:  160  Loss:  1.2634379282593726\n",
      "Train Iter:  161  Loss:  1.2640262590432019\n",
      "Train Iter:  162  Loss:  1.2618864355263886\n",
      "Train Iter:  163  Loss:  1.2620039103221308\n",
      "Train Iter:  164  Loss:  1.260718368902439\n",
      "Train Iter:  165  Loss:  1.2613988255009507\n",
      "Train Iter:  166  Loss:  1.25890136662736\n",
      "Train Iter:  167  Loss:  1.2581583657664452\n",
      "Train Iter:  168  Loss:  1.2596034865294183\n",
      "Train Iter:  169  Loss:  1.2606320850242525\n",
      "Train Iter:  170  Loss:  1.259312406357597\n",
      "Train Iter:  171  Loss:  1.2586118355829117\n",
      "Train Iter:  172  Loss:  1.2581847597693288\n",
      "Train Iter:  173  Loss:  1.2592562895289736\n",
      "Train Iter:  174  Loss:  1.2584052794966205\n",
      "Train Iter:  175  Loss:  1.2592185643741063\n",
      "Train Iter:  176  Loss:  1.2581072405657985\n",
      "Train Iter:  177  Loss:  1.2570008851040555\n",
      "Train Iter:  178  Loss:  1.2574587360526739\n",
      "Train Iter:  179  Loss:  1.257015524296787\n",
      "Train Iter:  180  Loss:  1.2565222531557083\n",
      "Train Iter:  181  Loss:  1.2539081718381597\n",
      "Train Iter:  182  Loss:  1.256876065835848\n",
      "Train Iter:  183  Loss:  1.2570223261098392\n",
      "Train Iter:  184  Loss:  1.2561629576527553\n",
      "Train Iter:  185  Loss:  1.257036473300006\n",
      "Train Iter:  186  Loss:  1.256066515881528\n",
      "Val Iter: Loss:  1.5973323583602905\n",
      "Epoch 26, Mean Train Loss: 1.256066515881528, Val Loss (1 sample): 1.5973323583602905\n",
      "Train Iter:  1  Loss:  1.1673964262008667\n",
      "Train Iter:  2  Loss:  1.1779489517211914\n",
      "Train Iter:  3  Loss:  1.2703205744425456\n",
      "Train Iter:  4  Loss:  1.2523208856582642\n",
      "Train Iter:  5  Loss:  1.2831942558288574\n",
      "Train Iter:  6  Loss:  1.2395207285881042\n",
      "Train Iter:  7  Loss:  1.215027093887329\n",
      "Train Iter:  8  Loss:  1.1956112384796143\n",
      "Train Iter:  9  Loss:  1.1878373622894287\n",
      "Train Iter:  10  Loss:  1.1842618942260743\n",
      "Train Iter:  11  Loss:  1.1819119236686013\n",
      "Train Iter:  12  Loss:  1.17752210299174\n",
      "Train Iter:  13  Loss:  1.155112046461839\n",
      "Train Iter:  14  Loss:  1.1639509115900313\n",
      "Train Iter:  15  Loss:  1.1719123045603435\n",
      "Train Iter:  16  Loss:  1.2060299143195152\n",
      "Train Iter:  17  Loss:  1.2074903389986824\n",
      "Train Iter:  18  Loss:  1.1998110347323947\n",
      "Train Iter:  19  Loss:  1.2216477143137079\n",
      "Train Iter:  20  Loss:  1.2335830807685852\n",
      "Train Iter:  21  Loss:  1.2381450164885748\n",
      "Train Iter:  22  Loss:  1.2345047809860923\n",
      "Train Iter:  23  Loss:  1.2372365412504778\n",
      "Train Iter:  24  Loss:  1.2380485286315281\n",
      "Train Iter:  25  Loss:  1.2394170618057252\n",
      "Train Iter:  26  Loss:  1.2399177688818712\n",
      "Train Iter:  27  Loss:  1.2318494893886425\n",
      "Train Iter:  28  Loss:  1.232117282492774\n",
      "Train Iter:  29  Loss:  1.2326418284712166\n",
      "Train Iter:  30  Loss:  1.2356900056203206\n",
      "Train Iter:  31  Loss:  1.2328790849254978\n",
      "Train Iter:  32  Loss:  1.2263630144298077\n",
      "Train Iter:  33  Loss:  1.2291561076135347\n",
      "Train Iter:  34  Loss:  1.2316957081065458\n",
      "Train Iter:  35  Loss:  1.2358500616891044\n",
      "Train Iter:  36  Loss:  1.2289569973945618\n",
      "Train Iter:  37  Loss:  1.214061372988933\n",
      "Train Iter:  38  Loss:  1.225011053838228\n",
      "Train Iter:  39  Loss:  1.2300861921065893\n",
      "Train Iter:  40  Loss:  1.237297424674034\n",
      "Train Iter:  41  Loss:  1.2333032590586965\n",
      "Train Iter:  42  Loss:  1.2265308981850034\n",
      "Train Iter:  43  Loss:  1.2297492055005805\n",
      "Train Iter:  44  Loss:  1.2341431758620522\n",
      "Train Iter:  45  Loss:  1.238285862074958\n",
      "Train Iter:  46  Loss:  1.2380782391714014\n",
      "Train Iter:  47  Loss:  1.2356918314669996\n",
      "Train Iter:  48  Loss:  1.2345712060729663\n",
      "Train Iter:  49  Loss:  1.2267462696347917\n",
      "Train Iter:  50  Loss:  1.227910304069519\n",
      "Train Iter:  51  Loss:  1.2354324448342417\n",
      "Train Iter:  52  Loss:  1.2366936734089484\n",
      "Train Iter:  53  Loss:  1.2369667111702685\n",
      "Train Iter:  54  Loss:  1.2351441427513405\n",
      "Train Iter:  55  Loss:  1.2369775620373813\n",
      "Train Iter:  56  Loss:  1.234862859760012\n",
      "Train Iter:  57  Loss:  1.2278379139147306\n",
      "Train Iter:  58  Loss:  1.2246471890087784\n",
      "Train Iter:  59  Loss:  1.2304132833319195\n",
      "Train Iter:  60  Loss:  1.2282331943511964\n",
      "Train Iter:  61  Loss:  1.2221243449899017\n",
      "Train Iter:  62  Loss:  1.2124233111258476\n",
      "Train Iter:  63  Loss:  1.2090363985016233\n",
      "Train Iter:  64  Loss:  1.2082252176478505\n",
      "Train Iter:  65  Loss:  1.2106892796663138\n",
      "Train Iter:  66  Loss:  1.2123379066134945\n",
      "Train Iter:  67  Loss:  1.2145993415989094\n",
      "Train Iter:  68  Loss:  1.215783963308615\n",
      "Train Iter:  69  Loss:  1.2171156674191572\n",
      "Train Iter:  70  Loss:  1.2196455998080118\n",
      "Train Iter:  71  Loss:  1.2289748821460025\n",
      "Train Iter:  72  Loss:  1.2284606918692589\n",
      "Train Iter:  73  Loss:  1.231789232116856\n",
      "Train Iter:  74  Loss:  1.2328739013220813\n",
      "Train Iter:  75  Loss:  1.2302117260297138\n",
      "Train Iter:  76  Loss:  1.2324538819099728\n",
      "Train Iter:  77  Loss:  1.2334863466101806\n",
      "Train Iter:  78  Loss:  1.232432519014065\n",
      "Train Iter:  79  Loss:  1.2337363362312317\n",
      "Train Iter:  80  Loss:  1.2378767050802708\n",
      "Train Iter:  81  Loss:  1.2389643170215465\n",
      "Train Iter:  82  Loss:  1.2443816523726394\n",
      "Train Iter:  83  Loss:  1.2491493648793324\n",
      "Train Iter:  84  Loss:  1.2509168606428873\n",
      "Train Iter:  85  Loss:  1.2505028984125923\n",
      "Train Iter:  86  Loss:  1.2486671493497006\n",
      "Train Iter:  87  Loss:  1.2497412302028175\n",
      "Train Iter:  88  Loss:  1.2472578334537419\n",
      "Train Iter:  89  Loss:  1.251875078410245\n",
      "Train Iter:  90  Loss:  1.2549637125598059\n",
      "Train Iter:  91  Loss:  1.2568496223334427\n",
      "Train Iter:  92  Loss:  1.2534575773322063\n",
      "Train Iter:  93  Loss:  1.2547775647973503\n",
      "Train Iter:  94  Loss:  1.2522387212895332\n",
      "Train Iter:  95  Loss:  1.2517739697506554\n",
      "Train Iter:  96  Loss:  1.251383550465107\n",
      "Train Iter:  97  Loss:  1.2494316617238153\n",
      "Train Iter:  98  Loss:  1.2479280938907547\n",
      "Train Iter:  99  Loss:  1.2478798230489094\n",
      "Train Iter:  100  Loss:  1.2450799983739853\n",
      "Train Iter:  101  Loss:  1.2447996700164115\n",
      "Train Iter:  102  Loss:  1.2420452289721544\n",
      "Train Iter:  103  Loss:  1.2447458303090437\n",
      "Train Iter:  104  Loss:  1.2417207767183964\n",
      "Train Iter:  105  Loss:  1.2413173953692118\n",
      "Train Iter:  106  Loss:  1.241168663749155\n",
      "Train Iter:  107  Loss:  1.240054397382469\n",
      "Train Iter:  108  Loss:  1.2371596522905208\n",
      "Train Iter:  109  Loss:  1.2360075039601108\n",
      "Train Iter:  110  Loss:  1.2383532973853024\n",
      "Train Iter:  111  Loss:  1.2349368194202046\n",
      "Train Iter:  112  Loss:  1.2339441616620337\n",
      "Train Iter:  113  Loss:  1.2321532184043817\n",
      "Train Iter:  114  Loss:  1.2309084649671589\n",
      "Train Iter:  115  Loss:  1.2283801260201828\n",
      "Train Iter:  116  Loss:  1.2290942992629676\n",
      "Train Iter:  117  Loss:  1.2303273234611902\n",
      "Train Iter:  118  Loss:  1.2281760113724207\n",
      "Train Iter:  119  Loss:  1.2281461468263835\n",
      "Train Iter:  120  Loss:  1.230804492533207\n",
      "Train Iter:  121  Loss:  1.2318494580993968\n",
      "Train Iter:  122  Loss:  1.2301125687653902\n",
      "Train Iter:  123  Loss:  1.2309374329520435\n",
      "Train Iter:  124  Loss:  1.2325931398137924\n",
      "Train Iter:  125  Loss:  1.2309904294013978\n",
      "Train Iter:  126  Loss:  1.229216056683707\n",
      "Train Iter:  127  Loss:  1.2323121681926756\n",
      "Train Iter:  128  Loss:  1.231315775308758\n",
      "Train Iter:  129  Loss:  1.2313908250756966\n",
      "Train Iter:  130  Loss:  1.2295035880345564\n",
      "Train Iter:  131  Loss:  1.2308274703171418\n",
      "Train Iter:  132  Loss:  1.2311769981275906\n",
      "Train Iter:  133  Loss:  1.2302231873784746\n",
      "Train Iter:  134  Loss:  1.2318734273092071\n",
      "Train Iter:  135  Loss:  1.2324323411341067\n",
      "Train Iter:  136  Loss:  1.2316795101060587\n",
      "Train Iter:  137  Loss:  1.2335711217274632\n",
      "Train Iter:  138  Loss:  1.2342320244381393\n",
      "Train Iter:  139  Loss:  1.2336886876778637\n",
      "Train Iter:  140  Loss:  1.233675453066826\n",
      "Train Iter:  141  Loss:  1.2363407920438347\n",
      "Train Iter:  142  Loss:  1.2367701845269807\n",
      "Train Iter:  143  Loss:  1.2362562847304177\n",
      "Train Iter:  144  Loss:  1.235831732965178\n",
      "Train Iter:  145  Loss:  1.238261457558336\n",
      "Train Iter:  146  Loss:  1.2403353407774886\n",
      "Train Iter:  147  Loss:  1.2408716820535206\n",
      "Train Iter:  148  Loss:  1.2418518215417862\n",
      "Train Iter:  149  Loss:  1.240006918475132\n",
      "Train Iter:  150  Loss:  1.2405881798267364\n",
      "Train Iter:  151  Loss:  1.238338756245493\n",
      "Train Iter:  152  Loss:  1.2390169681687104\n",
      "Train Iter:  153  Loss:  1.238790627398522\n",
      "Train Iter:  154  Loss:  1.2381901501061081\n",
      "Train Iter:  155  Loss:  1.2364752138814619\n",
      "Train Iter:  156  Loss:  1.2360762388278277\n",
      "Train Iter:  157  Loss:  1.236309762972935\n",
      "Train Iter:  158  Loss:  1.2347916990895815\n",
      "Train Iter:  159  Loss:  1.2330910750904922\n",
      "Train Iter:  160  Loss:  1.2333632949739695\n",
      "Train Iter:  161  Loss:  1.2326900859056793\n",
      "Train Iter:  162  Loss:  1.2312479618890786\n",
      "Train Iter:  163  Loss:  1.2295500376473176\n",
      "Train Iter:  164  Loss:  1.2296471857443088\n",
      "Train Iter:  165  Loss:  1.228699703650041\n",
      "Train Iter:  166  Loss:  1.2280827875596931\n",
      "Train Iter:  167  Loss:  1.2267612440143516\n",
      "Train Iter:  168  Loss:  1.227077746675128\n",
      "Train Iter:  169  Loss:  1.22496768426613\n",
      "Train Iter:  170  Loss:  1.2261918215190664\n",
      "Train Iter:  171  Loss:  1.2268902628045333\n",
      "Train Iter:  172  Loss:  1.2261880747107572\n",
      "Train Iter:  173  Loss:  1.2282014251444382\n",
      "Train Iter:  174  Loss:  1.228065540735749\n",
      "Train Iter:  175  Loss:  1.2321145956856865\n",
      "Train Iter:  176  Loss:  1.2319419675252654\n",
      "Train Iter:  177  Loss:  1.231711634808341\n",
      "Train Iter:  178  Loss:  1.2339782359894742\n",
      "Train Iter:  179  Loss:  1.2341947595500413\n",
      "Train Iter:  180  Loss:  1.2362333754698436\n",
      "Train Iter:  181  Loss:  1.236747872104961\n",
      "Train Iter:  182  Loss:  1.2355381158682017\n",
      "Train Iter:  183  Loss:  1.2371784650562891\n",
      "Train Iter:  184  Loss:  1.2407167916712554\n",
      "Train Iter:  185  Loss:  1.2411495595364956\n",
      "Train Iter:  186  Loss:  1.2402476687585153\n",
      "Val Iter: Loss:  0.9976663589477539\n",
      "Epoch 27, Mean Train Loss: 1.2402476687585153, Val Loss (1 sample): 0.9976663589477539\n",
      "Train Iter:  1  Loss:  1.3154090642929077\n",
      "Train Iter:  2  Loss:  1.1489169597625732\n",
      "Train Iter:  3  Loss:  1.1560228268305461\n",
      "Train Iter:  4  Loss:  1.155726283788681\n",
      "Train Iter:  5  Loss:  1.1506718873977662\n",
      "Train Iter:  6  Loss:  1.1496513485908508\n",
      "Train Iter:  7  Loss:  1.2332683631352015\n",
      "Train Iter:  8  Loss:  1.2276550382375717\n",
      "Train Iter:  9  Loss:  1.19734874036577\n",
      "Train Iter:  10  Loss:  1.217254137992859\n",
      "Train Iter:  11  Loss:  1.2112674929878928\n",
      "Train Iter:  12  Loss:  1.2174130082130432\n",
      "Train Iter:  13  Loss:  1.2354114055633545\n",
      "Train Iter:  14  Loss:  1.2358576229640417\n",
      "Train Iter:  15  Loss:  1.2618540128072102\n",
      "Train Iter:  16  Loss:  1.2616943642497063\n",
      "Train Iter:  17  Loss:  1.3007475628572351\n",
      "Train Iter:  18  Loss:  1.2890829311476812\n",
      "Train Iter:  19  Loss:  1.292310871576008\n",
      "Train Iter:  20  Loss:  1.3018548786640167\n",
      "Train Iter:  21  Loss:  1.3178531499136061\n",
      "Train Iter:  22  Loss:  1.3162986690347844\n",
      "Train Iter:  23  Loss:  1.3082651884659477\n",
      "Train Iter:  24  Loss:  1.3096460849046707\n",
      "Train Iter:  25  Loss:  1.3059144496917725\n",
      "Train Iter:  26  Loss:  1.294512546979464\n",
      "Train Iter:  27  Loss:  1.3057171592005976\n",
      "Train Iter:  28  Loss:  1.3040361489568437\n",
      "Train Iter:  29  Loss:  1.3116504159466973\n",
      "Train Iter:  30  Loss:  1.3034053365389506\n",
      "Train Iter:  31  Loss:  1.3051514625549316\n",
      "Train Iter:  32  Loss:  1.2971745654940605\n",
      "Train Iter:  33  Loss:  1.290843017173536\n",
      "Train Iter:  34  Loss:  1.3009243291967056\n",
      "Train Iter:  35  Loss:  1.3002242258616856\n",
      "Train Iter:  36  Loss:  1.3018966515858967\n",
      "Train Iter:  37  Loss:  1.3022600573462408\n",
      "Train Iter:  38  Loss:  1.308262536400243\n",
      "Train Iter:  39  Loss:  1.3096640507380168\n",
      "Train Iter:  40  Loss:  1.3062714159488678\n",
      "Train Iter:  41  Loss:  1.296181635158818\n",
      "Train Iter:  42  Loss:  1.2962150744029455\n",
      "Train Iter:  43  Loss:  1.2942103674245435\n",
      "Train Iter:  44  Loss:  1.293439507484436\n",
      "Train Iter:  45  Loss:  1.286247271961636\n",
      "Train Iter:  46  Loss:  1.2784047165642614\n",
      "Train Iter:  47  Loss:  1.2870438415953454\n",
      "Train Iter:  48  Loss:  1.285031359642744\n",
      "Train Iter:  49  Loss:  1.280656443566692\n",
      "Train Iter:  50  Loss:  1.2810590589046478\n",
      "Train Iter:  51  Loss:  1.27132248878479\n",
      "Train Iter:  52  Loss:  1.2661771751367128\n",
      "Train Iter:  53  Loss:  1.266009456706497\n",
      "Train Iter:  54  Loss:  1.2594439486662548\n",
      "Train Iter:  55  Loss:  1.260703066262332\n",
      "Train Iter:  56  Loss:  1.2580947460872787\n",
      "Train Iter:  57  Loss:  1.260238974763636\n",
      "Train Iter:  58  Loss:  1.2628569880436207\n",
      "Train Iter:  59  Loss:  1.256720063039812\n",
      "Train Iter:  60  Loss:  1.2646200388669968\n",
      "Train Iter:  61  Loss:  1.2681954034039231\n",
      "Train Iter:  62  Loss:  1.2720508681189628\n",
      "Train Iter:  63  Loss:  1.267561774405222\n",
      "Train Iter:  64  Loss:  1.2670646887272596\n",
      "Train Iter:  65  Loss:  1.2672893028992873\n",
      "Train Iter:  66  Loss:  1.2655787504080571\n",
      "Train Iter:  67  Loss:  1.2669035641115103\n",
      "Train Iter:  68  Loss:  1.2660006968414081\n",
      "Train Iter:  69  Loss:  1.2646333929421245\n",
      "Train Iter:  70  Loss:  1.2616911343165806\n",
      "Train Iter:  71  Loss:  1.2632212336634245\n",
      "Train Iter:  72  Loss:  1.2618293397956424\n",
      "Train Iter:  73  Loss:  1.2626586727900049\n",
      "Train Iter:  74  Loss:  1.263745212877119\n",
      "Train Iter:  75  Loss:  1.2628826983769734\n",
      "Train Iter:  76  Loss:  1.261464358944642\n",
      "Train Iter:  77  Loss:  1.2650781064838559\n",
      "Train Iter:  78  Loss:  1.2641104857126872\n",
      "Train Iter:  79  Loss:  1.2639955508558056\n",
      "Train Iter:  80  Loss:  1.2662920907139779\n",
      "Train Iter:  81  Loss:  1.271760450469123\n",
      "Train Iter:  82  Loss:  1.2681614225957452\n",
      "Train Iter:  83  Loss:  1.2685024874756134\n",
      "Train Iter:  84  Loss:  1.2679328670104344\n",
      "Train Iter:  85  Loss:  1.2678420003722697\n",
      "Train Iter:  86  Loss:  1.2687654127908308\n",
      "Train Iter:  87  Loss:  1.2665162134444576\n",
      "Train Iter:  88  Loss:  1.2706394093957813\n",
      "Train Iter:  89  Loss:  1.2651479157169214\n",
      "Train Iter:  90  Loss:  1.2656856490506065\n",
      "Train Iter:  91  Loss:  1.265457114675543\n",
      "Train Iter:  92  Loss:  1.2682500928640366\n",
      "Train Iter:  93  Loss:  1.270716299933772\n",
      "Train Iter:  94  Loss:  1.2730474224750032\n",
      "Train Iter:  95  Loss:  1.2726036190986634\n",
      "Train Iter:  96  Loss:  1.2736671871195238\n",
      "Train Iter:  97  Loss:  1.2762774161456787\n",
      "Train Iter:  98  Loss:  1.274152426695337\n",
      "Train Iter:  99  Loss:  1.273544515624191\n",
      "Train Iter:  100  Loss:  1.2731849986314774\n",
      "Train Iter:  101  Loss:  1.2700515554683043\n",
      "Train Iter:  102  Loss:  1.2706832412411184\n",
      "Train Iter:  103  Loss:  1.2698866968016023\n",
      "Train Iter:  104  Loss:  1.2693946092174604\n",
      "Train Iter:  105  Loss:  1.2661116350264776\n",
      "Train Iter:  106  Loss:  1.2654551031454555\n",
      "Train Iter:  107  Loss:  1.2632347311929009\n",
      "Train Iter:  108  Loss:  1.2629775879559693\n",
      "Train Iter:  109  Loss:  1.2630926687783057\n",
      "Train Iter:  110  Loss:  1.2597629189491273\n",
      "Train Iter:  111  Loss:  1.2595246353664913\n",
      "Train Iter:  112  Loss:  1.2579861187509127\n",
      "Train Iter:  113  Loss:  1.2588558967134593\n",
      "Train Iter:  114  Loss:  1.2568773480883815\n",
      "Train Iter:  115  Loss:  1.258508969389874\n",
      "Train Iter:  116  Loss:  1.2554654330015182\n",
      "Train Iter:  117  Loss:  1.2547310412439525\n",
      "Train Iter:  118  Loss:  1.2552860838881994\n",
      "Train Iter:  119  Loss:  1.2528318001442598\n",
      "Train Iter:  120  Loss:  1.2542386417587599\n",
      "Train Iter:  121  Loss:  1.2526252235262847\n",
      "Train Iter:  122  Loss:  1.2546815368972841\n",
      "Train Iter:  123  Loss:  1.252212982352187\n",
      "Train Iter:  124  Loss:  1.255307035580758\n",
      "Train Iter:  125  Loss:  1.255055037021637\n",
      "Train Iter:  126  Loss:  1.2542349653584617\n",
      "Train Iter:  127  Loss:  1.253288427206475\n",
      "Train Iter:  128  Loss:  1.2510351832024753\n",
      "Train Iter:  129  Loss:  1.24972783733708\n",
      "Train Iter:  130  Loss:  1.2491961126144115\n",
      "Train Iter:  131  Loss:  1.2478509599925907\n",
      "Train Iter:  132  Loss:  1.2493830797347156\n",
      "Train Iter:  133  Loss:  1.2483267394223607\n",
      "Train Iter:  134  Loss:  1.2476160112601609\n",
      "Train Iter:  135  Loss:  1.247769973454652\n",
      "Train Iter:  136  Loss:  1.2472948410055216\n",
      "Train Iter:  137  Loss:  1.2441999063874682\n",
      "Train Iter:  138  Loss:  1.246071340813153\n",
      "Train Iter:  139  Loss:  1.243899275501855\n",
      "Train Iter:  140  Loss:  1.2424771381275994\n",
      "Train Iter:  141  Loss:  1.2425583241679143\n",
      "Train Iter:  142  Loss:  1.2451184933454218\n",
      "Train Iter:  143  Loss:  1.2446265174792364\n",
      "Train Iter:  144  Loss:  1.2445590243571334\n",
      "Train Iter:  145  Loss:  1.2430475896802442\n",
      "Train Iter:  146  Loss:  1.2420449750880673\n",
      "Train Iter:  147  Loss:  1.2429356862898586\n",
      "Train Iter:  148  Loss:  1.241585215603983\n",
      "Train Iter:  149  Loss:  1.242384979788889\n",
      "Train Iter:  150  Loss:  1.2415949141979217\n",
      "Train Iter:  151  Loss:  1.2405210034736733\n",
      "Train Iter:  152  Loss:  1.241848947578355\n",
      "Train Iter:  153  Loss:  1.2427211115562837\n",
      "Train Iter:  154  Loss:  1.2439683548041753\n",
      "Train Iter:  155  Loss:  1.2427482178134304\n",
      "Train Iter:  156  Loss:  1.2441015996229954\n",
      "Train Iter:  157  Loss:  1.2437785093192082\n",
      "Train Iter:  158  Loss:  1.2455933565580393\n",
      "Train Iter:  159  Loss:  1.247584897392201\n",
      "Train Iter:  160  Loss:  1.2497000824660063\n",
      "Train Iter:  161  Loss:  1.2488683502866615\n",
      "Train Iter:  162  Loss:  1.246249109506607\n",
      "Train Iter:  163  Loss:  1.2473314762846823\n",
      "Train Iter:  164  Loss:  1.2471504215060212\n",
      "Train Iter:  165  Loss:  1.2486431436105208\n",
      "Train Iter:  166  Loss:  1.2498857368187732\n",
      "Train Iter:  167  Loss:  1.2499079679300684\n",
      "Train Iter:  168  Loss:  1.2511355511489368\n",
      "Train Iter:  169  Loss:  1.2498537517158237\n",
      "Train Iter:  170  Loss:  1.251389677734936\n",
      "Train Iter:  171  Loss:  1.2534502915471617\n",
      "Train Iter:  172  Loss:  1.2539513939341833\n",
      "Train Iter:  173  Loss:  1.256259387283656\n",
      "Train Iter:  174  Loss:  1.2567636579617687\n",
      "Train Iter:  175  Loss:  1.256564073222024\n",
      "Train Iter:  176  Loss:  1.256185168908401\n",
      "Train Iter:  177  Loss:  1.254468945460131\n",
      "Train Iter:  178  Loss:  1.253583105092638\n",
      "Train Iter:  179  Loss:  1.2532914210964181\n",
      "Train Iter:  180  Loss:  1.2521572927633922\n",
      "Train Iter:  181  Loss:  1.2535444906403346\n",
      "Train Iter:  182  Loss:  1.252431610783378\n",
      "Train Iter:  183  Loss:  1.2532887100521983\n",
      "Train Iter:  184  Loss:  1.252924122888109\n",
      "Train Iter:  185  Loss:  1.2542610522863027\n",
      "Train Iter:  186  Loss:  1.2529628654961944\n",
      "Val Iter: Loss:  1.1567631959915161\n",
      "Epoch 28, Mean Train Loss: 1.2529628654961944, Val Loss (1 sample): 1.1567631959915161\n",
      "Train Iter:  1  Loss:  1.084108829498291\n",
      "Train Iter:  2  Loss:  1.0319821238517761\n",
      "Train Iter:  3  Loss:  0.9831087390581766\n",
      "Train Iter:  4  Loss:  1.0460156351327896\n",
      "Train Iter:  5  Loss:  1.0603670954704285\n",
      "Train Iter:  6  Loss:  1.1048154731591542\n",
      "Train Iter:  7  Loss:  1.1051606025014604\n",
      "Train Iter:  8  Loss:  1.1190261468291283\n",
      "Train Iter:  9  Loss:  1.1390198138025072\n",
      "Train Iter:  10  Loss:  1.122990107536316\n",
      "Train Iter:  11  Loss:  1.11879440871152\n",
      "Train Iter:  12  Loss:  1.1372751792271931\n",
      "Train Iter:  13  Loss:  1.1296739945044885\n",
      "Train Iter:  14  Loss:  1.162682490689414\n",
      "Train Iter:  15  Loss:  1.1502457499504088\n",
      "Train Iter:  16  Loss:  1.1589245311915874\n",
      "Train Iter:  17  Loss:  1.1637814921491287\n",
      "Train Iter:  18  Loss:  1.1853670842117734\n",
      "Train Iter:  19  Loss:  1.1917962494649386\n",
      "Train Iter:  20  Loss:  1.191352155804634\n",
      "Train Iter:  21  Loss:  1.175542910893758\n",
      "Train Iter:  22  Loss:  1.1626068949699402\n",
      "Train Iter:  23  Loss:  1.154945028864819\n",
      "Train Iter:  24  Loss:  1.14810607333978\n",
      "Train Iter:  25  Loss:  1.1510882902145385\n",
      "Train Iter:  26  Loss:  1.1534281235474806\n",
      "Train Iter:  27  Loss:  1.1553726770259716\n",
      "Train Iter:  28  Loss:  1.1536499176706587\n",
      "Train Iter:  29  Loss:  1.1566236183561127\n",
      "Train Iter:  30  Loss:  1.1612014691034953\n",
      "Train Iter:  31  Loss:  1.1686842057012743\n",
      "Train Iter:  32  Loss:  1.1614077854901552\n",
      "Train Iter:  33  Loss:  1.1616846413323374\n",
      "Train Iter:  34  Loss:  1.1717401234542622\n",
      "Train Iter:  35  Loss:  1.17380485023771\n",
      "Train Iter:  36  Loss:  1.1698696729209688\n",
      "Train Iter:  37  Loss:  1.177094760778788\n",
      "Train Iter:  38  Loss:  1.17900570913365\n",
      "Train Iter:  39  Loss:  1.1824533435014577\n",
      "Train Iter:  40  Loss:  1.178037779033184\n",
      "Train Iter:  41  Loss:  1.1832058967613592\n",
      "Train Iter:  42  Loss:  1.1975946468966348\n",
      "Train Iter:  43  Loss:  1.2104653106179348\n",
      "Train Iter:  44  Loss:  1.2033125934275715\n",
      "Train Iter:  45  Loss:  1.1979615211486816\n",
      "Train Iter:  46  Loss:  1.196139104988264\n",
      "Train Iter:  47  Loss:  1.1968615080447906\n",
      "Train Iter:  48  Loss:  1.2016589889923732\n",
      "Train Iter:  49  Loss:  1.204485501561846\n",
      "Train Iter:  50  Loss:  1.2017151832580566\n",
      "Train Iter:  51  Loss:  1.2010413314781936\n",
      "Train Iter:  52  Loss:  1.1998147116257594\n",
      "Train Iter:  53  Loss:  1.1979228932902497\n",
      "Train Iter:  54  Loss:  1.1963418148182057\n",
      "Train Iter:  55  Loss:  1.1951120159842752\n",
      "Train Iter:  56  Loss:  1.1947689269270216\n",
      "Train Iter:  57  Loss:  1.1946234347527487\n",
      "Train Iter:  58  Loss:  1.1949697691818764\n",
      "Train Iter:  59  Loss:  1.1972822698496155\n",
      "Train Iter:  60  Loss:  1.1990827103455861\n",
      "Train Iter:  61  Loss:  1.2053498615984057\n",
      "Train Iter:  62  Loss:  1.205114414615016\n",
      "Train Iter:  63  Loss:  1.2052479357946486\n",
      "Train Iter:  64  Loss:  1.202508358284831\n",
      "Train Iter:  65  Loss:  1.2048308610916139\n",
      "Train Iter:  66  Loss:  1.211468927788012\n",
      "Train Iter:  67  Loss:  1.2112155636744713\n",
      "Train Iter:  68  Loss:  1.2149276663275326\n",
      "Train Iter:  69  Loss:  1.2155262439147285\n",
      "Train Iter:  70  Loss:  1.214502283505031\n",
      "Train Iter:  71  Loss:  1.212569040311894\n",
      "Train Iter:  72  Loss:  1.214489072561264\n",
      "Train Iter:  73  Loss:  1.2245138752950382\n",
      "Train Iter:  74  Loss:  1.222017504073478\n",
      "Train Iter:  75  Loss:  1.2223757855097452\n",
      "Train Iter:  76  Loss:  1.223000887193178\n",
      "Train Iter:  77  Loss:  1.2249951749653012\n",
      "Train Iter:  78  Loss:  1.2213084965180128\n",
      "Train Iter:  79  Loss:  1.2179499577872361\n",
      "Train Iter:  80  Loss:  1.2179108917713166\n",
      "Train Iter:  81  Loss:  1.2191495306697893\n",
      "Train Iter:  82  Loss:  1.2180098338824947\n",
      "Train Iter:  83  Loss:  1.2224586440856198\n",
      "Train Iter:  84  Loss:  1.2255769769350688\n",
      "Train Iter:  85  Loss:  1.2279480864019956\n",
      "Train Iter:  86  Loss:  1.2291889523350916\n",
      "Train Iter:  87  Loss:  1.2277894074889435\n",
      "Train Iter:  88  Loss:  1.2282974002036182\n",
      "Train Iter:  89  Loss:  1.2266175505820285\n",
      "Train Iter:  90  Loss:  1.2263345572683546\n",
      "Train Iter:  91  Loss:  1.224523275763124\n",
      "Train Iter:  92  Loss:  1.2239824805570685\n",
      "Train Iter:  93  Loss:  1.2254397048745105\n",
      "Train Iter:  94  Loss:  1.2285397521992947\n",
      "Train Iter:  95  Loss:  1.2266325837687442\n",
      "Train Iter:  96  Loss:  1.2295012325048447\n",
      "Train Iter:  97  Loss:  1.2277923429135196\n",
      "Train Iter:  98  Loss:  1.2285073496857468\n",
      "Train Iter:  99  Loss:  1.2275750239690144\n",
      "Train Iter:  100  Loss:  1.228894374370575\n",
      "Train Iter:  101  Loss:  1.2257821854978506\n",
      "Train Iter:  102  Loss:  1.2226815013324512\n",
      "Train Iter:  103  Loss:  1.2245710914574781\n",
      "Train Iter:  104  Loss:  1.2266500271283662\n",
      "Train Iter:  105  Loss:  1.2289439031055995\n",
      "Train Iter:  106  Loss:  1.2280311831888162\n",
      "Train Iter:  107  Loss:  1.22720581683043\n",
      "Train Iter:  108  Loss:  1.2289037428520344\n",
      "Train Iter:  109  Loss:  1.228718558582691\n",
      "Train Iter:  110  Loss:  1.226614380424673\n",
      "Train Iter:  111  Loss:  1.225729411249762\n",
      "Train Iter:  112  Loss:  1.2223341028605188\n",
      "Train Iter:  113  Loss:  1.2232974904828366\n",
      "Train Iter:  114  Loss:  1.2281694997820938\n",
      "Train Iter:  115  Loss:  1.2263562731120898\n",
      "Train Iter:  116  Loss:  1.224845339512003\n",
      "Train Iter:  117  Loss:  1.2274178521245973\n",
      "Train Iter:  118  Loss:  1.225901279409053\n",
      "Train Iter:  119  Loss:  1.2250275421543282\n",
      "Train Iter:  120  Loss:  1.226707457502683\n",
      "Train Iter:  121  Loss:  1.226046559239222\n",
      "Train Iter:  122  Loss:  1.2244955340369803\n",
      "Train Iter:  123  Loss:  1.2267341797913962\n",
      "Train Iter:  124  Loss:  1.2265813735223585\n",
      "Train Iter:  125  Loss:  1.2281581983566283\n",
      "Train Iter:  126  Loss:  1.2278140736004663\n",
      "Train Iter:  127  Loss:  1.2282251673420583\n",
      "Train Iter:  128  Loss:  1.22839099727571\n",
      "Train Iter:  129  Loss:  1.2299507745476657\n",
      "Train Iter:  130  Loss:  1.2307721880766063\n",
      "Train Iter:  131  Loss:  1.2304608658069873\n",
      "Train Iter:  132  Loss:  1.2300864078781821\n",
      "Train Iter:  133  Loss:  1.2315287769288945\n",
      "Train Iter:  134  Loss:  1.2287857483572036\n",
      "Train Iter:  135  Loss:  1.2286180871504324\n",
      "Train Iter:  136  Loss:  1.2301278057343819\n",
      "Train Iter:  137  Loss:  1.2281585226963907\n",
      "Train Iter:  138  Loss:  1.2313392343728438\n",
      "Train Iter:  139  Loss:  1.2295299026605895\n",
      "Train Iter:  140  Loss:  1.2294982318367278\n",
      "Train Iter:  141  Loss:  1.2285565015272046\n",
      "Train Iter:  142  Loss:  1.2283039575731252\n",
      "Train Iter:  143  Loss:  1.22716436561171\n",
      "Train Iter:  144  Loss:  1.2266596485343244\n",
      "Train Iter:  145  Loss:  1.228177457431267\n",
      "Train Iter:  146  Loss:  1.224872993688061\n",
      "Train Iter:  147  Loss:  1.2274624867504145\n",
      "Train Iter:  148  Loss:  1.226194146919895\n",
      "Train Iter:  149  Loss:  1.2265479392653345\n",
      "Train Iter:  150  Loss:  1.2272654712200164\n",
      "Train Iter:  151  Loss:  1.2254968660556718\n",
      "Train Iter:  152  Loss:  1.2263419949694683\n",
      "Train Iter:  153  Loss:  1.2286045652588988\n",
      "Train Iter:  154  Loss:  1.2299549455766554\n",
      "Train Iter:  155  Loss:  1.2287792005846578\n",
      "Train Iter:  156  Loss:  1.2281574240097632\n",
      "Train Iter:  157  Loss:  1.2266650606112874\n",
      "Train Iter:  158  Loss:  1.2271223698235765\n",
      "Train Iter:  159  Loss:  1.2268791052530397\n",
      "Train Iter:  160  Loss:  1.2255487699061631\n",
      "Train Iter:  161  Loss:  1.2272489415192456\n",
      "Train Iter:  162  Loss:  1.2276359314535872\n",
      "Train Iter:  163  Loss:  1.228883832144591\n",
      "Train Iter:  164  Loss:  1.2292410880327225\n",
      "Train Iter:  165  Loss:  1.2309990720315414\n",
      "Train Iter:  166  Loss:  1.2312513952513775\n",
      "Train Iter:  167  Loss:  1.2286794767408313\n",
      "Train Iter:  168  Loss:  1.228393714697588\n",
      "Train Iter:  169  Loss:  1.2276995855675648\n",
      "Train Iter:  170  Loss:  1.2277079052784863\n",
      "Train Iter:  171  Loss:  1.2313147180261668\n",
      "Train Iter:  172  Loss:  1.2315044822387917\n",
      "Train Iter:  173  Loss:  1.233694305654206\n",
      "Train Iter:  174  Loss:  1.234707814180988\n",
      "Train Iter:  175  Loss:  1.2361390634945462\n",
      "Train Iter:  176  Loss:  1.2361055886880918\n",
      "Train Iter:  177  Loss:  1.2337541812557284\n",
      "Train Iter:  178  Loss:  1.235453409760186\n",
      "Train Iter:  179  Loss:  1.2362700224588703\n",
      "Train Iter:  180  Loss:  1.2382307724820243\n",
      "Train Iter:  181  Loss:  1.237810011397409\n",
      "Train Iter:  182  Loss:  1.2382605721007336\n",
      "Train Iter:  183  Loss:  1.2384993635891566\n",
      "Train Iter:  184  Loss:  1.239170964969241\n",
      "Train Iter:  185  Loss:  1.2389921803732176\n",
      "Train Iter:  186  Loss:  1.2388752311147668\n",
      "Val Iter: Loss:  1.3003549575805664\n",
      "Epoch 29, Mean Train Loss: 1.2388752311147668, Val Loss (1 sample): 1.3003549575805664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# help from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "losses_tr=[]\n",
    "losses_val=[]\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    learner.train()\n",
    "    train_epoch_loss = 0\n",
    "    val_epoch_loss=0\n",
    "    i=0\n",
    "    for imgs, lbls in dl_train:\n",
    "        \n",
    "        imgs = imgs.to(device=device)\n",
    "        lbls = lbls.to(device=device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        #forward\n",
    "        pred = learner(imgs)\n",
    "        \n",
    "        #shrink labels for nn.CrossEntropy\n",
    "        lbls = lbls.squeeze(dim=1)\n",
    "        loss = criterion(pred, lbls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_epoch_loss += loss.item()\n",
    "\n",
    "        del imgs, lbls, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        i+=1\n",
    "        print(\"Train Iter: \", i,\" Loss: \", train_epoch_loss/i)\n",
    "\n",
    "    saveState(\"save\", startEpoch+1+epoch, learner, optimizer, loss)\n",
    "    \n",
    "    # i=0\n",
    "    with torch.no_grad():\n",
    "      learner.eval()\n",
    "      \n",
    "      imgs, lbls = val_iter.next()\n",
    "      \n",
    "      imgs = imgs.to(device=device)\n",
    "      lbls = lbls.to(device=device)\n",
    "\n",
    "      pred = learner(imgs)\n",
    "      lbls = lbls.squeeze(dim=1)\n",
    "      loss = criterion(pred, lbls)\n",
    "      val_epoch_loss = loss.item()\n",
    "    print(\"Val Iter: Loss: \", val_epoch_loss)\n",
    "      \n",
    "    losses_tr.append(train_epoch_loss/len(dl_train))\n",
    "    losses_val.append(val_epoch_loss)\n",
    "    \n",
    "    print(\"Epoch {}, Mean Train Loss: {}, Val Loss (1 sample): {}\".format(epoch, losses_tr[-1],  losses_val[-1]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUpyPplXi59N"
   },
   "source": [
    "### Evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EzzIDa6mib3O"
   },
   "outputs": [],
   "source": [
    "savesToTest = [5, 10, 15 , 22, 30, 37, 45, 52, 60, 67, 75, 82, 90]\n",
    "valLoss = []\n",
    "trainLoss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tNdhkv4qC0tr",
    "outputId": "b6c6abbc-b745-4dc0-eb73-247d6ad53b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Iter:  1  Loss:  2.0990240573883057\n",
      "Train Iter:  2  Loss:  2.018247604370117\n",
      "Train Iter:  3  Loss:  1.9618510007858276\n",
      "Train Iter:  4  Loss:  1.8019577264785767\n",
      "Train Iter:  5  Loss:  1.8739922046661377\n",
      "Train Iter:  6  Loss:  2.3695545196533203\n",
      "Train Iter:  7  Loss:  2.221893548965454\n",
      "Train Iter:  8  Loss:  2.499788522720337\n",
      "Train Iter:  9  Loss:  2.3715617656707764\n",
      "Train Iter:  10  Loss:  1.7543060779571533\n",
      "Train Iter:  11  Loss:  2.4217073917388916\n",
      "Train Iter:  12  Loss:  1.7611764669418335\n",
      "Train Iter:  13  Loss:  2.241316080093384\n",
      "Train Iter:  14  Loss:  2.500420331954956\n",
      "Train Iter:  15  Loss:  2.288271903991699\n",
      "Train Iter:  16  Loss:  2.5026373863220215\n",
      "Train Iter:  17  Loss:  2.2317841053009033\n",
      "Train Iter:  18  Loss:  2.715876817703247\n",
      "Train Iter:  19  Loss:  2.476958751678467\n",
      "Train Iter:  20  Loss:  1.9415231943130493\n",
      "Train Iter:  21  Loss:  1.8827944993972778\n",
      "Train Iter:  22  Loss:  2.293013095855713\n",
      "Train Iter:  23  Loss:  2.3712105751037598\n",
      "Train Iter:  24  Loss:  2.1713008880615234\n",
      "Train Iter:  25  Loss:  1.6615561246871948\n",
      "Train Iter:  26  Loss:  2.2280445098876953\n",
      "Train Iter:  27  Loss:  2.1692450046539307\n",
      "Train Iter:  28  Loss:  1.7304991483688354\n",
      "Train Iter:  29  Loss:  2.196169853210449\n",
      "Train Iter:  30  Loss:  1.6768455505371094\n",
      "Train Iter:  31  Loss:  2.242187738418579\n",
      "Train Iter:  32  Loss:  1.9161884784698486\n",
      "Train Iter:  33  Loss:  2.249514579772949\n",
      "Train Iter:  34  Loss:  2.3177847862243652\n",
      "Train Iter:  35  Loss:  2.223529815673828\n",
      "Train Iter:  36  Loss:  2.170154094696045\n",
      "Train Iter:  37  Loss:  2.010592222213745\n",
      "Train Iter:  38  Loss:  2.0035006999969482\n",
      "Train Iter:  39  Loss:  1.8365710973739624\n",
      "Train Iter:  40  Loss:  2.136303186416626\n",
      "Train Iter:  41  Loss:  1.9905567169189453\n",
      "Train Iter:  42  Loss:  2.0803940296173096\n",
      "Train Iter:  43  Loss:  1.8461697101593018\n",
      "Train Iter:  44  Loss:  2.516310691833496\n",
      "Train Iter:  45  Loss:  2.193155288696289\n",
      "Train Iter:  46  Loss:  2.080744981765747\n",
      "Train Iter:  47  Loss:  2.1598398685455322\n",
      "Train Iter:  48  Loss:  2.2228336334228516\n",
      "Train Iter:  49  Loss:  1.6029428243637085\n",
      "Train Iter:  50  Loss:  2.035398006439209\n",
      "Train Iter:  51  Loss:  2.4653444290161133\n",
      "Train Iter:  52  Loss:  2.7074403762817383\n",
      "Train Iter:  53  Loss:  1.9377251863479614\n",
      "Train Iter:  54  Loss:  1.9214380979537964\n",
      "Train Iter:  55  Loss:  2.183955669403076\n",
      "Train Iter:  56  Loss:  2.46217942237854\n",
      "Train Iter:  57  Loss:  2.2398972511291504\n",
      "Train Iter:  58  Loss:  2.0391688346862793\n",
      "Train Iter:  59  Loss:  2.65967059135437\n",
      "Train Iter:  60  Loss:  2.4208672046661377\n",
      "Val Iter:  1  Loss:  2.0088701248168945\n",
      "Val Iter:  2  Loss:  2.0540859699249268\n",
      "Val Iter:  3  Loss:  1.8203357458114624\n",
      "Val Iter:  4  Loss:  1.6292436122894287\n",
      "Val Iter:  5  Loss:  2.2289583683013916\n",
      "Val Iter:  6  Loss:  2.063844680786133\n",
      "Val Iter:  7  Loss:  1.7092957496643066\n",
      "Val Iter:  8  Loss:  2.255906105041504\n",
      "Val Iter:  9  Loss:  1.9710946083068848\n",
      "Val Iter:  10  Loss:  2.075073480606079\n",
      "Val Iter:  11  Loss:  1.8110824823379517\n",
      "Val Iter:  12  Loss:  2.119760513305664\n",
      "Val Iter:  13  Loss:  2.655160665512085\n",
      "Val Iter:  14  Loss:  2.7667760848999023\n",
      "Val Iter:  15  Loss:  2.17354154586792\n",
      "Val Iter:  16  Loss:  2.075855255126953\n",
      "Val Iter:  17  Loss:  2.2454378604888916\n",
      "Val Iter:  18  Loss:  1.9992634057998657\n",
      "Val Iter:  19  Loss:  1.766649842262268\n",
      "Val Iter:  20  Loss:  1.8649146556854248\n",
      "Val Iter:  21  Loss:  2.4140431880950928\n",
      "Val Iter:  22  Loss:  2.567645311355591\n",
      "Val Iter:  23  Loss:  1.6779111623764038\n",
      "Val Iter:  24  Loss:  2.227248430252075\n",
      "Val Iter:  25  Loss:  2.32188081741333\n",
      "Val Iter:  26  Loss:  1.8838770389556885\n",
      "Val Iter:  27  Loss:  1.8702082633972168\n",
      "Val Iter:  28  Loss:  1.9112191200256348\n",
      "Val Iter:  29  Loss:  2.216954469680786\n",
      "Val Iter:  30  Loss:  2.3012804985046387\n",
      "Val Iter:  31  Loss:  2.58611798286438\n",
      "Val Iter:  32  Loss:  1.7930189371109009\n",
      "Val Iter:  33  Loss:  2.4094579219818115\n",
      "Val Iter:  34  Loss:  1.918460488319397\n",
      "Val Iter:  35  Loss:  2.106618881225586\n",
      "Val Iter:  36  Loss:  2.4860963821411133\n",
      "Val Iter:  37  Loss:  1.8197580575942993\n",
      "Val Iter:  38  Loss:  2.516815185546875\n",
      "Val Iter:  39  Loss:  1.8896892070770264\n",
      "Val Iter:  40  Loss:  2.0116121768951416\n",
      "Val Iter:  41  Loss:  2.1769206523895264\n",
      "Val Iter:  42  Loss:  2.48174786567688\n",
      "Val Iter:  43  Loss:  2.757596731185913\n",
      "Val Iter:  44  Loss:  1.8901177644729614\n",
      "Val Iter:  45  Loss:  2.3263189792633057\n",
      "Val Iter:  46  Loss:  2.044072151184082\n",
      "Val Iter:  47  Loss:  2.2853734493255615\n",
      "Val Iter:  48  Loss:  2.24049973487854\n",
      "Val Iter:  49  Loss:  2.0122649669647217\n",
      "Val Iter:  50  Loss:  2.539396047592163\n",
      "Val Iter:  51  Loss:  2.0815608501434326\n",
      "Val Iter:  52  Loss:  2.1435205936431885\n",
      "Val Iter:  53  Loss:  1.9269193410873413\n",
      "Val Iter:  54  Loss:  1.9223272800445557\n",
      "Val Iter:  55  Loss:  1.9380592107772827\n",
      "Val Iter:  56  Loss:  1.7228575944900513\n",
      "Val Iter:  57  Loss:  2.264578104019165\n",
      "Val Iter:  58  Loss:  2.2548720836639404\n",
      "Val Iter:  59  Loss:  2.241142511367798\n",
      "Val Iter:  60  Loss:  2.3977303504943848\n",
      "Val Iter:  61  Loss:  1.8674299716949463\n",
      "Val Iter:  62  Loss:  2.175288438796997\n",
      "Val Iter:  63  Loss:  3.1189420223236084\n",
      "Val Iter:  64  Loss:  2.2958858013153076\n",
      "Val Iter:  65  Loss:  2.1713273525238037\n",
      "Val Iter:  66  Loss:  2.5725598335266113\n",
      "Val Iter:  67  Loss:  1.9028267860412598\n",
      "Val Iter:  68  Loss:  2.4015822410583496\n",
      "Val Iter:  69  Loss:  1.847507119178772\n",
      "Val Iter:  70  Loss:  1.8995720148086548\n",
      "Val Iter:  71  Loss:  2.564577341079712\n",
      "Val Iter:  72  Loss:  2.114032030105591\n",
      "Val Iter:  73  Loss:  2.2789063453674316\n",
      "Val Iter:  74  Loss:  2.6771581172943115\n",
      "Val Iter:  75  Loss:  2.131406784057617\n",
      "Val Iter:  76  Loss:  2.0555644035339355\n",
      "Val Iter:  77  Loss:  2.4683263301849365\n",
      "Val Iter:  78  Loss:  1.7349021434783936\n",
      "Val Iter:  79  Loss:  2.160961389541626\n",
      "Val Iter:  80  Loss:  2.4745795726776123\n",
      "Val Iter:  81  Loss:  2.599789619445801\n",
      "Val Iter:  82  Loss:  2.540389060974121\n",
      "Val Iter:  83  Loss:  2.3296360969543457\n",
      "Val Iter:  84  Loss:  2.081707715988159\n",
      "Val Iter:  85  Loss:  2.2347023487091064\n",
      "Val Iter:  86  Loss:  2.7338898181915283\n",
      "Val Iter:  87  Loss:  2.656572103500366\n",
      "Val Iter:  88  Loss:  2.3397676944732666\n",
      "Val Iter:  89  Loss:  2.148146152496338\n",
      "Val Iter:  90  Loss:  2.23982834815979\n",
      "Val Iter:  91  Loss:  2.183572292327881\n",
      "Val Iter:  92  Loss:  2.1833202838897705\n",
      "Val Iter:  93  Loss:  1.9084198474884033\n",
      "Val Iter:  94  Loss:  2.480445623397827\n",
      "Val Iter:  95  Loss:  2.3089044094085693\n",
      "Val Iter:  96  Loss:  3.582186460494995\n",
      "Val Iter:  97  Loss:  2.284813165664673\n",
      "Val Iter:  98  Loss:  1.9159777164459229\n",
      "Val Iter:  99  Loss:  2.4383232593536377\n",
      "Val Iter:  100  Loss:  1.9459757804870605\n",
      "FINAL Mean Train Loss: 2.1551148037115735  Mean Val Loss:  2.1992264437675475\n",
      "Train Iter:  1  Loss:  1.1260366439819336\n",
      "Train Iter:  2  Loss:  1.2968997955322266\n",
      "Train Iter:  3  Loss:  1.703064203262329\n",
      "Train Iter:  4  Loss:  1.0185048580169678\n",
      "Train Iter:  5  Loss:  1.2200132608413696\n",
      "Train Iter:  6  Loss:  1.4028743505477905\n",
      "Train Iter:  7  Loss:  1.0798509120941162\n",
      "Train Iter:  8  Loss:  1.6302913427352905\n",
      "Train Iter:  9  Loss:  1.552054524421692\n",
      "Train Iter:  10  Loss:  1.1103676557540894\n",
      "Train Iter:  11  Loss:  1.2466164827346802\n",
      "Train Iter:  12  Loss:  1.1535052061080933\n",
      "Train Iter:  13  Loss:  1.628838062286377\n",
      "Train Iter:  14  Loss:  1.6135172843933105\n",
      "Train Iter:  15  Loss:  1.1834615468978882\n",
      "Train Iter:  16  Loss:  1.4551682472229004\n",
      "Train Iter:  17  Loss:  1.620833158493042\n",
      "Train Iter:  18  Loss:  1.7103800773620605\n",
      "Train Iter:  19  Loss:  1.6601850986480713\n",
      "Train Iter:  20  Loss:  1.954964518547058\n",
      "Train Iter:  21  Loss:  1.282529592514038\n",
      "Train Iter:  22  Loss:  1.8289532661437988\n",
      "Train Iter:  23  Loss:  1.0926450490951538\n",
      "Train Iter:  24  Loss:  1.1544004678726196\n",
      "Train Iter:  25  Loss:  1.568142294883728\n",
      "Train Iter:  26  Loss:  1.1880619525909424\n",
      "Train Iter:  27  Loss:  1.4430922269821167\n",
      "Train Iter:  28  Loss:  1.4371025562286377\n",
      "Train Iter:  29  Loss:  1.09795343875885\n",
      "Train Iter:  30  Loss:  1.0564088821411133\n",
      "Train Iter:  31  Loss:  1.1714110374450684\n",
      "Train Iter:  32  Loss:  1.27198326587677\n",
      "Train Iter:  33  Loss:  1.998537540435791\n",
      "Train Iter:  34  Loss:  1.5742937326431274\n",
      "Train Iter:  35  Loss:  1.2080961465835571\n",
      "Train Iter:  36  Loss:  1.0851924419403076\n",
      "Train Iter:  37  Loss:  1.372446060180664\n",
      "Train Iter:  38  Loss:  1.6367698907852173\n",
      "Train Iter:  39  Loss:  1.1540988683700562\n",
      "Train Iter:  40  Loss:  1.1467044353485107\n",
      "Train Iter:  41  Loss:  1.714865803718567\n",
      "Train Iter:  42  Loss:  1.5069202184677124\n",
      "Train Iter:  43  Loss:  1.2856876850128174\n",
      "Train Iter:  44  Loss:  1.3229055404663086\n",
      "Train Iter:  45  Loss:  0.9404528141021729\n",
      "Train Iter:  46  Loss:  1.779369592666626\n",
      "Train Iter:  47  Loss:  1.2267738580703735\n",
      "Train Iter:  48  Loss:  1.149994134902954\n",
      "Train Iter:  49  Loss:  1.6945685148239136\n",
      "Train Iter:  50  Loss:  1.3891420364379883\n",
      "Train Iter:  51  Loss:  1.5343437194824219\n",
      "Train Iter:  52  Loss:  1.755375862121582\n",
      "Train Iter:  53  Loss:  1.0587838888168335\n",
      "Train Iter:  54  Loss:  1.5020737648010254\n",
      "Train Iter:  55  Loss:  1.0415388345718384\n",
      "Train Iter:  56  Loss:  0.9941311478614807\n",
      "Train Iter:  57  Loss:  1.3415770530700684\n",
      "Train Iter:  58  Loss:  1.1146069765090942\n",
      "Train Iter:  59  Loss:  1.638118863105774\n",
      "Train Iter:  60  Loss:  1.2950034141540527\n",
      "Val Iter:  1  Loss:  1.1686313152313232\n",
      "Val Iter:  2  Loss:  1.0417965650558472\n",
      "Val Iter:  3  Loss:  1.204898715019226\n",
      "Val Iter:  4  Loss:  1.3487530946731567\n",
      "Val Iter:  5  Loss:  1.6619682312011719\n",
      "Val Iter:  6  Loss:  1.365510106086731\n",
      "Val Iter:  7  Loss:  1.6105071306228638\n",
      "Val Iter:  8  Loss:  1.8796191215515137\n",
      "Val Iter:  9  Loss:  1.172702670097351\n",
      "Val Iter:  10  Loss:  1.3595346212387085\n",
      "Val Iter:  11  Loss:  1.7872127294540405\n",
      "Val Iter:  12  Loss:  1.3612005710601807\n",
      "Val Iter:  13  Loss:  1.3124301433563232\n",
      "Val Iter:  14  Loss:  1.045605182647705\n",
      "Val Iter:  15  Loss:  0.8908491730690002\n",
      "Val Iter:  16  Loss:  1.0759303569793701\n",
      "Val Iter:  17  Loss:  2.200995683670044\n",
      "Val Iter:  18  Loss:  1.1118383407592773\n",
      "Val Iter:  19  Loss:  1.2506096363067627\n",
      "Val Iter:  20  Loss:  1.56155526638031\n",
      "Val Iter:  21  Loss:  1.420013666152954\n",
      "Val Iter:  22  Loss:  2.8220813274383545\n",
      "Val Iter:  23  Loss:  1.2327057123184204\n",
      "Val Iter:  24  Loss:  1.369330644607544\n",
      "Val Iter:  25  Loss:  1.3697699308395386\n",
      "Val Iter:  26  Loss:  1.473371148109436\n",
      "Val Iter:  27  Loss:  0.8765360713005066\n",
      "Val Iter:  28  Loss:  1.5528277158737183\n",
      "Val Iter:  29  Loss:  1.6827139854431152\n",
      "Val Iter:  30  Loss:  1.2435020208358765\n",
      "Val Iter:  31  Loss:  1.9123677015304565\n",
      "Val Iter:  32  Loss:  1.3985645771026611\n",
      "Val Iter:  33  Loss:  1.3667949438095093\n",
      "Val Iter:  34  Loss:  1.7535598278045654\n",
      "Val Iter:  35  Loss:  0.973901093006134\n",
      "Val Iter:  36  Loss:  1.285364031791687\n",
      "Val Iter:  37  Loss:  1.181424617767334\n",
      "Val Iter:  38  Loss:  1.7069387435913086\n",
      "Val Iter:  39  Loss:  1.312533974647522\n",
      "Val Iter:  40  Loss:  0.784065842628479\n",
      "Val Iter:  41  Loss:  1.3319391012191772\n",
      "Val Iter:  42  Loss:  2.0453834533691406\n",
      "Val Iter:  43  Loss:  0.9794427752494812\n",
      "Val Iter:  44  Loss:  1.6349730491638184\n",
      "Val Iter:  45  Loss:  1.5733815431594849\n",
      "Val Iter:  46  Loss:  1.3321837186813354\n",
      "Val Iter:  47  Loss:  1.2544960975646973\n",
      "Val Iter:  48  Loss:  1.746113657951355\n",
      "Val Iter:  49  Loss:  1.278586983680725\n",
      "Val Iter:  50  Loss:  1.4575790166854858\n",
      "Val Iter:  51  Loss:  2.7600672245025635\n",
      "Val Iter:  52  Loss:  1.151661992073059\n",
      "Val Iter:  53  Loss:  2.241657018661499\n",
      "Val Iter:  54  Loss:  1.0975886583328247\n",
      "Val Iter:  55  Loss:  1.458791732788086\n",
      "Val Iter:  56  Loss:  2.030256509780884\n",
      "Val Iter:  57  Loss:  1.3376684188842773\n",
      "Val Iter:  58  Loss:  1.4788665771484375\n",
      "Val Iter:  59  Loss:  1.4684382677078247\n",
      "Val Iter:  60  Loss:  1.3292207717895508\n",
      "Val Iter:  61  Loss:  1.9787847995758057\n",
      "Val Iter:  62  Loss:  1.1580097675323486\n",
      "Val Iter:  63  Loss:  1.0434309244155884\n",
      "Val Iter:  64  Loss:  1.4239872694015503\n",
      "Val Iter:  65  Loss:  1.459139347076416\n",
      "Val Iter:  66  Loss:  1.2466442584991455\n",
      "Val Iter:  67  Loss:  1.5418450832366943\n",
      "Val Iter:  68  Loss:  1.3215057849884033\n",
      "Val Iter:  69  Loss:  1.1604324579238892\n",
      "Val Iter:  70  Loss:  1.4222134351730347\n",
      "Val Iter:  71  Loss:  1.299435019493103\n",
      "Val Iter:  72  Loss:  1.1207603216171265\n",
      "Val Iter:  73  Loss:  1.4081672430038452\n",
      "Val Iter:  74  Loss:  1.0749609470367432\n",
      "Val Iter:  75  Loss:  0.9827681183815002\n",
      "Val Iter:  76  Loss:  1.017195701599121\n",
      "Val Iter:  77  Loss:  1.1257292032241821\n",
      "Val Iter:  78  Loss:  1.3063724040985107\n",
      "Val Iter:  79  Loss:  0.9964279532432556\n",
      "Val Iter:  80  Loss:  0.9807528257369995\n",
      "Val Iter:  81  Loss:  1.0142799615859985\n",
      "Val Iter:  82  Loss:  1.7956019639968872\n",
      "Val Iter:  83  Loss:  1.5294146537780762\n",
      "Val Iter:  84  Loss:  1.0554250478744507\n",
      "Val Iter:  85  Loss:  1.2483712434768677\n",
      "Val Iter:  86  Loss:  1.267618179321289\n",
      "Val Iter:  87  Loss:  1.6953330039978027\n",
      "Val Iter:  88  Loss:  1.1163208484649658\n",
      "Val Iter:  89  Loss:  1.2559572458267212\n",
      "Val Iter:  90  Loss:  1.0352333784103394\n",
      "Val Iter:  91  Loss:  0.8847988843917847\n",
      "Val Iter:  92  Loss:  1.461897373199463\n",
      "Val Iter:  93  Loss:  1.3292673826217651\n",
      "Val Iter:  94  Loss:  1.8316351175308228\n",
      "Val Iter:  95  Loss:  1.4099597930908203\n",
      "Val Iter:  96  Loss:  1.585185170173645\n",
      "Val Iter:  97  Loss:  1.0937448740005493\n",
      "Val Iter:  98  Loss:  1.4659854173660278\n",
      "Val Iter:  99  Loss:  1.6069772243499756\n",
      "Val Iter:  100  Loss:  1.1592217683792114\n",
      "FINAL Mean Train Loss: 1.3737080683310827  Mean Val Loss:  1.3902960419654846\n",
      "Train Iter:  1  Loss:  1.401530385017395\n",
      "Train Iter:  2  Loss:  1.2880711555480957\n",
      "Train Iter:  3  Loss:  1.5635080337524414\n",
      "Train Iter:  4  Loss:  0.9777370691299438\n",
      "Train Iter:  5  Loss:  1.1410201787948608\n",
      "Train Iter:  6  Loss:  1.4232509136199951\n",
      "Train Iter:  7  Loss:  1.4172178506851196\n",
      "Train Iter:  8  Loss:  1.0429953336715698\n",
      "Train Iter:  9  Loss:  1.0566165447235107\n",
      "Train Iter:  10  Loss:  1.1231558322906494\n",
      "Train Iter:  11  Loss:  1.4149703979492188\n",
      "Train Iter:  12  Loss:  1.4553594589233398\n",
      "Train Iter:  13  Loss:  1.2316441535949707\n",
      "Train Iter:  14  Loss:  1.2969311475753784\n",
      "Train Iter:  15  Loss:  1.3668898344039917\n",
      "Train Iter:  16  Loss:  1.3118810653686523\n",
      "Train Iter:  17  Loss:  1.7663177251815796\n",
      "Train Iter:  18  Loss:  1.3444589376449585\n",
      "Train Iter:  19  Loss:  1.1358642578125\n",
      "Train Iter:  20  Loss:  1.4353888034820557\n",
      "Train Iter:  21  Loss:  1.3805620670318604\n",
      "Train Iter:  22  Loss:  1.2782741785049438\n",
      "Train Iter:  23  Loss:  1.3010294437408447\n",
      "Train Iter:  24  Loss:  1.3373465538024902\n",
      "Train Iter:  25  Loss:  1.2992709875106812\n",
      "Train Iter:  26  Loss:  1.2249372005462646\n",
      "Train Iter:  27  Loss:  1.2457947731018066\n",
      "Train Iter:  28  Loss:  1.2780143022537231\n",
      "Train Iter:  29  Loss:  1.5197619199752808\n",
      "Train Iter:  30  Loss:  1.076684594154358\n",
      "Train Iter:  31  Loss:  1.1669423580169678\n",
      "Train Iter:  32  Loss:  1.3352634906768799\n",
      "Train Iter:  33  Loss:  1.2842556238174438\n",
      "Train Iter:  34  Loss:  1.6012145280838013\n",
      "Train Iter:  35  Loss:  1.3258965015411377\n",
      "Train Iter:  36  Loss:  0.9837007522583008\n",
      "Train Iter:  37  Loss:  1.271584391593933\n",
      "Train Iter:  38  Loss:  1.4993664026260376\n",
      "Train Iter:  39  Loss:  1.1417230367660522\n",
      "Train Iter:  40  Loss:  1.1728252172470093\n",
      "Train Iter:  41  Loss:  1.2970284223556519\n",
      "Train Iter:  42  Loss:  1.0702170133590698\n",
      "Train Iter:  43  Loss:  1.0218961238861084\n",
      "Train Iter:  44  Loss:  1.2263327836990356\n",
      "Train Iter:  45  Loss:  0.97972172498703\n",
      "Train Iter:  46  Loss:  1.3372111320495605\n",
      "Train Iter:  47  Loss:  1.3005954027175903\n",
      "Train Iter:  48  Loss:  1.4587358236312866\n",
      "Train Iter:  49  Loss:  1.2705612182617188\n",
      "Train Iter:  50  Loss:  1.4752002954483032\n",
      "Train Iter:  51  Loss:  1.6715178489685059\n",
      "Train Iter:  52  Loss:  1.4122191667556763\n",
      "Train Iter:  53  Loss:  1.6930347681045532\n",
      "Train Iter:  54  Loss:  1.6846052408218384\n",
      "Train Iter:  55  Loss:  1.1553027629852295\n",
      "Train Iter:  56  Loss:  1.1313011646270752\n",
      "Train Iter:  57  Loss:  1.3624111413955688\n",
      "Train Iter:  58  Loss:  1.1491169929504395\n",
      "Train Iter:  59  Loss:  1.2030328512191772\n",
      "Train Iter:  60  Loss:  1.118632197380066\n",
      "Val Iter:  1  Loss:  0.9860107898712158\n",
      "Val Iter:  2  Loss:  1.4320447444915771\n",
      "Val Iter:  3  Loss:  1.2146776914596558\n",
      "Val Iter:  4  Loss:  0.9254519939422607\n",
      "Val Iter:  5  Loss:  1.2882659435272217\n",
      "Val Iter:  6  Loss:  1.3652211427688599\n",
      "Val Iter:  7  Loss:  1.629572868347168\n",
      "Val Iter:  8  Loss:  1.1664798259735107\n",
      "Val Iter:  9  Loss:  1.0960006713867188\n",
      "Val Iter:  10  Loss:  1.2056232690811157\n",
      "Val Iter:  11  Loss:  1.311812162399292\n",
      "Val Iter:  12  Loss:  1.2942805290222168\n",
      "Val Iter:  13  Loss:  1.22553288936615\n",
      "Val Iter:  14  Loss:  1.0652391910552979\n",
      "Val Iter:  15  Loss:  1.3098863363265991\n",
      "Val Iter:  16  Loss:  1.3024218082427979\n",
      "Val Iter:  17  Loss:  1.6093863248825073\n",
      "Val Iter:  18  Loss:  1.0729132890701294\n",
      "Val Iter:  19  Loss:  0.9895408153533936\n",
      "Val Iter:  20  Loss:  1.406990647315979\n",
      "Val Iter:  21  Loss:  1.45734441280365\n",
      "Val Iter:  22  Loss:  1.1736949682235718\n",
      "Val Iter:  23  Loss:  1.2295141220092773\n",
      "Val Iter:  24  Loss:  1.0998636484146118\n",
      "Val Iter:  25  Loss:  1.0827265977859497\n",
      "Val Iter:  26  Loss:  1.511032223701477\n",
      "Val Iter:  27  Loss:  1.0861997604370117\n",
      "Val Iter:  28  Loss:  1.346992015838623\n",
      "Val Iter:  29  Loss:  1.0372745990753174\n",
      "Val Iter:  30  Loss:  1.8042234182357788\n",
      "Val Iter:  31  Loss:  1.6451524496078491\n",
      "Val Iter:  32  Loss:  1.930986762046814\n",
      "Val Iter:  33  Loss:  1.7760505676269531\n",
      "Val Iter:  34  Loss:  1.4954460859298706\n",
      "Val Iter:  35  Loss:  1.2906396389007568\n",
      "Val Iter:  36  Loss:  1.1033965349197388\n",
      "Val Iter:  37  Loss:  1.4482386112213135\n",
      "Val Iter:  38  Loss:  1.2059087753295898\n",
      "Val Iter:  39  Loss:  1.0754307508468628\n",
      "Val Iter:  40  Loss:  1.1285043954849243\n",
      "Val Iter:  41  Loss:  1.2556793689727783\n",
      "Val Iter:  42  Loss:  1.1220173835754395\n",
      "Val Iter:  43  Loss:  1.297812581062317\n",
      "Val Iter:  44  Loss:  2.446403980255127\n",
      "Val Iter:  45  Loss:  1.7284730672836304\n",
      "Val Iter:  46  Loss:  1.3380253314971924\n",
      "Val Iter:  47  Loss:  1.297865867614746\n",
      "Val Iter:  48  Loss:  1.1489918231964111\n",
      "Val Iter:  49  Loss:  1.7943540811538696\n",
      "Val Iter:  50  Loss:  0.9150093197822571\n",
      "Val Iter:  51  Loss:  1.3568997383117676\n",
      "Val Iter:  52  Loss:  0.9865031838417053\n",
      "Val Iter:  53  Loss:  2.0699689388275146\n",
      "Val Iter:  54  Loss:  1.3391611576080322\n",
      "Val Iter:  55  Loss:  1.1446235179901123\n",
      "Val Iter:  56  Loss:  1.7174209356307983\n",
      "Val Iter:  57  Loss:  1.7899776697158813\n",
      "Val Iter:  58  Loss:  1.5985687971115112\n",
      "Val Iter:  59  Loss:  1.2516405582427979\n",
      "Val Iter:  60  Loss:  1.1459258794784546\n",
      "Val Iter:  61  Loss:  1.8162355422973633\n",
      "Val Iter:  62  Loss:  1.1754292249679565\n",
      "Val Iter:  63  Loss:  1.4460111856460571\n",
      "Val Iter:  64  Loss:  1.8995741605758667\n",
      "Val Iter:  65  Loss:  1.1497302055358887\n",
      "Val Iter:  66  Loss:  0.9608180522918701\n",
      "Val Iter:  67  Loss:  1.1212067604064941\n",
      "Val Iter:  68  Loss:  1.6994632482528687\n",
      "Val Iter:  69  Loss:  1.538355827331543\n",
      "Val Iter:  70  Loss:  1.3242605924606323\n",
      "Val Iter:  71  Loss:  1.6123970746994019\n",
      "Val Iter:  72  Loss:  1.2525646686553955\n",
      "Val Iter:  73  Loss:  1.2551352977752686\n",
      "Val Iter:  74  Loss:  1.1791691780090332\n",
      "Val Iter:  75  Loss:  1.3835411071777344\n",
      "Val Iter:  76  Loss:  1.682828664779663\n",
      "Val Iter:  77  Loss:  1.3784422874450684\n",
      "Val Iter:  78  Loss:  0.9118329286575317\n",
      "Val Iter:  79  Loss:  1.3396564722061157\n",
      "Val Iter:  80  Loss:  1.4867640733718872\n",
      "Val Iter:  81  Loss:  0.9647960662841797\n",
      "Val Iter:  82  Loss:  1.056952714920044\n",
      "Val Iter:  83  Loss:  1.5803236961364746\n",
      "Val Iter:  84  Loss:  1.3556745052337646\n",
      "Val Iter:  85  Loss:  0.9561249613761902\n",
      "Val Iter:  86  Loss:  1.0396829843521118\n",
      "Val Iter:  87  Loss:  1.5810106992721558\n",
      "Val Iter:  88  Loss:  0.8277810215950012\n",
      "Val Iter:  89  Loss:  1.2699114084243774\n",
      "Val Iter:  90  Loss:  1.3924003839492798\n",
      "Val Iter:  91  Loss:  1.179336667060852\n",
      "Val Iter:  92  Loss:  1.1995538473129272\n",
      "Val Iter:  93  Loss:  1.0654189586639404\n",
      "Val Iter:  94  Loss:  1.2396601438522339\n",
      "Val Iter:  95  Loss:  1.441977620124817\n",
      "Val Iter:  96  Loss:  1.1661417484283447\n",
      "Val Iter:  97  Loss:  1.3797684907913208\n",
      "Val Iter:  98  Loss:  1.217944622039795\n",
      "Val Iter:  99  Loss:  1.0677205324172974\n",
      "Val Iter:  100  Loss:  1.1584492921829224\n",
      "FINAL Mean Train Loss: 1.2989655246337255  Mean Val Loss:  1.3232534337043762\n",
      "Train Iter:  1  Loss:  1.4366697072982788\n",
      "Train Iter:  2  Loss:  1.2713658809661865\n",
      "Train Iter:  3  Loss:  1.0195703506469727\n",
      "Train Iter:  4  Loss:  1.5524331331253052\n",
      "Train Iter:  5  Loss:  1.3910117149353027\n",
      "Train Iter:  6  Loss:  1.0430831909179688\n",
      "Train Iter:  7  Loss:  1.3348774909973145\n",
      "Train Iter:  8  Loss:  1.3321583271026611\n",
      "Train Iter:  9  Loss:  1.5191506147384644\n",
      "Train Iter:  10  Loss:  1.0323522090911865\n",
      "Train Iter:  11  Loss:  1.435332179069519\n",
      "Train Iter:  12  Loss:  1.4169530868530273\n",
      "Train Iter:  13  Loss:  1.3999366760253906\n",
      "Train Iter:  14  Loss:  1.183464765548706\n",
      "Train Iter:  15  Loss:  1.274592399597168\n",
      "Train Iter:  16  Loss:  1.2543389797210693\n",
      "Train Iter:  17  Loss:  1.4933953285217285\n",
      "Train Iter:  18  Loss:  1.2767090797424316\n",
      "Train Iter:  19  Loss:  1.1376861333847046\n",
      "Train Iter:  20  Loss:  1.3431365489959717\n",
      "Train Iter:  21  Loss:  1.5253934860229492\n",
      "Train Iter:  22  Loss:  1.2853859663009644\n",
      "Train Iter:  23  Loss:  1.277004361152649\n",
      "Train Iter:  24  Loss:  1.1843315362930298\n",
      "Train Iter:  25  Loss:  1.6764622926712036\n",
      "Train Iter:  26  Loss:  1.3615974187850952\n",
      "Train Iter:  27  Loss:  1.5569263696670532\n",
      "Train Iter:  28  Loss:  1.413437008857727\n",
      "Train Iter:  29  Loss:  1.6659390926361084\n",
      "Train Iter:  30  Loss:  1.1996862888336182\n",
      "Train Iter:  31  Loss:  1.0888679027557373\n",
      "Train Iter:  32  Loss:  1.5696970224380493\n",
      "Train Iter:  33  Loss:  1.3939015865325928\n",
      "Train Iter:  34  Loss:  1.536025047302246\n",
      "Train Iter:  35  Loss:  1.6118497848510742\n",
      "Train Iter:  36  Loss:  1.0728272199630737\n",
      "Train Iter:  37  Loss:  1.198768973350525\n",
      "Train Iter:  38  Loss:  1.1669589281082153\n",
      "Train Iter:  39  Loss:  1.3728125095367432\n",
      "Train Iter:  40  Loss:  1.0897432565689087\n",
      "Train Iter:  41  Loss:  1.2179336547851562\n",
      "Train Iter:  42  Loss:  1.3329328298568726\n",
      "Train Iter:  43  Loss:  1.2058069705963135\n",
      "Train Iter:  44  Loss:  1.19867742061615\n",
      "Train Iter:  45  Loss:  1.502132773399353\n",
      "Train Iter:  46  Loss:  1.245522379875183\n",
      "Train Iter:  47  Loss:  1.7435811758041382\n",
      "Train Iter:  48  Loss:  1.401137113571167\n",
      "Train Iter:  49  Loss:  1.468288779258728\n",
      "Train Iter:  50  Loss:  1.7277122735977173\n",
      "Train Iter:  51  Loss:  1.1305408477783203\n",
      "Train Iter:  52  Loss:  1.556118130683899\n",
      "Train Iter:  53  Loss:  1.531211495399475\n",
      "Train Iter:  54  Loss:  1.2954301834106445\n",
      "Train Iter:  55  Loss:  1.711058497428894\n",
      "Train Iter:  56  Loss:  1.2062351703643799\n",
      "Train Iter:  57  Loss:  1.3863327503204346\n",
      "Train Iter:  58  Loss:  1.237940788269043\n",
      "Train Iter:  59  Loss:  1.56769597530365\n",
      "Train Iter:  60  Loss:  1.3050440549850464\n",
      "Val Iter:  1  Loss:  1.0778956413269043\n",
      "Val Iter:  2  Loss:  1.5435978174209595\n",
      "Val Iter:  3  Loss:  1.6299749612808228\n",
      "Val Iter:  4  Loss:  1.4619001150131226\n",
      "Val Iter:  5  Loss:  1.1951407194137573\n",
      "Val Iter:  6  Loss:  1.6455647945404053\n",
      "Val Iter:  7  Loss:  1.1912145614624023\n",
      "Val Iter:  8  Loss:  1.243398666381836\n",
      "Val Iter:  9  Loss:  1.558200478553772\n",
      "Val Iter:  10  Loss:  0.9922155141830444\n",
      "Val Iter:  11  Loss:  1.3842538595199585\n",
      "Val Iter:  12  Loss:  1.4249612092971802\n",
      "Val Iter:  13  Loss:  1.4265007972717285\n",
      "Val Iter:  14  Loss:  1.0886085033416748\n",
      "Val Iter:  15  Loss:  1.3446760177612305\n",
      "Val Iter:  16  Loss:  0.9323276877403259\n",
      "Val Iter:  17  Loss:  1.359576940536499\n",
      "Val Iter:  18  Loss:  0.97139573097229\n",
      "Val Iter:  19  Loss:  1.0423893928527832\n",
      "Val Iter:  20  Loss:  1.376111626625061\n",
      "Val Iter:  21  Loss:  1.304789423942566\n",
      "Val Iter:  22  Loss:  1.7595332860946655\n",
      "Val Iter:  23  Loss:  1.827895998954773\n",
      "Val Iter:  24  Loss:  1.2954121828079224\n",
      "Val Iter:  25  Loss:  2.092778444290161\n",
      "Val Iter:  26  Loss:  1.8079090118408203\n",
      "Val Iter:  27  Loss:  1.07156240940094\n",
      "Val Iter:  28  Loss:  1.510366439819336\n",
      "Val Iter:  29  Loss:  0.9823022484779358\n",
      "Val Iter:  30  Loss:  2.0952508449554443\n",
      "Val Iter:  31  Loss:  1.1317998170852661\n",
      "Val Iter:  32  Loss:  1.1040914058685303\n",
      "Val Iter:  33  Loss:  1.2552679777145386\n",
      "Val Iter:  34  Loss:  1.4226330518722534\n",
      "Val Iter:  35  Loss:  2.398790121078491\n",
      "Val Iter:  36  Loss:  1.1396764516830444\n",
      "Val Iter:  37  Loss:  1.6566872596740723\n",
      "Val Iter:  38  Loss:  1.334420084953308\n",
      "Val Iter:  39  Loss:  1.43053138256073\n",
      "Val Iter:  40  Loss:  1.4814506769180298\n",
      "Val Iter:  41  Loss:  1.537441372871399\n",
      "Val Iter:  42  Loss:  1.1388250589370728\n",
      "Val Iter:  43  Loss:  1.7453935146331787\n",
      "Val Iter:  44  Loss:  1.5732842683792114\n",
      "Val Iter:  45  Loss:  1.1362158060073853\n",
      "Val Iter:  46  Loss:  2.8162527084350586\n",
      "Val Iter:  47  Loss:  1.3101563453674316\n",
      "Val Iter:  48  Loss:  1.3263180255889893\n",
      "Val Iter:  49  Loss:  1.3596326112747192\n",
      "Val Iter:  50  Loss:  1.261667013168335\n",
      "Val Iter:  51  Loss:  1.8058573007583618\n",
      "Val Iter:  52  Loss:  1.0680432319641113\n",
      "Val Iter:  53  Loss:  1.1372227668762207\n",
      "Val Iter:  54  Loss:  1.4125819206237793\n",
      "Val Iter:  55  Loss:  1.1208161115646362\n",
      "Val Iter:  56  Loss:  1.3583012819290161\n",
      "Val Iter:  57  Loss:  1.3196183443069458\n",
      "Val Iter:  58  Loss:  1.8457906246185303\n",
      "Val Iter:  59  Loss:  1.0881786346435547\n",
      "Val Iter:  60  Loss:  1.3133034706115723\n",
      "Val Iter:  61  Loss:  1.2322072982788086\n",
      "Val Iter:  62  Loss:  1.284224510192871\n",
      "Val Iter:  63  Loss:  1.0130094289779663\n",
      "Val Iter:  64  Loss:  1.1881144046783447\n",
      "Val Iter:  65  Loss:  1.3276323080062866\n",
      "Val Iter:  66  Loss:  1.268546223640442\n",
      "Val Iter:  67  Loss:  1.3521174192428589\n",
      "Val Iter:  68  Loss:  0.9770776033401489\n",
      "Val Iter:  69  Loss:  1.5987586975097656\n",
      "Val Iter:  70  Loss:  1.2621006965637207\n",
      "Val Iter:  71  Loss:  1.2423148155212402\n",
      "Val Iter:  72  Loss:  0.9769119620323181\n",
      "Val Iter:  73  Loss:  1.1606180667877197\n",
      "Val Iter:  74  Loss:  0.8912202715873718\n",
      "Val Iter:  75  Loss:  2.3641374111175537\n",
      "Val Iter:  76  Loss:  1.05637526512146\n",
      "Val Iter:  77  Loss:  0.9818761348724365\n",
      "Val Iter:  78  Loss:  1.3177707195281982\n",
      "Val Iter:  79  Loss:  1.3442720174789429\n",
      "Val Iter:  80  Loss:  0.7848620414733887\n",
      "Val Iter:  81  Loss:  1.0392807722091675\n",
      "Val Iter:  82  Loss:  0.9914941787719727\n",
      "Val Iter:  83  Loss:  1.0838038921356201\n",
      "Val Iter:  84  Loss:  1.0424491167068481\n",
      "Val Iter:  85  Loss:  0.9278447031974792\n",
      "Val Iter:  86  Loss:  1.1569360494613647\n",
      "Val Iter:  87  Loss:  2.218850612640381\n",
      "Val Iter:  88  Loss:  1.4472278356552124\n",
      "Val Iter:  89  Loss:  1.0616835355758667\n",
      "Val Iter:  90  Loss:  1.5306973457336426\n",
      "Val Iter:  91  Loss:  2.083505630493164\n",
      "Val Iter:  92  Loss:  1.2974213361740112\n",
      "Val Iter:  93  Loss:  1.7267149686813354\n",
      "Val Iter:  94  Loss:  1.5229644775390625\n",
      "Val Iter:  95  Loss:  0.999873161315918\n",
      "Val Iter:  96  Loss:  1.088906168937683\n",
      "Val Iter:  97  Loss:  1.234318733215332\n",
      "Val Iter:  98  Loss:  1.205918312072754\n",
      "Val Iter:  99  Loss:  1.5967355966567993\n",
      "Val Iter:  100  Loss:  0.9785659313201904\n",
      "FINAL Mean Train Loss: 1.3561194519201913  Mean Val Loss:  1.3552729362249374\n",
      "Train Iter:  1  Loss:  1.4302945137023926\n",
      "Train Iter:  2  Loss:  1.1198711395263672\n",
      "Train Iter:  3  Loss:  1.611260175704956\n",
      "Train Iter:  4  Loss:  1.5656657218933105\n",
      "Train Iter:  5  Loss:  1.3886064291000366\n",
      "Train Iter:  6  Loss:  1.159924030303955\n",
      "Train Iter:  7  Loss:  1.206367015838623\n",
      "Train Iter:  8  Loss:  1.3024470806121826\n",
      "Train Iter:  9  Loss:  1.5022788047790527\n",
      "Train Iter:  10  Loss:  1.280421495437622\n",
      "Train Iter:  11  Loss:  1.4639250040054321\n",
      "Train Iter:  12  Loss:  1.513480305671692\n",
      "Train Iter:  13  Loss:  1.037427544593811\n",
      "Train Iter:  14  Loss:  1.275524616241455\n",
      "Train Iter:  15  Loss:  1.4098784923553467\n",
      "Train Iter:  16  Loss:  1.113609790802002\n",
      "Train Iter:  17  Loss:  1.0833271741867065\n",
      "Train Iter:  18  Loss:  1.3581507205963135\n",
      "Train Iter:  19  Loss:  1.508057951927185\n",
      "Train Iter:  20  Loss:  1.5414797067642212\n",
      "Train Iter:  21  Loss:  0.9892760515213013\n",
      "Train Iter:  22  Loss:  1.4234658479690552\n",
      "Train Iter:  23  Loss:  1.4575183391571045\n",
      "Train Iter:  24  Loss:  1.5454281568527222\n",
      "Train Iter:  25  Loss:  0.8032732605934143\n",
      "Train Iter:  26  Loss:  1.3579516410827637\n",
      "Train Iter:  27  Loss:  1.1671751737594604\n",
      "Train Iter:  28  Loss:  1.3767404556274414\n",
      "Train Iter:  29  Loss:  1.3349360227584839\n",
      "Train Iter:  30  Loss:  1.43932044506073\n",
      "Train Iter:  31  Loss:  1.3765825033187866\n",
      "Train Iter:  32  Loss:  1.4413493871688843\n",
      "Train Iter:  33  Loss:  0.8332658410072327\n",
      "Train Iter:  34  Loss:  2.013948678970337\n",
      "Train Iter:  35  Loss:  1.5165562629699707\n",
      "Train Iter:  36  Loss:  1.1606965065002441\n",
      "Train Iter:  37  Loss:  1.2031569480895996\n",
      "Train Iter:  38  Loss:  1.038750171661377\n",
      "Train Iter:  39  Loss:  1.2690346240997314\n",
      "Train Iter:  40  Loss:  1.53355073928833\n",
      "Train Iter:  41  Loss:  1.4498106241226196\n",
      "Train Iter:  42  Loss:  1.4177262783050537\n",
      "Train Iter:  43  Loss:  1.1991162300109863\n",
      "Train Iter:  44  Loss:  1.3663966655731201\n",
      "Train Iter:  45  Loss:  1.0157532691955566\n",
      "Train Iter:  46  Loss:  1.5544624328613281\n",
      "Train Iter:  47  Loss:  1.6512202024459839\n",
      "Train Iter:  48  Loss:  1.0425541400909424\n",
      "Train Iter:  49  Loss:  1.653154969215393\n",
      "Train Iter:  50  Loss:  1.0080845355987549\n",
      "Train Iter:  51  Loss:  1.4842160940170288\n",
      "Train Iter:  52  Loss:  1.3411197662353516\n",
      "Train Iter:  53  Loss:  1.908082365989685\n",
      "Train Iter:  54  Loss:  1.5247774124145508\n",
      "Train Iter:  55  Loss:  1.3994488716125488\n",
      "Train Iter:  56  Loss:  1.3734806776046753\n",
      "Train Iter:  57  Loss:  1.4949742555618286\n",
      "Train Iter:  58  Loss:  1.4414191246032715\n",
      "Train Iter:  59  Loss:  0.9911525249481201\n",
      "Train Iter:  60  Loss:  1.1879286766052246\n",
      "Val Iter:  1  Loss:  1.0312860012054443\n",
      "Val Iter:  2  Loss:  1.3947995901107788\n",
      "Val Iter:  3  Loss:  0.7846516370773315\n",
      "Val Iter:  4  Loss:  1.156257152557373\n",
      "Val Iter:  5  Loss:  0.6972488164901733\n",
      "Val Iter:  6  Loss:  1.28093421459198\n",
      "Val Iter:  7  Loss:  1.1139185428619385\n",
      "Val Iter:  8  Loss:  1.4120323657989502\n",
      "Val Iter:  9  Loss:  1.2826576232910156\n",
      "Val Iter:  10  Loss:  1.1927751302719116\n",
      "Val Iter:  11  Loss:  1.9996892213821411\n",
      "Val Iter:  12  Loss:  0.949313759803772\n",
      "Val Iter:  13  Loss:  1.6637884378433228\n",
      "Val Iter:  14  Loss:  1.4523532390594482\n",
      "Val Iter:  15  Loss:  1.2098944187164307\n",
      "Val Iter:  16  Loss:  1.3004786968231201\n",
      "Val Iter:  17  Loss:  0.9899770021438599\n",
      "Val Iter:  18  Loss:  1.1401275396347046\n",
      "Val Iter:  19  Loss:  1.3242710828781128\n",
      "Val Iter:  20  Loss:  1.3360036611557007\n",
      "Val Iter:  21  Loss:  1.6227835416793823\n",
      "Val Iter:  22  Loss:  1.1271429061889648\n",
      "Val Iter:  23  Loss:  1.6257373094558716\n",
      "Val Iter:  24  Loss:  1.4839966297149658\n",
      "Val Iter:  25  Loss:  1.513672113418579\n",
      "Val Iter:  26  Loss:  1.2046222686767578\n",
      "Val Iter:  27  Loss:  0.980265736579895\n",
      "Val Iter:  28  Loss:  1.2804094552993774\n",
      "Val Iter:  29  Loss:  1.11539626121521\n",
      "Val Iter:  30  Loss:  1.2795377969741821\n",
      "Val Iter:  31  Loss:  1.8152400255203247\n",
      "Val Iter:  32  Loss:  1.421021580696106\n",
      "Val Iter:  33  Loss:  1.2649588584899902\n",
      "Val Iter:  34  Loss:  0.854214072227478\n",
      "Val Iter:  35  Loss:  1.4163120985031128\n",
      "Val Iter:  36  Loss:  2.9553840160369873\n",
      "Val Iter:  37  Loss:  1.0526326894760132\n",
      "Val Iter:  38  Loss:  1.2925665378570557\n",
      "Val Iter:  39  Loss:  1.5128624439239502\n",
      "Val Iter:  40  Loss:  1.5915286540985107\n",
      "Val Iter:  41  Loss:  1.3232908248901367\n",
      "Val Iter:  42  Loss:  1.2612959146499634\n",
      "Val Iter:  43  Loss:  1.1958234310150146\n",
      "Val Iter:  44  Loss:  1.7826029062271118\n",
      "Val Iter:  45  Loss:  1.2904438972473145\n",
      "Val Iter:  46  Loss:  1.4840456247329712\n",
      "Val Iter:  47  Loss:  0.8367452025413513\n",
      "Val Iter:  48  Loss:  1.209438681602478\n",
      "Val Iter:  49  Loss:  1.573643684387207\n",
      "Val Iter:  50  Loss:  2.0079753398895264\n",
      "Val Iter:  51  Loss:  1.9167400598526\n",
      "Val Iter:  52  Loss:  1.6559499502182007\n",
      "Val Iter:  53  Loss:  0.8692024946212769\n",
      "Val Iter:  54  Loss:  1.1477547883987427\n",
      "Val Iter:  55  Loss:  1.3129380941390991\n",
      "Val Iter:  56  Loss:  1.521898865699768\n",
      "Val Iter:  57  Loss:  0.9726961851119995\n",
      "Val Iter:  58  Loss:  1.6266353130340576\n",
      "Val Iter:  59  Loss:  1.372248649597168\n",
      "Val Iter:  60  Loss:  1.5715445280075073\n",
      "Val Iter:  61  Loss:  1.2576134204864502\n",
      "Val Iter:  62  Loss:  1.475872278213501\n",
      "Val Iter:  63  Loss:  1.0250221490859985\n",
      "Val Iter:  64  Loss:  1.2840123176574707\n",
      "Val Iter:  65  Loss:  1.6818889379501343\n",
      "Val Iter:  66  Loss:  1.3020421266555786\n",
      "Val Iter:  67  Loss:  3.0362555980682373\n",
      "Val Iter:  68  Loss:  1.307090401649475\n",
      "Val Iter:  69  Loss:  1.5752110481262207\n",
      "Val Iter:  70  Loss:  0.792376697063446\n",
      "Val Iter:  71  Loss:  1.9663312435150146\n",
      "Val Iter:  72  Loss:  1.2534854412078857\n",
      "Val Iter:  73  Loss:  1.593086838722229\n",
      "Val Iter:  74  Loss:  0.8449236154556274\n",
      "Val Iter:  75  Loss:  1.6941335201263428\n",
      "Val Iter:  76  Loss:  1.9982388019561768\n",
      "Val Iter:  77  Loss:  1.4085116386413574\n",
      "Val Iter:  78  Loss:  0.9184088110923767\n",
      "Val Iter:  79  Loss:  1.1145110130310059\n",
      "Val Iter:  80  Loss:  1.3828394412994385\n",
      "Val Iter:  81  Loss:  1.7598233222961426\n",
      "Val Iter:  82  Loss:  1.496739149093628\n",
      "Val Iter:  83  Loss:  1.122893214225769\n",
      "Val Iter:  84  Loss:  0.9032709002494812\n",
      "Val Iter:  85  Loss:  1.3947999477386475\n",
      "Val Iter:  86  Loss:  1.3754130601882935\n",
      "Val Iter:  87  Loss:  0.9839864373207092\n",
      "Val Iter:  88  Loss:  1.5920240879058838\n",
      "Val Iter:  89  Loss:  1.2046369314193726\n",
      "Val Iter:  90  Loss:  1.2236050367355347\n",
      "Val Iter:  91  Loss:  1.1779437065124512\n",
      "Val Iter:  92  Loss:  1.1106239557266235\n",
      "Val Iter:  93  Loss:  1.4756970405578613\n",
      "Val Iter:  94  Loss:  1.858582615852356\n",
      "Val Iter:  95  Loss:  0.8142831921577454\n",
      "Val Iter:  96  Loss:  1.3569300174713135\n",
      "Val Iter:  97  Loss:  0.9672158360481262\n",
      "Val Iter:  98  Loss:  1.4834848642349243\n",
      "Val Iter:  99  Loss:  1.4044790267944336\n",
      "Val Iter:  100  Loss:  1.1054097414016724\n",
      "FINAL Mean Train Loss: 1.3443142314751944  Mean Val Loss:  1.3577935498952867\n",
      "Train Iter:  1  Loss:  1.1172794103622437\n",
      "Train Iter:  2  Loss:  0.9342795610427856\n",
      "Train Iter:  3  Loss:  1.3794965744018555\n",
      "Train Iter:  4  Loss:  1.2644312381744385\n",
      "Train Iter:  5  Loss:  1.1397862434387207\n",
      "Train Iter:  6  Loss:  1.2894461154937744\n",
      "Train Iter:  7  Loss:  1.303048849105835\n",
      "Train Iter:  8  Loss:  1.2536420822143555\n",
      "Train Iter:  9  Loss:  1.1522096395492554\n",
      "Train Iter:  10  Loss:  1.642809510231018\n",
      "Train Iter:  11  Loss:  0.9901676177978516\n",
      "Train Iter:  12  Loss:  1.033423662185669\n",
      "Train Iter:  13  Loss:  1.0647441148757935\n",
      "Train Iter:  14  Loss:  1.3004372119903564\n",
      "Train Iter:  15  Loss:  1.484369158744812\n",
      "Train Iter:  16  Loss:  1.4233736991882324\n",
      "Train Iter:  17  Loss:  1.1240603923797607\n",
      "Train Iter:  18  Loss:  1.4907270669937134\n",
      "Train Iter:  19  Loss:  1.2674682140350342\n",
      "Train Iter:  20  Loss:  1.1324468851089478\n",
      "Train Iter:  21  Loss:  1.5072567462921143\n",
      "Train Iter:  22  Loss:  1.0058181285858154\n",
      "Train Iter:  23  Loss:  1.706847906112671\n",
      "Train Iter:  24  Loss:  1.158650517463684\n",
      "Train Iter:  25  Loss:  1.003623604774475\n",
      "Train Iter:  26  Loss:  1.3093236684799194\n",
      "Train Iter:  27  Loss:  1.3626375198364258\n",
      "Train Iter:  28  Loss:  0.9734486937522888\n",
      "Train Iter:  29  Loss:  1.5910124778747559\n",
      "Train Iter:  30  Loss:  1.6818102598190308\n",
      "Train Iter:  31  Loss:  1.5973789691925049\n",
      "Train Iter:  32  Loss:  1.3579463958740234\n",
      "Train Iter:  33  Loss:  1.7922534942626953\n",
      "Train Iter:  34  Loss:  1.195928692817688\n",
      "Train Iter:  35  Loss:  1.2136905193328857\n",
      "Train Iter:  36  Loss:  1.086283802986145\n",
      "Train Iter:  37  Loss:  1.5053002834320068\n",
      "Train Iter:  38  Loss:  1.3772814273834229\n",
      "Train Iter:  39  Loss:  1.1050281524658203\n",
      "Train Iter:  40  Loss:  1.2264724969863892\n",
      "Train Iter:  41  Loss:  1.1834019422531128\n",
      "Train Iter:  42  Loss:  1.1049001216888428\n",
      "Train Iter:  43  Loss:  1.1950513124465942\n",
      "Train Iter:  44  Loss:  1.5088328123092651\n",
      "Train Iter:  45  Loss:  1.1818429231643677\n",
      "Train Iter:  46  Loss:  1.1943212747573853\n",
      "Train Iter:  47  Loss:  1.0512902736663818\n",
      "Train Iter:  48  Loss:  1.2616961002349854\n",
      "Train Iter:  49  Loss:  0.9121103882789612\n",
      "Train Iter:  50  Loss:  1.3778280019760132\n",
      "Train Iter:  51  Loss:  1.3176053762435913\n",
      "Train Iter:  52  Loss:  1.264969825744629\n",
      "Train Iter:  53  Loss:  1.3740392923355103\n",
      "Train Iter:  54  Loss:  1.0573005676269531\n",
      "Train Iter:  55  Loss:  1.6680504083633423\n",
      "Train Iter:  56  Loss:  1.282565712928772\n",
      "Train Iter:  57  Loss:  1.7008414268493652\n",
      "Train Iter:  58  Loss:  1.5231702327728271\n",
      "Train Iter:  59  Loss:  1.5401455163955688\n",
      "Train Iter:  60  Loss:  1.4436014890670776\n",
      "Val Iter:  1  Loss:  1.1025793552398682\n",
      "Val Iter:  2  Loss:  2.124925136566162\n",
      "Val Iter:  3  Loss:  0.9944224953651428\n",
      "Val Iter:  4  Loss:  0.9307560324668884\n",
      "Val Iter:  5  Loss:  1.4614782333374023\n",
      "Val Iter:  6  Loss:  1.3572653532028198\n",
      "Val Iter:  7  Loss:  1.1073596477508545\n",
      "Val Iter:  8  Loss:  1.1794456243515015\n",
      "Val Iter:  9  Loss:  0.7705603241920471\n",
      "Val Iter:  10  Loss:  1.1879643201828003\n",
      "Val Iter:  11  Loss:  1.1734434366226196\n",
      "Val Iter:  12  Loss:  1.1053125858306885\n",
      "Val Iter:  13  Loss:  1.3665693998336792\n",
      "Val Iter:  14  Loss:  2.143172264099121\n",
      "Val Iter:  15  Loss:  1.2979100942611694\n",
      "Val Iter:  16  Loss:  1.7619986534118652\n",
      "Val Iter:  17  Loss:  0.9847091436386108\n",
      "Val Iter:  18  Loss:  0.8999360203742981\n",
      "Val Iter:  19  Loss:  1.805653691291809\n",
      "Val Iter:  20  Loss:  1.234580636024475\n",
      "Val Iter:  21  Loss:  1.800458550453186\n",
      "Val Iter:  22  Loss:  1.112919569015503\n",
      "Val Iter:  23  Loss:  1.6712534427642822\n",
      "Val Iter:  24  Loss:  1.0035101175308228\n",
      "Val Iter:  25  Loss:  1.1748254299163818\n",
      "Val Iter:  26  Loss:  0.7533835172653198\n",
      "Val Iter:  27  Loss:  0.9888515472412109\n",
      "Val Iter:  28  Loss:  1.2730212211608887\n",
      "Val Iter:  29  Loss:  1.078304409980774\n",
      "Val Iter:  30  Loss:  0.9428166151046753\n",
      "Val Iter:  31  Loss:  1.1995693445205688\n",
      "Val Iter:  32  Loss:  1.0244890451431274\n",
      "Val Iter:  33  Loss:  1.1934314966201782\n",
      "Val Iter:  34  Loss:  1.1344091892242432\n",
      "Val Iter:  35  Loss:  1.523205280303955\n",
      "Val Iter:  36  Loss:  1.8067845106124878\n",
      "Val Iter:  37  Loss:  1.4008513689041138\n",
      "Val Iter:  38  Loss:  0.9296209812164307\n",
      "Val Iter:  39  Loss:  1.4289690256118774\n",
      "Val Iter:  40  Loss:  1.0277862548828125\n",
      "Val Iter:  41  Loss:  1.1268428564071655\n",
      "Val Iter:  42  Loss:  1.250552773475647\n",
      "Val Iter:  43  Loss:  0.8014990091323853\n",
      "Val Iter:  44  Loss:  2.0926084518432617\n",
      "Val Iter:  45  Loss:  1.1179524660110474\n",
      "Val Iter:  46  Loss:  0.8009664416313171\n",
      "Val Iter:  47  Loss:  1.5161184072494507\n",
      "Val Iter:  48  Loss:  1.155935525894165\n",
      "Val Iter:  49  Loss:  1.0639194250106812\n",
      "Val Iter:  50  Loss:  1.3237512111663818\n",
      "Val Iter:  51  Loss:  1.0996794700622559\n",
      "Val Iter:  52  Loss:  1.1720154285430908\n",
      "Val Iter:  53  Loss:  1.4308900833129883\n",
      "Val Iter:  54  Loss:  1.176239252090454\n",
      "Val Iter:  55  Loss:  1.163600206375122\n",
      "Val Iter:  56  Loss:  0.9654335379600525\n",
      "Val Iter:  57  Loss:  1.9555325508117676\n",
      "Val Iter:  58  Loss:  2.8974997997283936\n",
      "Val Iter:  59  Loss:  2.0539493560791016\n",
      "Val Iter:  60  Loss:  1.2227811813354492\n",
      "Val Iter:  61  Loss:  0.9927039742469788\n",
      "Val Iter:  62  Loss:  1.357040286064148\n",
      "Val Iter:  63  Loss:  1.1720290184020996\n",
      "Val Iter:  64  Loss:  1.188589334487915\n",
      "Val Iter:  65  Loss:  1.1556270122528076\n",
      "Val Iter:  66  Loss:  1.0387868881225586\n",
      "Val Iter:  67  Loss:  1.4150550365447998\n",
      "Val Iter:  68  Loss:  1.1215198040008545\n",
      "Val Iter:  69  Loss:  1.388403296470642\n",
      "Val Iter:  70  Loss:  1.5955297946929932\n",
      "Val Iter:  71  Loss:  0.9499357342720032\n",
      "Val Iter:  72  Loss:  0.8916064500808716\n",
      "Val Iter:  73  Loss:  1.5205202102661133\n",
      "Val Iter:  74  Loss:  1.6461752653121948\n",
      "Val Iter:  75  Loss:  1.1831945180892944\n",
      "Val Iter:  76  Loss:  1.3994063138961792\n",
      "Val Iter:  77  Loss:  1.7457369565963745\n",
      "Val Iter:  78  Loss:  1.1273212432861328\n",
      "Val Iter:  79  Loss:  0.8936614990234375\n",
      "Val Iter:  80  Loss:  1.5121655464172363\n",
      "Val Iter:  81  Loss:  1.263372778892517\n",
      "Val Iter:  82  Loss:  1.2566503286361694\n",
      "Val Iter:  83  Loss:  1.1675009727478027\n",
      "Val Iter:  84  Loss:  0.967449426651001\n",
      "Val Iter:  85  Loss:  1.5277024507522583\n",
      "Val Iter:  86  Loss:  1.4295095205307007\n",
      "Val Iter:  87  Loss:  1.4731073379516602\n",
      "Val Iter:  88  Loss:  1.1513192653656006\n",
      "Val Iter:  89  Loss:  1.5110912322998047\n",
      "Val Iter:  90  Loss:  1.5007222890853882\n",
      "Val Iter:  91  Loss:  1.0245493650436401\n",
      "Val Iter:  92  Loss:  1.0175957679748535\n",
      "Val Iter:  93  Loss:  1.3399133682250977\n",
      "Val Iter:  94  Loss:  1.2172750234603882\n",
      "Val Iter:  95  Loss:  1.276857852935791\n",
      "Val Iter:  96  Loss:  1.0946205854415894\n",
      "Val Iter:  97  Loss:  1.432741641998291\n",
      "Val Iter:  98  Loss:  0.8224444389343262\n",
      "Val Iter:  99  Loss:  0.8005959391593933\n",
      "Val Iter:  100  Loss:  1.0393927097320557\n",
      "FINAL Mean Train Loss: 1.294820100069046  Mean Val Loss:  1.274556912779808\n",
      "Train Iter:  1  Loss:  1.2236069440841675\n",
      "Train Iter:  2  Loss:  1.5642027854919434\n",
      "Train Iter:  3  Loss:  1.1738158464431763\n",
      "Train Iter:  4  Loss:  1.5546518564224243\n",
      "Train Iter:  5  Loss:  1.422038197517395\n",
      "Train Iter:  6  Loss:  1.2772082090377808\n",
      "Train Iter:  7  Loss:  1.4608137607574463\n",
      "Train Iter:  8  Loss:  1.3244893550872803\n",
      "Train Iter:  9  Loss:  1.3297758102416992\n",
      "Train Iter:  10  Loss:  1.0432310104370117\n",
      "Train Iter:  11  Loss:  1.3295282125473022\n",
      "Train Iter:  12  Loss:  0.9967944025993347\n",
      "Train Iter:  13  Loss:  0.9525963664054871\n",
      "Train Iter:  14  Loss:  1.2126483917236328\n",
      "Train Iter:  15  Loss:  1.4262484312057495\n",
      "Train Iter:  16  Loss:  1.3021888732910156\n",
      "Train Iter:  17  Loss:  2.04913067817688\n",
      "Train Iter:  18  Loss:  1.1028127670288086\n",
      "Train Iter:  19  Loss:  1.224061131477356\n",
      "Train Iter:  20  Loss:  1.363318681716919\n",
      "Train Iter:  21  Loss:  1.405901551246643\n",
      "Train Iter:  22  Loss:  1.1238138675689697\n",
      "Train Iter:  23  Loss:  1.4320406913757324\n",
      "Train Iter:  24  Loss:  1.491875171661377\n",
      "Train Iter:  25  Loss:  1.2225236892700195\n",
      "Train Iter:  26  Loss:  0.942298948764801\n",
      "Train Iter:  27  Loss:  1.1538424491882324\n",
      "Train Iter:  28  Loss:  1.1623022556304932\n",
      "Train Iter:  29  Loss:  1.2214759588241577\n",
      "Train Iter:  30  Loss:  1.10889732837677\n",
      "Train Iter:  31  Loss:  1.2039544582366943\n",
      "Train Iter:  32  Loss:  1.6207376718521118\n",
      "Train Iter:  33  Loss:  1.5825010538101196\n",
      "Train Iter:  34  Loss:  1.5242465734481812\n",
      "Train Iter:  35  Loss:  1.1135436296463013\n",
      "Train Iter:  36  Loss:  1.4260915517807007\n",
      "Train Iter:  37  Loss:  1.298804759979248\n",
      "Train Iter:  38  Loss:  1.3051079511642456\n",
      "Train Iter:  39  Loss:  1.316969633102417\n",
      "Train Iter:  40  Loss:  1.3791654109954834\n",
      "Train Iter:  41  Loss:  1.0686687231063843\n",
      "Train Iter:  42  Loss:  1.209673523902893\n",
      "Train Iter:  43  Loss:  1.1971828937530518\n",
      "Train Iter:  44  Loss:  0.8019471764564514\n",
      "Train Iter:  45  Loss:  1.2218774557113647\n",
      "Train Iter:  46  Loss:  1.0562419891357422\n",
      "Train Iter:  47  Loss:  1.5133901834487915\n",
      "Train Iter:  48  Loss:  1.106878399848938\n",
      "Train Iter:  49  Loss:  1.1336374282836914\n",
      "Train Iter:  50  Loss:  1.14289128780365\n",
      "Train Iter:  51  Loss:  1.154171109199524\n",
      "Train Iter:  52  Loss:  1.4137009382247925\n",
      "Train Iter:  53  Loss:  1.5736266374588013\n",
      "Train Iter:  54  Loss:  1.7796461582183838\n",
      "Train Iter:  55  Loss:  1.03095543384552\n",
      "Train Iter:  56  Loss:  1.2120392322540283\n",
      "Train Iter:  57  Loss:  1.1584773063659668\n",
      "Train Iter:  58  Loss:  1.298175573348999\n",
      "Train Iter:  59  Loss:  1.1269476413726807\n",
      "Train Iter:  60  Loss:  1.0559675693511963\n",
      "Val Iter:  1  Loss:  1.1765068769454956\n",
      "Val Iter:  2  Loss:  0.9618545174598694\n",
      "Val Iter:  3  Loss:  0.8912688493728638\n",
      "Val Iter:  4  Loss:  1.7156373262405396\n",
      "Val Iter:  5  Loss:  0.8485603928565979\n",
      "Val Iter:  6  Loss:  0.7378272414207458\n",
      "Val Iter:  7  Loss:  1.7567085027694702\n",
      "Val Iter:  8  Loss:  1.819176197052002\n",
      "Val Iter:  9  Loss:  1.0161703824996948\n",
      "Val Iter:  10  Loss:  1.2776132822036743\n",
      "Val Iter:  11  Loss:  1.21304190158844\n",
      "Val Iter:  12  Loss:  0.8293253779411316\n",
      "Val Iter:  13  Loss:  1.149256706237793\n",
      "Val Iter:  14  Loss:  1.5785717964172363\n",
      "Val Iter:  15  Loss:  1.5605591535568237\n",
      "Val Iter:  16  Loss:  1.7818368673324585\n",
      "Val Iter:  17  Loss:  1.4940409660339355\n",
      "Val Iter:  18  Loss:  1.2194732427597046\n",
      "Val Iter:  19  Loss:  1.5391713380813599\n",
      "Val Iter:  20  Loss:  1.5456066131591797\n",
      "Val Iter:  21  Loss:  1.3271616697311401\n",
      "Val Iter:  22  Loss:  1.2854610681533813\n",
      "Val Iter:  23  Loss:  0.986862063407898\n",
      "Val Iter:  24  Loss:  1.104504942893982\n",
      "Val Iter:  25  Loss:  1.0729511976242065\n",
      "Val Iter:  26  Loss:  1.3233283758163452\n",
      "Val Iter:  27  Loss:  0.7879555821418762\n",
      "Val Iter:  28  Loss:  1.1307849884033203\n",
      "Val Iter:  29  Loss:  1.160824179649353\n",
      "Val Iter:  30  Loss:  1.5590612888336182\n",
      "Val Iter:  31  Loss:  1.5092101097106934\n",
      "Val Iter:  32  Loss:  1.1077500581741333\n",
      "Val Iter:  33  Loss:  0.8865250945091248\n",
      "Val Iter:  34  Loss:  1.0929876565933228\n",
      "Val Iter:  35  Loss:  0.9424200654029846\n",
      "Val Iter:  36  Loss:  2.0012125968933105\n",
      "Val Iter:  37  Loss:  1.0524277687072754\n",
      "Val Iter:  38  Loss:  0.7882738709449768\n",
      "Val Iter:  39  Loss:  1.175810694694519\n",
      "Val Iter:  40  Loss:  0.978475034236908\n",
      "Val Iter:  41  Loss:  1.0281872749328613\n",
      "Val Iter:  42  Loss:  1.641576886177063\n",
      "Val Iter:  43  Loss:  1.8195264339447021\n",
      "Val Iter:  44  Loss:  0.9272975325584412\n",
      "Val Iter:  45  Loss:  0.9599810242652893\n",
      "Val Iter:  46  Loss:  1.0786114931106567\n",
      "Val Iter:  47  Loss:  2.063511848449707\n",
      "Val Iter:  48  Loss:  0.9950636625289917\n",
      "Val Iter:  49  Loss:  1.4402590990066528\n",
      "Val Iter:  50  Loss:  1.4745959043502808\n",
      "Val Iter:  51  Loss:  1.6066473722457886\n",
      "Val Iter:  52  Loss:  1.1324259042739868\n",
      "Val Iter:  53  Loss:  1.8170955181121826\n",
      "Val Iter:  54  Loss:  1.3953973054885864\n",
      "Val Iter:  55  Loss:  1.0746006965637207\n",
      "Val Iter:  56  Loss:  0.6210152506828308\n",
      "Val Iter:  57  Loss:  1.0765659809112549\n",
      "Val Iter:  58  Loss:  1.5770728588104248\n",
      "Val Iter:  59  Loss:  1.3999392986297607\n",
      "Val Iter:  60  Loss:  0.9553914070129395\n",
      "Val Iter:  61  Loss:  1.4386470317840576\n",
      "Val Iter:  62  Loss:  1.221487283706665\n",
      "Val Iter:  63  Loss:  1.3256021738052368\n",
      "Val Iter:  64  Loss:  1.3233858346939087\n",
      "Val Iter:  65  Loss:  1.1124234199523926\n",
      "Val Iter:  66  Loss:  1.9454302787780762\n",
      "Val Iter:  67  Loss:  1.0544826984405518\n",
      "Val Iter:  68  Loss:  1.1299870014190674\n",
      "Val Iter:  69  Loss:  1.953728437423706\n",
      "Val Iter:  70  Loss:  0.7906364798545837\n",
      "Val Iter:  71  Loss:  1.2085453271865845\n",
      "Val Iter:  72  Loss:  1.5086033344268799\n",
      "Val Iter:  73  Loss:  0.9346892833709717\n",
      "Val Iter:  74  Loss:  1.0799944400787354\n",
      "Val Iter:  75  Loss:  1.413139820098877\n",
      "Val Iter:  76  Loss:  1.52122962474823\n",
      "Val Iter:  77  Loss:  1.0154932737350464\n",
      "Val Iter:  78  Loss:  1.6793829202651978\n",
      "Val Iter:  79  Loss:  1.2028776407241821\n",
      "Val Iter:  80  Loss:  0.8177466988563538\n",
      "Val Iter:  81  Loss:  1.1418594121932983\n",
      "Val Iter:  82  Loss:  2.0774712562561035\n",
      "Val Iter:  83  Loss:  1.2229586839675903\n",
      "Val Iter:  84  Loss:  2.071718454360962\n",
      "Val Iter:  85  Loss:  1.0380810499191284\n",
      "Val Iter:  86  Loss:  0.7540954351425171\n",
      "Val Iter:  87  Loss:  1.2058804035186768\n",
      "Val Iter:  88  Loss:  1.1148908138275146\n",
      "Val Iter:  89  Loss:  1.1218297481536865\n",
      "Val Iter:  90  Loss:  1.415753722190857\n",
      "Val Iter:  91  Loss:  1.4046039581298828\n",
      "Val Iter:  92  Loss:  0.9134197235107422\n",
      "Val Iter:  93  Loss:  1.4958510398864746\n",
      "Val Iter:  94  Loss:  1.673714280128479\n",
      "Val Iter:  95  Loss:  1.1155436038970947\n",
      "Val Iter:  96  Loss:  1.2944831848144531\n",
      "Val Iter:  97  Loss:  0.9360848665237427\n",
      "Val Iter:  98  Loss:  0.8832305669784546\n",
      "Val Iter:  99  Loss:  1.1462825536727905\n",
      "Val Iter:  100  Loss:  1.19550359249115\n",
      "FINAL Mean Train Loss: 1.2770892163117726  Mean Val Loss:  1.2637172794342042\n",
      "Train Iter:  1  Loss:  1.2778993844985962\n",
      "Train Iter:  2  Loss:  1.3591647148132324\n",
      "Train Iter:  3  Loss:  1.319168210029602\n",
      "Train Iter:  4  Loss:  1.1435134410858154\n",
      "Train Iter:  5  Loss:  1.2076250314712524\n",
      "Train Iter:  6  Loss:  1.6681387424468994\n",
      "Train Iter:  7  Loss:  1.0966112613677979\n",
      "Train Iter:  8  Loss:  1.1631537675857544\n",
      "Train Iter:  9  Loss:  1.4104498624801636\n",
      "Train Iter:  10  Loss:  1.2531228065490723\n",
      "Train Iter:  11  Loss:  1.2330089807510376\n",
      "Train Iter:  12  Loss:  1.103888750076294\n",
      "Train Iter:  13  Loss:  1.2465651035308838\n",
      "Train Iter:  14  Loss:  0.8829540610313416\n",
      "Train Iter:  15  Loss:  1.144932508468628\n",
      "Train Iter:  16  Loss:  1.6028156280517578\n",
      "Train Iter:  17  Loss:  1.222688913345337\n",
      "Train Iter:  18  Loss:  1.4923784732818604\n",
      "Train Iter:  19  Loss:  1.20758855342865\n",
      "Train Iter:  20  Loss:  1.0744259357452393\n",
      "Train Iter:  21  Loss:  1.316909670829773\n",
      "Train Iter:  22  Loss:  0.9671071767807007\n",
      "Train Iter:  23  Loss:  1.2734217643737793\n",
      "Train Iter:  24  Loss:  1.5059517621994019\n",
      "Train Iter:  25  Loss:  1.3451213836669922\n",
      "Train Iter:  26  Loss:  1.2779436111450195\n",
      "Train Iter:  27  Loss:  1.6001871824264526\n",
      "Train Iter:  28  Loss:  1.1503804922103882\n",
      "Train Iter:  29  Loss:  1.2114735841751099\n",
      "Train Iter:  30  Loss:  1.2616266012191772\n",
      "Train Iter:  31  Loss:  1.1161285638809204\n",
      "Train Iter:  32  Loss:  1.037241816520691\n",
      "Train Iter:  33  Loss:  1.2240179777145386\n",
      "Train Iter:  34  Loss:  1.3218275308609009\n",
      "Train Iter:  35  Loss:  1.027869462966919\n",
      "Train Iter:  36  Loss:  1.3465219736099243\n",
      "Train Iter:  37  Loss:  1.3791412115097046\n",
      "Train Iter:  38  Loss:  1.1294525861740112\n",
      "Train Iter:  39  Loss:  1.1014769077301025\n",
      "Train Iter:  40  Loss:  1.1998872756958008\n",
      "Train Iter:  41  Loss:  1.0594677925109863\n",
      "Train Iter:  42  Loss:  1.3562480211257935\n",
      "Train Iter:  43  Loss:  1.2432940006256104\n",
      "Train Iter:  44  Loss:  1.1022158861160278\n",
      "Train Iter:  45  Loss:  1.2638040781021118\n",
      "Train Iter:  46  Loss:  1.0211875438690186\n",
      "Train Iter:  47  Loss:  1.3692253828048706\n",
      "Train Iter:  48  Loss:  1.457520842552185\n",
      "Train Iter:  49  Loss:  1.3783788681030273\n",
      "Train Iter:  50  Loss:  1.4418843984603882\n",
      "Train Iter:  51  Loss:  1.2785621881484985\n",
      "Train Iter:  52  Loss:  1.1228944063186646\n",
      "Train Iter:  53  Loss:  1.4225600957870483\n",
      "Train Iter:  54  Loss:  1.1458594799041748\n",
      "Train Iter:  55  Loss:  1.5217053890228271\n",
      "Train Iter:  56  Loss:  1.2669847011566162\n",
      "Train Iter:  57  Loss:  1.149643063545227\n",
      "Train Iter:  58  Loss:  1.193664312362671\n",
      "Train Iter:  59  Loss:  0.8666234612464905\n",
      "Train Iter:  60  Loss:  1.8643722534179688\n",
      "Val Iter:  1  Loss:  2.580768585205078\n",
      "Val Iter:  2  Loss:  1.6457633972167969\n",
      "Val Iter:  3  Loss:  0.7982631325721741\n",
      "Val Iter:  4  Loss:  0.9529864192008972\n",
      "Val Iter:  5  Loss:  1.1885929107666016\n",
      "Val Iter:  6  Loss:  1.2073360681533813\n",
      "Val Iter:  7  Loss:  0.9401500821113586\n",
      "Val Iter:  8  Loss:  0.9816992282867432\n",
      "Val Iter:  9  Loss:  1.4509474039077759\n",
      "Val Iter:  10  Loss:  1.2723127603530884\n",
      "Val Iter:  11  Loss:  0.9379801750183105\n",
      "Val Iter:  12  Loss:  1.304734468460083\n",
      "Val Iter:  13  Loss:  0.9329527020454407\n",
      "Val Iter:  14  Loss:  0.9772356152534485\n",
      "Val Iter:  15  Loss:  1.58694326877594\n",
      "Val Iter:  16  Loss:  1.0930426120758057\n",
      "Val Iter:  17  Loss:  1.2506349086761475\n",
      "Val Iter:  18  Loss:  1.3293836116790771\n",
      "Val Iter:  19  Loss:  1.1769733428955078\n",
      "Val Iter:  20  Loss:  1.0636144876480103\n",
      "Val Iter:  21  Loss:  1.1076065301895142\n",
      "Val Iter:  22  Loss:  1.2851982116699219\n",
      "Val Iter:  23  Loss:  0.9761410355567932\n",
      "Val Iter:  24  Loss:  1.4156957864761353\n",
      "Val Iter:  25  Loss:  1.1810871362686157\n",
      "Val Iter:  26  Loss:  1.8307292461395264\n",
      "Val Iter:  27  Loss:  1.161506175994873\n",
      "Val Iter:  28  Loss:  1.5706301927566528\n",
      "Val Iter:  29  Loss:  0.8625907897949219\n",
      "Val Iter:  30  Loss:  0.7984512448310852\n",
      "Val Iter:  31  Loss:  0.9430952072143555\n",
      "Val Iter:  32  Loss:  1.3022650480270386\n",
      "Val Iter:  33  Loss:  0.8577151298522949\n",
      "Val Iter:  34  Loss:  1.3576760292053223\n",
      "Val Iter:  35  Loss:  1.1274207830429077\n",
      "Val Iter:  36  Loss:  1.2714406251907349\n",
      "Val Iter:  37  Loss:  1.3359967470169067\n",
      "Val Iter:  38  Loss:  2.046196222305298\n",
      "Val Iter:  39  Loss:  0.9009956121444702\n",
      "Val Iter:  40  Loss:  1.5727375745773315\n",
      "Val Iter:  41  Loss:  1.2345829010009766\n",
      "Val Iter:  42  Loss:  1.5594874620437622\n",
      "Val Iter:  43  Loss:  2.396557569503784\n",
      "Val Iter:  44  Loss:  1.871046543121338\n",
      "Val Iter:  45  Loss:  1.9339054822921753\n",
      "Val Iter:  46  Loss:  0.8265867829322815\n",
      "Val Iter:  47  Loss:  1.198158621788025\n",
      "Val Iter:  48  Loss:  1.3560330867767334\n",
      "Val Iter:  49  Loss:  0.9611845016479492\n",
      "Val Iter:  50  Loss:  1.1771904230117798\n",
      "Val Iter:  51  Loss:  1.118560552597046\n",
      "Val Iter:  52  Loss:  0.9884742498397827\n",
      "Val Iter:  53  Loss:  1.4062265157699585\n",
      "Val Iter:  54  Loss:  0.9638404250144958\n",
      "Val Iter:  55  Loss:  1.3156256675720215\n",
      "Val Iter:  56  Loss:  1.1855343580245972\n",
      "Val Iter:  57  Loss:  1.413352131843567\n",
      "Val Iter:  58  Loss:  1.2721585035324097\n",
      "Val Iter:  59  Loss:  0.98536616563797\n",
      "Val Iter:  60  Loss:  0.975334644317627\n",
      "Val Iter:  61  Loss:  1.1422663927078247\n",
      "Val Iter:  62  Loss:  1.3183985948562622\n",
      "Val Iter:  63  Loss:  0.8531649112701416\n",
      "Val Iter:  64  Loss:  1.0828187465667725\n",
      "Val Iter:  65  Loss:  1.1547527313232422\n",
      "Val Iter:  66  Loss:  0.914270281791687\n",
      "Val Iter:  67  Loss:  0.7114759087562561\n",
      "Val Iter:  68  Loss:  1.469238042831421\n",
      "Val Iter:  69  Loss:  1.6617932319641113\n",
      "Val Iter:  70  Loss:  1.241742491722107\n",
      "Val Iter:  71  Loss:  1.4899994134902954\n",
      "Val Iter:  72  Loss:  1.0367460250854492\n",
      "Val Iter:  73  Loss:  1.295501708984375\n",
      "Val Iter:  74  Loss:  0.9177873730659485\n",
      "Val Iter:  75  Loss:  0.9210661053657532\n",
      "Val Iter:  76  Loss:  1.2232617139816284\n",
      "Val Iter:  77  Loss:  1.489858865737915\n",
      "Val Iter:  78  Loss:  1.2447264194488525\n",
      "Val Iter:  79  Loss:  1.461665153503418\n",
      "Val Iter:  80  Loss:  0.9669619798660278\n",
      "Val Iter:  81  Loss:  1.3119605779647827\n",
      "Val Iter:  82  Loss:  1.2637423276901245\n",
      "Val Iter:  83  Loss:  2.406226396560669\n",
      "Val Iter:  84  Loss:  1.14973783493042\n",
      "Val Iter:  85  Loss:  1.2569507360458374\n",
      "Val Iter:  86  Loss:  1.160165786743164\n",
      "Val Iter:  87  Loss:  1.396109938621521\n",
      "Val Iter:  88  Loss:  1.1198270320892334\n",
      "Val Iter:  89  Loss:  1.171277403831482\n",
      "Val Iter:  90  Loss:  1.4209588766098022\n",
      "Val Iter:  91  Loss:  0.8361154198646545\n",
      "Val Iter:  92  Loss:  1.148152470588684\n",
      "Val Iter:  93  Loss:  1.57823646068573\n",
      "Val Iter:  94  Loss:  1.4662097692489624\n",
      "Val Iter:  95  Loss:  1.44340980052948\n",
      "Val Iter:  96  Loss:  1.1394933462142944\n",
      "Val Iter:  97  Loss:  1.5938351154327393\n",
      "Val Iter:  98  Loss:  0.7401854395866394\n",
      "Val Iter:  99  Loss:  1.3622820377349854\n",
      "Val Iter:  100  Loss:  1.136155128479004\n",
      "FINAL Mean Train Loss: 1.2571646471818287  Mean Val Loss:  1.2541522508859635\n",
      "Train Iter:  1  Loss:  1.4003663063049316\n",
      "Train Iter:  2  Loss:  0.9529449939727783\n",
      "Train Iter:  3  Loss:  1.0862913131713867\n",
      "Train Iter:  4  Loss:  1.3687388896942139\n",
      "Train Iter:  5  Loss:  1.6855297088623047\n",
      "Train Iter:  6  Loss:  0.9168310165405273\n",
      "Train Iter:  7  Loss:  1.0963674783706665\n",
      "Train Iter:  8  Loss:  1.1692594289779663\n",
      "Train Iter:  9  Loss:  1.2493654489517212\n",
      "Train Iter:  10  Loss:  1.410089373588562\n",
      "Train Iter:  11  Loss:  1.0732216835021973\n",
      "Train Iter:  12  Loss:  1.508316159248352\n",
      "Train Iter:  13  Loss:  1.1418187618255615\n",
      "Train Iter:  14  Loss:  1.2468498945236206\n",
      "Train Iter:  15  Loss:  1.3251720666885376\n",
      "Train Iter:  16  Loss:  1.3987672328948975\n",
      "Train Iter:  17  Loss:  1.4387354850769043\n",
      "Train Iter:  18  Loss:  1.3424402475357056\n",
      "Train Iter:  19  Loss:  1.30311119556427\n",
      "Train Iter:  20  Loss:  1.6306504011154175\n",
      "Train Iter:  21  Loss:  1.069252610206604\n",
      "Train Iter:  22  Loss:  1.6411181688308716\n",
      "Train Iter:  23  Loss:  1.3896207809448242\n",
      "Train Iter:  24  Loss:  1.1015841960906982\n",
      "Train Iter:  25  Loss:  1.3369501829147339\n",
      "Train Iter:  26  Loss:  1.2526180744171143\n",
      "Train Iter:  27  Loss:  1.3281797170639038\n",
      "Train Iter:  28  Loss:  1.097882866859436\n",
      "Train Iter:  29  Loss:  1.7218554019927979\n",
      "Train Iter:  30  Loss:  1.3498748540878296\n",
      "Train Iter:  31  Loss:  0.8888076543807983\n",
      "Train Iter:  32  Loss:  1.2562780380249023\n",
      "Train Iter:  33  Loss:  1.037367820739746\n",
      "Train Iter:  34  Loss:  1.1623705625534058\n",
      "Train Iter:  35  Loss:  1.306642770767212\n",
      "Train Iter:  36  Loss:  1.1848704814910889\n",
      "Train Iter:  37  Loss:  1.379141092300415\n",
      "Train Iter:  38  Loss:  1.5098226070404053\n",
      "Train Iter:  39  Loss:  1.2926791906356812\n",
      "Train Iter:  40  Loss:  0.8570436239242554\n",
      "Train Iter:  41  Loss:  1.323264241218567\n",
      "Train Iter:  42  Loss:  1.2323682308197021\n",
      "Train Iter:  43  Loss:  1.2704213857650757\n",
      "Train Iter:  44  Loss:  1.4022634029388428\n",
      "Train Iter:  45  Loss:  1.2217568159103394\n",
      "Train Iter:  46  Loss:  1.1881251335144043\n",
      "Train Iter:  47  Loss:  1.0490130186080933\n",
      "Train Iter:  48  Loss:  1.0180010795593262\n",
      "Train Iter:  49  Loss:  1.12834894657135\n",
      "Train Iter:  50  Loss:  1.5223263502120972\n",
      "Train Iter:  51  Loss:  1.0265170335769653\n",
      "Train Iter:  52  Loss:  1.1095398664474487\n",
      "Train Iter:  53  Loss:  1.3450067043304443\n",
      "Train Iter:  54  Loss:  1.3937128782272339\n",
      "Train Iter:  55  Loss:  1.3439538478851318\n",
      "Train Iter:  56  Loss:  1.384377121925354\n",
      "Train Iter:  57  Loss:  1.222008228302002\n",
      "Train Iter:  58  Loss:  1.4619067907333374\n",
      "Train Iter:  59  Loss:  1.3667712211608887\n",
      "Train Iter:  60  Loss:  1.4405592679977417\n",
      "Val Iter:  1  Loss:  1.4562162160873413\n",
      "Val Iter:  2  Loss:  1.0192036628723145\n",
      "Val Iter:  3  Loss:  1.2233742475509644\n",
      "Val Iter:  4  Loss:  1.5676454305648804\n",
      "Val Iter:  5  Loss:  1.2047783136367798\n",
      "Val Iter:  6  Loss:  1.3183614015579224\n",
      "Val Iter:  7  Loss:  2.071699380874634\n",
      "Val Iter:  8  Loss:  0.9867009520530701\n",
      "Val Iter:  9  Loss:  1.2724884748458862\n",
      "Val Iter:  10  Loss:  1.3314673900604248\n",
      "Val Iter:  11  Loss:  1.99853515625\n",
      "Val Iter:  12  Loss:  1.349618673324585\n",
      "Val Iter:  13  Loss:  1.2903568744659424\n",
      "Val Iter:  14  Loss:  0.9073612689971924\n",
      "Val Iter:  15  Loss:  1.4032764434814453\n",
      "Val Iter:  16  Loss:  1.292635440826416\n",
      "Val Iter:  17  Loss:  0.977759838104248\n",
      "Val Iter:  18  Loss:  1.1873505115509033\n",
      "Val Iter:  19  Loss:  1.3116254806518555\n",
      "Val Iter:  20  Loss:  0.8687764406204224\n",
      "Val Iter:  21  Loss:  1.2664865255355835\n",
      "Val Iter:  22  Loss:  1.6700901985168457\n",
      "Val Iter:  23  Loss:  1.1819324493408203\n",
      "Val Iter:  24  Loss:  1.0028249025344849\n",
      "Val Iter:  25  Loss:  2.177771806716919\n",
      "Val Iter:  26  Loss:  1.1204321384429932\n",
      "Val Iter:  27  Loss:  1.1589354276657104\n",
      "Val Iter:  28  Loss:  1.4715631008148193\n",
      "Val Iter:  29  Loss:  0.9034990668296814\n",
      "Val Iter:  30  Loss:  1.0638676881790161\n",
      "Val Iter:  31  Loss:  1.1640098094940186\n",
      "Val Iter:  32  Loss:  1.0946117639541626\n",
      "Val Iter:  33  Loss:  1.086600422859192\n",
      "Val Iter:  34  Loss:  2.0910794734954834\n",
      "Val Iter:  35  Loss:  0.8851466178894043\n",
      "Val Iter:  36  Loss:  1.3482255935668945\n",
      "Val Iter:  37  Loss:  1.1722840070724487\n",
      "Val Iter:  38  Loss:  1.5397052764892578\n",
      "Val Iter:  39  Loss:  1.1437324285507202\n",
      "Val Iter:  40  Loss:  1.5424472093582153\n",
      "Val Iter:  41  Loss:  1.2918964624404907\n",
      "Val Iter:  42  Loss:  1.248852252960205\n",
      "Val Iter:  43  Loss:  0.8171294927597046\n",
      "Val Iter:  44  Loss:  1.7576795816421509\n",
      "Val Iter:  45  Loss:  1.8979642391204834\n",
      "Val Iter:  46  Loss:  0.6254339218139648\n",
      "Val Iter:  47  Loss:  1.3487590551376343\n",
      "Val Iter:  48  Loss:  0.9664210081100464\n",
      "Val Iter:  49  Loss:  1.4844781160354614\n",
      "Val Iter:  50  Loss:  1.5276902914047241\n",
      "Val Iter:  51  Loss:  1.4865734577178955\n",
      "Val Iter:  52  Loss:  1.5478028059005737\n",
      "Val Iter:  53  Loss:  1.484114170074463\n",
      "Val Iter:  54  Loss:  1.4580622911453247\n",
      "Val Iter:  55  Loss:  0.8225203156471252\n",
      "Val Iter:  56  Loss:  1.3196420669555664\n",
      "Val Iter:  57  Loss:  1.457810401916504\n",
      "Val Iter:  58  Loss:  1.4628450870513916\n",
      "Val Iter:  59  Loss:  0.779987096786499\n",
      "Val Iter:  60  Loss:  1.0815471410751343\n",
      "Val Iter:  61  Loss:  1.0795503854751587\n",
      "Val Iter:  62  Loss:  1.4226495027542114\n",
      "Val Iter:  63  Loss:  2.461042881011963\n",
      "Val Iter:  64  Loss:  0.7205697894096375\n",
      "Val Iter:  65  Loss:  1.0569852590560913\n",
      "Val Iter:  66  Loss:  1.7952725887298584\n",
      "Val Iter:  67  Loss:  2.098341464996338\n",
      "Val Iter:  68  Loss:  2.050663709640503\n",
      "Val Iter:  69  Loss:  1.390238881111145\n",
      "Val Iter:  70  Loss:  1.1704668998718262\n",
      "Val Iter:  71  Loss:  1.2246959209442139\n",
      "Val Iter:  72  Loss:  1.2003031969070435\n",
      "Val Iter:  73  Loss:  1.055185317993164\n",
      "Val Iter:  74  Loss:  1.120030164718628\n",
      "Val Iter:  75  Loss:  1.169205904006958\n",
      "Val Iter:  76  Loss:  1.612878441810608\n",
      "Val Iter:  77  Loss:  1.1230123043060303\n",
      "Val Iter:  78  Loss:  1.0876458883285522\n",
      "Val Iter:  79  Loss:  1.151361107826233\n",
      "Val Iter:  80  Loss:  1.5381410121917725\n",
      "Val Iter:  81  Loss:  0.8993880748748779\n",
      "Val Iter:  82  Loss:  0.8377555012702942\n",
      "Val Iter:  83  Loss:  1.2441216707229614\n",
      "Val Iter:  84  Loss:  1.1608227491378784\n",
      "Val Iter:  85  Loss:  1.0007209777832031\n",
      "Val Iter:  86  Loss:  1.511078953742981\n",
      "Val Iter:  87  Loss:  1.4248112440109253\n",
      "Val Iter:  88  Loss:  0.9232330918312073\n",
      "Val Iter:  89  Loss:  1.106600284576416\n",
      "Val Iter:  90  Loss:  1.5357428789138794\n",
      "Val Iter:  91  Loss:  1.0307527780532837\n",
      "Val Iter:  92  Loss:  1.135724425315857\n",
      "Val Iter:  93  Loss:  1.5755828619003296\n",
      "Val Iter:  94  Loss:  1.3208818435668945\n",
      "Val Iter:  95  Loss:  0.8823356628417969\n",
      "Val Iter:  96  Loss:  1.312966227531433\n",
      "Val Iter:  97  Loss:  1.6068931818008423\n",
      "Val Iter:  98  Loss:  1.5830175876617432\n",
      "Val Iter:  99  Loss:  1.0273884534835815\n",
      "Val Iter:  100  Loss:  1.1731730699539185\n",
      "FINAL Mean Train Loss: 1.2726511557896931  Mean Val Loss:  1.2981291890144349\n",
      "Train Iter:  1  Loss:  1.1248106956481934\n",
      "Train Iter:  2  Loss:  1.372249960899353\n",
      "Train Iter:  3  Loss:  0.9350995421409607\n",
      "Train Iter:  4  Loss:  1.9215342998504639\n",
      "Train Iter:  5  Loss:  1.5137311220169067\n",
      "Train Iter:  6  Loss:  1.2590075731277466\n",
      "Train Iter:  7  Loss:  1.512510061264038\n",
      "Train Iter:  8  Loss:  0.9161912798881531\n",
      "Train Iter:  9  Loss:  1.094962239265442\n",
      "Train Iter:  10  Loss:  1.1267235279083252\n",
      "Train Iter:  11  Loss:  1.274570345878601\n",
      "Train Iter:  12  Loss:  1.6927841901779175\n",
      "Train Iter:  13  Loss:  1.0910009145736694\n",
      "Train Iter:  14  Loss:  1.4081109762191772\n",
      "Train Iter:  15  Loss:  1.5791704654693604\n",
      "Train Iter:  16  Loss:  1.2450371980667114\n",
      "Train Iter:  17  Loss:  1.4405971765518188\n",
      "Train Iter:  18  Loss:  1.337302803993225\n",
      "Train Iter:  19  Loss:  0.9012041091918945\n",
      "Train Iter:  20  Loss:  1.6525661945343018\n",
      "Train Iter:  21  Loss:  1.0807157754898071\n",
      "Train Iter:  22  Loss:  1.058374047279358\n",
      "Train Iter:  23  Loss:  1.2174365520477295\n",
      "Train Iter:  24  Loss:  1.5412285327911377\n",
      "Train Iter:  25  Loss:  1.6235147714614868\n",
      "Train Iter:  26  Loss:  1.3517167568206787\n",
      "Train Iter:  27  Loss:  0.7455201148986816\n",
      "Train Iter:  28  Loss:  1.1143178939819336\n",
      "Train Iter:  29  Loss:  1.3047343492507935\n",
      "Train Iter:  30  Loss:  1.3369451761245728\n",
      "Train Iter:  31  Loss:  1.3347452878952026\n",
      "Train Iter:  32  Loss:  1.2467435598373413\n",
      "Train Iter:  33  Loss:  1.4586280584335327\n",
      "Train Iter:  34  Loss:  1.3607895374298096\n",
      "Train Iter:  35  Loss:  0.9310722947120667\n",
      "Train Iter:  36  Loss:  1.7224340438842773\n",
      "Train Iter:  37  Loss:  1.1358397006988525\n",
      "Train Iter:  38  Loss:  1.0860185623168945\n",
      "Train Iter:  39  Loss:  1.1469320058822632\n",
      "Train Iter:  40  Loss:  1.139344334602356\n",
      "Train Iter:  41  Loss:  1.3770809173583984\n",
      "Train Iter:  42  Loss:  1.2740046977996826\n",
      "Train Iter:  43  Loss:  1.2696806192398071\n",
      "Train Iter:  44  Loss:  1.0773262977600098\n",
      "Train Iter:  45  Loss:  1.0810580253601074\n",
      "Train Iter:  46  Loss:  0.9149425625801086\n",
      "Train Iter:  47  Loss:  1.403945803642273\n",
      "Train Iter:  48  Loss:  1.0960956811904907\n",
      "Train Iter:  49  Loss:  1.4944725036621094\n",
      "Train Iter:  50  Loss:  1.4805395603179932\n",
      "Train Iter:  51  Loss:  0.9638922810554504\n",
      "Train Iter:  52  Loss:  1.3529046773910522\n",
      "Train Iter:  53  Loss:  1.3653476238250732\n",
      "Train Iter:  54  Loss:  1.576644778251648\n",
      "Train Iter:  55  Loss:  1.3396961688995361\n",
      "Train Iter:  56  Loss:  1.14043128490448\n",
      "Train Iter:  57  Loss:  1.2710206508636475\n",
      "Train Iter:  58  Loss:  1.6048210859298706\n",
      "Train Iter:  59  Loss:  1.453799843788147\n",
      "Train Iter:  60  Loss:  1.159802794456482\n",
      "Val Iter:  1  Loss:  1.3527331352233887\n",
      "Val Iter:  2  Loss:  1.0041093826293945\n",
      "Val Iter:  3  Loss:  0.9860132336616516\n",
      "Val Iter:  4  Loss:  1.2223621606826782\n",
      "Val Iter:  5  Loss:  1.5077544450759888\n",
      "Val Iter:  6  Loss:  1.540918231010437\n",
      "Val Iter:  7  Loss:  1.4227544069290161\n",
      "Val Iter:  8  Loss:  1.4610106945037842\n",
      "Val Iter:  9  Loss:  0.8254051208496094\n",
      "Val Iter:  10  Loss:  1.027562141418457\n",
      "Val Iter:  11  Loss:  1.1218969821929932\n",
      "Val Iter:  12  Loss:  0.955085039138794\n",
      "Val Iter:  13  Loss:  1.2454465627670288\n",
      "Val Iter:  14  Loss:  1.3210136890411377\n",
      "Val Iter:  15  Loss:  1.3212696313858032\n",
      "Val Iter:  16  Loss:  1.1853336095809937\n",
      "Val Iter:  17  Loss:  1.346389651298523\n",
      "Val Iter:  18  Loss:  1.1222870349884033\n",
      "Val Iter:  19  Loss:  1.0005230903625488\n",
      "Val Iter:  20  Loss:  2.0148508548736572\n",
      "Val Iter:  21  Loss:  2.667050838470459\n",
      "Val Iter:  22  Loss:  1.4015008211135864\n",
      "Val Iter:  23  Loss:  1.196449875831604\n",
      "Val Iter:  24  Loss:  1.142481803894043\n",
      "Val Iter:  25  Loss:  1.0763272047042847\n",
      "Val Iter:  26  Loss:  1.08759605884552\n",
      "Val Iter:  27  Loss:  0.9473971128463745\n",
      "Val Iter:  28  Loss:  1.2741307020187378\n",
      "Val Iter:  29  Loss:  1.1789942979812622\n",
      "Val Iter:  30  Loss:  2.185816526412964\n",
      "Val Iter:  31  Loss:  2.7068772315979004\n",
      "Val Iter:  32  Loss:  1.022369623184204\n",
      "Val Iter:  33  Loss:  1.130856990814209\n",
      "Val Iter:  34  Loss:  1.2467474937438965\n",
      "Val Iter:  35  Loss:  1.5885382890701294\n",
      "Val Iter:  36  Loss:  1.0235744714736938\n",
      "Val Iter:  37  Loss:  1.5163499116897583\n",
      "Val Iter:  38  Loss:  1.4199919700622559\n",
      "Val Iter:  39  Loss:  2.0650625228881836\n",
      "Val Iter:  40  Loss:  1.0223172903060913\n",
      "Val Iter:  41  Loss:  1.3518420457839966\n",
      "Val Iter:  42  Loss:  1.0543805360794067\n",
      "Val Iter:  43  Loss:  1.006365180015564\n",
      "Val Iter:  44  Loss:  1.5590345859527588\n",
      "Val Iter:  45  Loss:  1.1222124099731445\n",
      "Val Iter:  46  Loss:  1.1404294967651367\n",
      "Val Iter:  47  Loss:  1.3291887044906616\n",
      "Val Iter:  48  Loss:  0.876313328742981\n",
      "Val Iter:  49  Loss:  0.978299081325531\n",
      "Val Iter:  50  Loss:  0.5625958442687988\n",
      "Val Iter:  51  Loss:  1.664089322090149\n",
      "Val Iter:  52  Loss:  1.0434949398040771\n",
      "Val Iter:  53  Loss:  1.1145693063735962\n",
      "Val Iter:  54  Loss:  1.1875603199005127\n",
      "Val Iter:  55  Loss:  1.4241069555282593\n",
      "Val Iter:  56  Loss:  1.2251930236816406\n",
      "Val Iter:  57  Loss:  1.6412482261657715\n",
      "Val Iter:  58  Loss:  1.1650277376174927\n",
      "Val Iter:  59  Loss:  0.9926561117172241\n",
      "Val Iter:  60  Loss:  1.4116257429122925\n",
      "Val Iter:  61  Loss:  1.1619629859924316\n",
      "Val Iter:  62  Loss:  1.905227780342102\n",
      "Val Iter:  63  Loss:  1.2160272598266602\n",
      "Val Iter:  64  Loss:  0.6145205497741699\n",
      "Val Iter:  65  Loss:  1.4381828308105469\n",
      "Val Iter:  66  Loss:  1.16582453250885\n",
      "Val Iter:  67  Loss:  0.8663800358772278\n",
      "Val Iter:  68  Loss:  1.6250792741775513\n",
      "Val Iter:  69  Loss:  0.9082541465759277\n",
      "Val Iter:  70  Loss:  0.9211829304695129\n",
      "Val Iter:  71  Loss:  1.1496440172195435\n",
      "Val Iter:  72  Loss:  1.4752118587493896\n",
      "Val Iter:  73  Loss:  1.089806079864502\n",
      "Val Iter:  74  Loss:  2.0022172927856445\n",
      "Val Iter:  75  Loss:  1.0625767707824707\n",
      "Val Iter:  76  Loss:  2.095863103866577\n",
      "Val Iter:  77  Loss:  1.373755931854248\n",
      "Val Iter:  78  Loss:  1.50376296043396\n",
      "Val Iter:  79  Loss:  1.6609987020492554\n",
      "Val Iter:  80  Loss:  1.1488842964172363\n",
      "Val Iter:  81  Loss:  2.0894343852996826\n",
      "Val Iter:  82  Loss:  2.8111255168914795\n",
      "Val Iter:  83  Loss:  1.4344267845153809\n",
      "Val Iter:  84  Loss:  1.2347537279129028\n",
      "Val Iter:  85  Loss:  1.1294399499893188\n",
      "Val Iter:  86  Loss:  1.087189793586731\n",
      "Val Iter:  87  Loss:  2.0791468620300293\n",
      "Val Iter:  88  Loss:  1.4868887662887573\n",
      "Val Iter:  89  Loss:  1.2413113117218018\n",
      "Val Iter:  90  Loss:  1.090238332748413\n",
      "Val Iter:  91  Loss:  1.8500326871871948\n",
      "Val Iter:  92  Loss:  1.3365974426269531\n",
      "Val Iter:  93  Loss:  1.1107172966003418\n",
      "Val Iter:  94  Loss:  1.2690231800079346\n",
      "Val Iter:  95  Loss:  1.065462589263916\n",
      "Val Iter:  96  Loss:  1.3962457180023193\n",
      "Val Iter:  97  Loss:  0.9810313582420349\n",
      "Val Iter:  98  Loss:  1.250754952430725\n",
      "Val Iter:  99  Loss:  0.8672651648521423\n",
      "Val Iter:  100  Loss:  1.8017570972442627\n",
      "FINAL Mean Train Loss: 1.2838953981796901  Mean Val Loss:  1.3273358929157257\n",
      "Train Iter:  1  Loss:  1.2534981966018677\n",
      "Train Iter:  2  Loss:  1.25272798538208\n",
      "Train Iter:  3  Loss:  1.4033544063568115\n",
      "Train Iter:  4  Loss:  1.3445833921432495\n",
      "Train Iter:  5  Loss:  0.9389628767967224\n",
      "Train Iter:  6  Loss:  1.2810429334640503\n",
      "Train Iter:  7  Loss:  1.2235499620437622\n",
      "Train Iter:  8  Loss:  1.3350204229354858\n",
      "Train Iter:  9  Loss:  1.199585199356079\n",
      "Train Iter:  10  Loss:  1.1751816272735596\n",
      "Train Iter:  11  Loss:  1.1332515478134155\n",
      "Train Iter:  12  Loss:  1.6261801719665527\n",
      "Train Iter:  13  Loss:  1.1503280401229858\n",
      "Train Iter:  14  Loss:  1.4384779930114746\n",
      "Train Iter:  15  Loss:  1.1464000940322876\n",
      "Train Iter:  16  Loss:  1.236081600189209\n",
      "Train Iter:  17  Loss:  1.7318847179412842\n",
      "Train Iter:  18  Loss:  1.5303775072097778\n",
      "Train Iter:  19  Loss:  1.5592060089111328\n",
      "Train Iter:  20  Loss:  1.0673426389694214\n",
      "Train Iter:  21  Loss:  0.9953087568283081\n",
      "Train Iter:  22  Loss:  1.1673429012298584\n",
      "Train Iter:  23  Loss:  1.2872681617736816\n",
      "Train Iter:  24  Loss:  1.5585808753967285\n",
      "Train Iter:  25  Loss:  1.0788980722427368\n",
      "Train Iter:  26  Loss:  1.2512574195861816\n",
      "Train Iter:  27  Loss:  1.649491548538208\n",
      "Train Iter:  28  Loss:  0.9226336479187012\n",
      "Train Iter:  29  Loss:  1.3599284887313843\n",
      "Train Iter:  30  Loss:  1.4558446407318115\n",
      "Train Iter:  31  Loss:  1.549456238746643\n",
      "Train Iter:  32  Loss:  1.650341272354126\n",
      "Train Iter:  33  Loss:  1.0861055850982666\n",
      "Train Iter:  34  Loss:  1.3808168172836304\n",
      "Train Iter:  35  Loss:  1.0105351209640503\n",
      "Train Iter:  36  Loss:  1.354151964187622\n",
      "Train Iter:  37  Loss:  1.1790432929992676\n",
      "Train Iter:  38  Loss:  1.2686467170715332\n",
      "Train Iter:  39  Loss:  1.3636730909347534\n",
      "Train Iter:  40  Loss:  1.0148741006851196\n",
      "Train Iter:  41  Loss:  1.0298799276351929\n",
      "Train Iter:  42  Loss:  1.4317878484725952\n",
      "Train Iter:  43  Loss:  1.6955773830413818\n",
      "Train Iter:  44  Loss:  1.3540699481964111\n",
      "Train Iter:  45  Loss:  0.9919448494911194\n",
      "Train Iter:  46  Loss:  0.9818331003189087\n",
      "Train Iter:  47  Loss:  1.2929294109344482\n",
      "Train Iter:  48  Loss:  1.2074106931686401\n",
      "Train Iter:  49  Loss:  1.6020379066467285\n",
      "Train Iter:  50  Loss:  1.1445207595825195\n",
      "Train Iter:  51  Loss:  1.3208997249603271\n",
      "Train Iter:  52  Loss:  1.1952592134475708\n",
      "Train Iter:  53  Loss:  1.4845048189163208\n",
      "Train Iter:  54  Loss:  1.2105510234832764\n",
      "Train Iter:  55  Loss:  1.3893605470657349\n",
      "Train Iter:  56  Loss:  1.1991010904312134\n",
      "Train Iter:  57  Loss:  1.1999551057815552\n",
      "Train Iter:  58  Loss:  1.3586726188659668\n",
      "Train Iter:  59  Loss:  1.1388139724731445\n",
      "Train Iter:  60  Loss:  1.4920276403427124\n",
      "Val Iter:  1  Loss:  0.9445828199386597\n",
      "Val Iter:  2  Loss:  1.2203391790390015\n",
      "Val Iter:  3  Loss:  1.4620754718780518\n",
      "Val Iter:  4  Loss:  1.331976056098938\n",
      "Val Iter:  5  Loss:  1.0533154010772705\n",
      "Val Iter:  6  Loss:  0.9567769765853882\n",
      "Val Iter:  7  Loss:  1.4478626251220703\n",
      "Val Iter:  8  Loss:  1.2880696058273315\n",
      "Val Iter:  9  Loss:  1.1114091873168945\n",
      "Val Iter:  10  Loss:  1.2405694723129272\n",
      "Val Iter:  11  Loss:  1.177941083908081\n",
      "Val Iter:  12  Loss:  1.9627183675765991\n",
      "Val Iter:  13  Loss:  1.2029458284378052\n",
      "Val Iter:  14  Loss:  1.5630444288253784\n",
      "Val Iter:  15  Loss:  1.1087051630020142\n",
      "Val Iter:  16  Loss:  2.655604362487793\n",
      "Val Iter:  17  Loss:  0.9917525053024292\n",
      "Val Iter:  18  Loss:  0.9894058704376221\n",
      "Val Iter:  19  Loss:  0.7973130941390991\n",
      "Val Iter:  20  Loss:  1.208078145980835\n",
      "Val Iter:  21  Loss:  0.9049992561340332\n",
      "Val Iter:  22  Loss:  0.8220639228820801\n",
      "Val Iter:  23  Loss:  0.9948551058769226\n",
      "Val Iter:  24  Loss:  1.3451651334762573\n",
      "Val Iter:  25  Loss:  0.9764574766159058\n",
      "Val Iter:  26  Loss:  1.5590815544128418\n",
      "Val Iter:  27  Loss:  1.5328363180160522\n",
      "Val Iter:  28  Loss:  1.5081969499588013\n",
      "Val Iter:  29  Loss:  0.9676832556724548\n",
      "Val Iter:  30  Loss:  1.1554268598556519\n",
      "Val Iter:  31  Loss:  1.4721044301986694\n",
      "Val Iter:  32  Loss:  1.3564366102218628\n",
      "Val Iter:  33  Loss:  1.150045394897461\n",
      "Val Iter:  34  Loss:  0.9835678339004517\n",
      "Val Iter:  35  Loss:  1.7940446138381958\n",
      "Val Iter:  36  Loss:  1.7499569654464722\n",
      "Val Iter:  37  Loss:  1.2343153953552246\n",
      "Val Iter:  38  Loss:  1.1155554056167603\n",
      "Val Iter:  39  Loss:  1.1498223543167114\n",
      "Val Iter:  40  Loss:  1.1080116033554077\n",
      "Val Iter:  41  Loss:  1.4961379766464233\n",
      "Val Iter:  42  Loss:  0.9663647413253784\n",
      "Val Iter:  43  Loss:  1.3362494707107544\n",
      "Val Iter:  44  Loss:  1.6504199504852295\n",
      "Val Iter:  45  Loss:  1.3103892803192139\n",
      "Val Iter:  46  Loss:  0.9973293542861938\n",
      "Val Iter:  47  Loss:  1.376603126525879\n",
      "Val Iter:  48  Loss:  1.2425141334533691\n",
      "Val Iter:  49  Loss:  1.1314665079116821\n",
      "Val Iter:  50  Loss:  1.3141300678253174\n",
      "Val Iter:  51  Loss:  1.1465098857879639\n",
      "Val Iter:  52  Loss:  0.9711147546768188\n",
      "Val Iter:  53  Loss:  1.1924530267715454\n",
      "Val Iter:  54  Loss:  1.7134535312652588\n",
      "Val Iter:  55  Loss:  1.549515724182129\n",
      "Val Iter:  56  Loss:  0.8580241799354553\n",
      "Val Iter:  57  Loss:  1.6926305294036865\n",
      "Val Iter:  58  Loss:  1.0420787334442139\n",
      "Val Iter:  59  Loss:  1.1974300146102905\n",
      "Val Iter:  60  Loss:  0.8468952178955078\n",
      "Val Iter:  61  Loss:  1.1126043796539307\n",
      "Val Iter:  62  Loss:  1.4435738325119019\n",
      "Val Iter:  63  Loss:  1.5045661926269531\n",
      "Val Iter:  64  Loss:  0.8755473494529724\n",
      "Val Iter:  65  Loss:  1.855297565460205\n",
      "Val Iter:  66  Loss:  1.394557237625122\n",
      "Val Iter:  67  Loss:  0.5759190320968628\n",
      "Val Iter:  68  Loss:  1.353920817375183\n",
      "Val Iter:  69  Loss:  0.8473494052886963\n",
      "Val Iter:  70  Loss:  1.9031095504760742\n",
      "Val Iter:  71  Loss:  1.0036503076553345\n",
      "Val Iter:  72  Loss:  1.0003042221069336\n",
      "Val Iter:  73  Loss:  1.1078022718429565\n",
      "Val Iter:  74  Loss:  1.4220765829086304\n",
      "Val Iter:  75  Loss:  1.0431864261627197\n",
      "Val Iter:  76  Loss:  1.04120934009552\n",
      "Val Iter:  77  Loss:  1.1689932346343994\n",
      "Val Iter:  78  Loss:  1.2618199586868286\n",
      "Val Iter:  79  Loss:  1.3612568378448486\n",
      "Val Iter:  80  Loss:  1.0962867736816406\n",
      "Val Iter:  81  Loss:  0.8903374075889587\n",
      "Val Iter:  82  Loss:  0.7721832990646362\n",
      "Val Iter:  83  Loss:  1.2636306285858154\n",
      "Val Iter:  84  Loss:  0.9426047205924988\n",
      "Val Iter:  85  Loss:  1.5361096858978271\n",
      "Val Iter:  86  Loss:  1.6281570196151733\n",
      "Val Iter:  87  Loss:  1.2386616468429565\n",
      "Val Iter:  88  Loss:  1.0226635932922363\n",
      "Val Iter:  89  Loss:  1.5045757293701172\n",
      "Val Iter:  90  Loss:  0.8915718197822571\n",
      "Val Iter:  91  Loss:  1.1655933856964111\n",
      "Val Iter:  92  Loss:  1.3943397998809814\n",
      "Val Iter:  93  Loss:  0.9750075340270996\n",
      "Val Iter:  94  Loss:  0.8905725479125977\n",
      "Val Iter:  95  Loss:  1.7672884464263916\n",
      "Val Iter:  96  Loss:  1.4405369758605957\n",
      "Val Iter:  97  Loss:  0.8665304183959961\n",
      "Val Iter:  98  Loss:  2.536661148071289\n",
      "Val Iter:  99  Loss:  1.6558408737182617\n",
      "Val Iter:  100  Loss:  1.0001486539840698\n",
      "FINAL Mean Train Loss: 1.2888728936513265  Mean Val Loss:  1.253828689455986\n",
      "Train Iter:  1  Loss:  0.8469143509864807\n",
      "Train Iter:  2  Loss:  1.2236218452453613\n",
      "Train Iter:  3  Loss:  1.5674588680267334\n",
      "Train Iter:  4  Loss:  1.458128571510315\n",
      "Train Iter:  5  Loss:  1.1255607604980469\n",
      "Train Iter:  6  Loss:  1.0742950439453125\n",
      "Train Iter:  7  Loss:  1.1470032930374146\n",
      "Train Iter:  8  Loss:  0.8622449040412903\n",
      "Train Iter:  9  Loss:  0.9485766291618347\n",
      "Train Iter:  10  Loss:  1.0026403665542603\n",
      "Train Iter:  11  Loss:  0.8863670825958252\n",
      "Train Iter:  12  Loss:  1.578877568244934\n",
      "Train Iter:  13  Loss:  1.0909180641174316\n",
      "Train Iter:  14  Loss:  1.1840296983718872\n",
      "Train Iter:  15  Loss:  1.7024651765823364\n",
      "Train Iter:  16  Loss:  1.4108316898345947\n",
      "Train Iter:  17  Loss:  1.3705177307128906\n",
      "Train Iter:  18  Loss:  1.2983884811401367\n",
      "Train Iter:  19  Loss:  1.5222691297531128\n",
      "Train Iter:  20  Loss:  1.4740673303604126\n",
      "Train Iter:  21  Loss:  1.2258718013763428\n",
      "Train Iter:  22  Loss:  1.1396803855895996\n",
      "Train Iter:  23  Loss:  0.9882699847221375\n",
      "Train Iter:  24  Loss:  1.4650681018829346\n",
      "Train Iter:  25  Loss:  1.2872892618179321\n",
      "Train Iter:  26  Loss:  1.575588583946228\n",
      "Train Iter:  27  Loss:  1.0932379961013794\n",
      "Train Iter:  28  Loss:  0.793991208076477\n",
      "Train Iter:  29  Loss:  1.3967266082763672\n",
      "Train Iter:  30  Loss:  1.2762105464935303\n",
      "Train Iter:  31  Loss:  1.470263957977295\n",
      "Train Iter:  32  Loss:  1.188430666923523\n",
      "Train Iter:  33  Loss:  0.8756846785545349\n",
      "Train Iter:  34  Loss:  1.4778450727462769\n",
      "Train Iter:  35  Loss:  1.3929142951965332\n",
      "Train Iter:  36  Loss:  0.9367923140525818\n",
      "Train Iter:  37  Loss:  1.2746634483337402\n",
      "Train Iter:  38  Loss:  1.0596075057983398\n",
      "Train Iter:  39  Loss:  1.213417410850525\n",
      "Train Iter:  40  Loss:  1.4488499164581299\n",
      "Train Iter:  41  Loss:  2.057572603225708\n",
      "Train Iter:  42  Loss:  1.2304255962371826\n",
      "Train Iter:  43  Loss:  1.5006853342056274\n",
      "Train Iter:  44  Loss:  1.0475481748580933\n",
      "Train Iter:  45  Loss:  1.5870383977890015\n",
      "Train Iter:  46  Loss:  0.8298780918121338\n",
      "Train Iter:  47  Loss:  1.59914231300354\n",
      "Train Iter:  48  Loss:  1.0314610004425049\n",
      "Train Iter:  49  Loss:  1.459703803062439\n",
      "Train Iter:  50  Loss:  1.7019195556640625\n",
      "Train Iter:  51  Loss:  1.1014318466186523\n",
      "Train Iter:  52  Loss:  1.246077537536621\n",
      "Train Iter:  53  Loss:  1.322300672531128\n",
      "Train Iter:  54  Loss:  1.455037236213684\n",
      "Train Iter:  55  Loss:  1.49555504322052\n",
      "Train Iter:  56  Loss:  1.3108932971954346\n",
      "Train Iter:  57  Loss:  1.598653793334961\n",
      "Train Iter:  58  Loss:  0.9536077976226807\n",
      "Train Iter:  59  Loss:  1.4110528230667114\n",
      "Train Iter:  60  Loss:  1.3759623765945435\n",
      "Val Iter:  1  Loss:  1.43788743019104\n",
      "Val Iter:  2  Loss:  2.23468017578125\n",
      "Val Iter:  3  Loss:  1.2803306579589844\n",
      "Val Iter:  4  Loss:  1.2281519174575806\n",
      "Val Iter:  5  Loss:  2.168302297592163\n",
      "Val Iter:  6  Loss:  1.7504477500915527\n",
      "Val Iter:  7  Loss:  1.2668416500091553\n",
      "Val Iter:  8  Loss:  1.2343913316726685\n",
      "Val Iter:  9  Loss:  1.076819658279419\n",
      "Val Iter:  10  Loss:  2.9347755908966064\n",
      "Val Iter:  11  Loss:  1.288109302520752\n",
      "Val Iter:  12  Loss:  1.2898622751235962\n",
      "Val Iter:  13  Loss:  0.9830539226531982\n",
      "Val Iter:  14  Loss:  1.2100495100021362\n",
      "Val Iter:  15  Loss:  1.5158672332763672\n",
      "Val Iter:  16  Loss:  1.0468597412109375\n",
      "Val Iter:  17  Loss:  1.0269016027450562\n",
      "Val Iter:  18  Loss:  1.107423186302185\n",
      "Val Iter:  19  Loss:  2.167070150375366\n",
      "Val Iter:  20  Loss:  1.2259602546691895\n",
      "Val Iter:  21  Loss:  1.227933645248413\n",
      "Val Iter:  22  Loss:  1.8408544063568115\n",
      "Val Iter:  23  Loss:  1.5648843050003052\n",
      "Val Iter:  24  Loss:  1.092707633972168\n",
      "Val Iter:  25  Loss:  1.45361328125\n",
      "Val Iter:  26  Loss:  1.473982334136963\n",
      "Val Iter:  27  Loss:  1.407148003578186\n",
      "Val Iter:  28  Loss:  0.7706803679466248\n",
      "Val Iter:  29  Loss:  1.5591729879379272\n",
      "Val Iter:  30  Loss:  0.982343852519989\n",
      "Val Iter:  31  Loss:  1.2097877264022827\n",
      "Val Iter:  32  Loss:  1.144557237625122\n",
      "Val Iter:  33  Loss:  1.1978572607040405\n",
      "Val Iter:  34  Loss:  0.9836738705635071\n",
      "Val Iter:  35  Loss:  1.5607941150665283\n",
      "Val Iter:  36  Loss:  1.1553192138671875\n",
      "Val Iter:  37  Loss:  1.7009719610214233\n",
      "Val Iter:  38  Loss:  1.3991894721984863\n",
      "Val Iter:  39  Loss:  1.25458824634552\n",
      "Val Iter:  40  Loss:  1.746945858001709\n",
      "Val Iter:  41  Loss:  1.1163051128387451\n",
      "Val Iter:  42  Loss:  1.6163640022277832\n",
      "Val Iter:  43  Loss:  1.0741790533065796\n",
      "Val Iter:  44  Loss:  1.0778484344482422\n",
      "Val Iter:  45  Loss:  1.0326212644577026\n",
      "Val Iter:  46  Loss:  1.3765575885772705\n",
      "Val Iter:  47  Loss:  1.6026538610458374\n",
      "Val Iter:  48  Loss:  0.8399638533592224\n",
      "Val Iter:  49  Loss:  1.0962340831756592\n",
      "Val Iter:  50  Loss:  1.1780661344528198\n",
      "Val Iter:  51  Loss:  1.544181227684021\n",
      "Val Iter:  52  Loss:  1.1057696342468262\n",
      "Val Iter:  53  Loss:  1.0178263187408447\n",
      "Val Iter:  54  Loss:  0.9995992183685303\n",
      "Val Iter:  55  Loss:  1.1051914691925049\n",
      "Val Iter:  56  Loss:  1.3503758907318115\n",
      "Val Iter:  57  Loss:  2.229288339614868\n",
      "Val Iter:  58  Loss:  0.8466949462890625\n",
      "Val Iter:  59  Loss:  1.3514162302017212\n",
      "Val Iter:  60  Loss:  0.666812002658844\n",
      "Val Iter:  61  Loss:  2.140561580657959\n",
      "Val Iter:  62  Loss:  1.4411628246307373\n",
      "Val Iter:  63  Loss:  2.12328839302063\n",
      "Val Iter:  64  Loss:  1.4417717456817627\n",
      "Val Iter:  65  Loss:  1.8097562789916992\n",
      "Val Iter:  66  Loss:  0.7961887121200562\n",
      "Val Iter:  67  Loss:  0.9514244794845581\n",
      "Val Iter:  68  Loss:  1.3288649320602417\n",
      "Val Iter:  69  Loss:  1.2666139602661133\n",
      "Val Iter:  70  Loss:  1.3916138410568237\n",
      "Val Iter:  71  Loss:  1.1542785167694092\n",
      "Val Iter:  72  Loss:  1.401755690574646\n",
      "Val Iter:  73  Loss:  0.7422502040863037\n",
      "Val Iter:  74  Loss:  1.2856335639953613\n",
      "Val Iter:  75  Loss:  1.4906420707702637\n",
      "Val Iter:  76  Loss:  0.9539604783058167\n",
      "Val Iter:  77  Loss:  1.218301773071289\n",
      "Val Iter:  78  Loss:  1.2329497337341309\n",
      "Val Iter:  79  Loss:  1.3131800889968872\n",
      "Val Iter:  80  Loss:  1.1840654611587524\n",
      "Val Iter:  81  Loss:  1.1203783750534058\n",
      "Val Iter:  82  Loss:  1.4533543586730957\n",
      "Val Iter:  83  Loss:  1.082139253616333\n",
      "Val Iter:  84  Loss:  1.7761094570159912\n",
      "Val Iter:  85  Loss:  1.505915641784668\n",
      "Val Iter:  86  Loss:  1.2290208339691162\n",
      "Val Iter:  87  Loss:  1.9102498292922974\n",
      "Val Iter:  88  Loss:  2.1654748916625977\n",
      "Val Iter:  89  Loss:  0.9405462741851807\n",
      "Val Iter:  90  Loss:  1.2991931438446045\n",
      "Val Iter:  91  Loss:  0.8284204602241516\n",
      "Val Iter:  92  Loss:  1.1564362049102783\n",
      "Val Iter:  93  Loss:  1.3459371328353882\n",
      "Val Iter:  94  Loss:  0.8549456596374512\n",
      "Val Iter:  95  Loss:  0.8861180543899536\n",
      "Val Iter:  96  Loss:  1.0276176929473877\n",
      "Val Iter:  97  Loss:  0.9839851260185242\n",
      "Val Iter:  98  Loss:  1.221394658088684\n",
      "Val Iter:  99  Loss:  1.3986636400222778\n",
      "Val Iter:  100  Loss:  1.2630950212478638\n",
      "FINAL Mean Train Loss: 1.2778587937355042  Mean Val Loss:  1.3254600012302398\n",
      "Train Iter:  1  Loss:  1.3809128999710083\n",
      "Train Iter:  2  Loss:  1.4006595611572266\n",
      "Train Iter:  3  Loss:  1.4369710683822632\n",
      "Train Iter:  4  Loss:  1.1928852796554565\n",
      "Train Iter:  5  Loss:  1.2875511646270752\n",
      "Train Iter:  6  Loss:  1.1759467124938965\n",
      "Train Iter:  7  Loss:  1.0583232641220093\n",
      "Train Iter:  8  Loss:  1.2545498609542847\n",
      "Train Iter:  9  Loss:  1.181453824043274\n",
      "Train Iter:  10  Loss:  1.2945947647094727\n",
      "Train Iter:  11  Loss:  1.1410207748413086\n",
      "Train Iter:  12  Loss:  1.2568258047103882\n",
      "Train Iter:  13  Loss:  1.0643459558486938\n",
      "Train Iter:  14  Loss:  1.1621081829071045\n",
      "Train Iter:  15  Loss:  1.1384587287902832\n",
      "Train Iter:  16  Loss:  1.0907721519470215\n",
      "Train Iter:  17  Loss:  1.2507275342941284\n",
      "Train Iter:  18  Loss:  1.7420254945755005\n",
      "Train Iter:  19  Loss:  1.1272399425506592\n",
      "Train Iter:  20  Loss:  1.4133068323135376\n",
      "Train Iter:  21  Loss:  1.2499669790267944\n",
      "Train Iter:  22  Loss:  1.1840367317199707\n",
      "Train Iter:  23  Loss:  1.4834123849868774\n",
      "Train Iter:  24  Loss:  1.4147720336914062\n",
      "Train Iter:  25  Loss:  1.1562632322311401\n",
      "Train Iter:  26  Loss:  1.5867239236831665\n",
      "Train Iter:  27  Loss:  1.0883408784866333\n",
      "Train Iter:  28  Loss:  1.4824575185775757\n",
      "Train Iter:  29  Loss:  1.2389163970947266\n",
      "Train Iter:  30  Loss:  0.9626485705375671\n",
      "Train Iter:  31  Loss:  1.0819734334945679\n",
      "Train Iter:  32  Loss:  1.5059173107147217\n",
      "Train Iter:  33  Loss:  1.4249119758605957\n",
      "Train Iter:  34  Loss:  1.3164006471633911\n",
      "Train Iter:  35  Loss:  1.240674614906311\n",
      "Train Iter:  36  Loss:  1.4417284727096558\n",
      "Train Iter:  37  Loss:  1.1305066347122192\n",
      "Train Iter:  38  Loss:  1.3179993629455566\n",
      "Train Iter:  39  Loss:  1.1335359811782837\n",
      "Train Iter:  40  Loss:  1.4847439527511597\n",
      "Train Iter:  41  Loss:  1.29256010055542\n",
      "Train Iter:  42  Loss:  0.9369091987609863\n",
      "Train Iter:  43  Loss:  1.3454737663269043\n",
      "Train Iter:  44  Loss:  1.029869794845581\n",
      "Train Iter:  45  Loss:  1.4483973979949951\n",
      "Train Iter:  46  Loss:  1.3279991149902344\n",
      "Train Iter:  47  Loss:  1.0221198797225952\n",
      "Train Iter:  48  Loss:  1.2439186573028564\n",
      "Train Iter:  49  Loss:  1.5599912405014038\n",
      "Train Iter:  50  Loss:  1.162211537361145\n",
      "Train Iter:  51  Loss:  1.552083134651184\n",
      "Train Iter:  52  Loss:  1.3111763000488281\n",
      "Train Iter:  53  Loss:  1.0776609182357788\n",
      "Train Iter:  54  Loss:  1.3526897430419922\n",
      "Train Iter:  55  Loss:  1.277930736541748\n",
      "Train Iter:  56  Loss:  1.4686858654022217\n",
      "Train Iter:  57  Loss:  1.310625672340393\n",
      "Train Iter:  58  Loss:  1.2583602666854858\n",
      "Train Iter:  59  Loss:  1.0711562633514404\n",
      "Train Iter:  60  Loss:  1.4264731407165527\n",
      "Val Iter:  1  Loss:  1.2651922702789307\n",
      "Val Iter:  2  Loss:  0.9788492918014526\n",
      "Val Iter:  3  Loss:  1.6537586450576782\n",
      "Val Iter:  4  Loss:  1.296433448791504\n",
      "Val Iter:  5  Loss:  1.604099154472351\n",
      "Val Iter:  6  Loss:  1.3293923139572144\n",
      "Val Iter:  7  Loss:  1.1647080183029175\n",
      "Val Iter:  8  Loss:  1.5538665056228638\n",
      "Val Iter:  9  Loss:  1.352559208869934\n",
      "Val Iter:  10  Loss:  1.8479317426681519\n",
      "Val Iter:  11  Loss:  1.2235578298568726\n",
      "Val Iter:  12  Loss:  0.6835829019546509\n",
      "Val Iter:  13  Loss:  1.0297516584396362\n",
      "Val Iter:  14  Loss:  0.9491637349128723\n",
      "Val Iter:  15  Loss:  1.2001632452011108\n",
      "Val Iter:  16  Loss:  1.2314904928207397\n",
      "Val Iter:  17  Loss:  0.8440126776695251\n",
      "Val Iter:  18  Loss:  1.5634896755218506\n",
      "Val Iter:  19  Loss:  1.1240688562393188\n",
      "Val Iter:  20  Loss:  1.3178675174713135\n",
      "Val Iter:  21  Loss:  1.5153344869613647\n",
      "Val Iter:  22  Loss:  1.2163065671920776\n",
      "Val Iter:  23  Loss:  0.9363868832588196\n",
      "Val Iter:  24  Loss:  1.0244523286819458\n",
      "Val Iter:  25  Loss:  1.7775925397872925\n",
      "Val Iter:  26  Loss:  2.1207807064056396\n",
      "Val Iter:  27  Loss:  0.986092209815979\n",
      "Val Iter:  28  Loss:  2.345170497894287\n",
      "Val Iter:  29  Loss:  1.5999013185501099\n",
      "Val Iter:  30  Loss:  1.130939245223999\n",
      "Val Iter:  31  Loss:  1.233197808265686\n",
      "Val Iter:  32  Loss:  0.9720410108566284\n",
      "Val Iter:  33  Loss:  1.5397672653198242\n",
      "Val Iter:  34  Loss:  1.96126127243042\n",
      "Val Iter:  35  Loss:  1.1104875802993774\n",
      "Val Iter:  36  Loss:  0.9714192152023315\n",
      "Val Iter:  37  Loss:  1.2029321193695068\n",
      "Val Iter:  38  Loss:  0.8535305261611938\n",
      "Val Iter:  39  Loss:  2.3400168418884277\n",
      "Val Iter:  40  Loss:  1.1780844926834106\n",
      "Val Iter:  41  Loss:  1.4753196239471436\n",
      "Val Iter:  42  Loss:  0.9421926140785217\n",
      "Val Iter:  43  Loss:  1.2110700607299805\n",
      "Val Iter:  44  Loss:  1.1422724723815918\n",
      "Val Iter:  45  Loss:  1.2907224893569946\n",
      "Val Iter:  46  Loss:  1.4203779697418213\n",
      "Val Iter:  47  Loss:  1.453934669494629\n",
      "Val Iter:  48  Loss:  0.7993406057357788\n",
      "Val Iter:  49  Loss:  1.0044395923614502\n",
      "Val Iter:  50  Loss:  0.9210838079452515\n",
      "Val Iter:  51  Loss:  1.0555920600891113\n",
      "Val Iter:  52  Loss:  1.5238232612609863\n",
      "Val Iter:  53  Loss:  1.1774072647094727\n",
      "Val Iter:  54  Loss:  1.6025279760360718\n",
      "Val Iter:  55  Loss:  1.0323673486709595\n",
      "Val Iter:  56  Loss:  1.441994547843933\n",
      "Val Iter:  57  Loss:  1.0017039775848389\n",
      "Val Iter:  58  Loss:  0.9520201683044434\n",
      "Val Iter:  59  Loss:  0.8529485464096069\n",
      "Val Iter:  60  Loss:  0.9962761998176575\n",
      "Val Iter:  61  Loss:  1.3269903659820557\n",
      "Val Iter:  62  Loss:  1.0406750440597534\n",
      "Val Iter:  63  Loss:  0.9634448885917664\n",
      "Val Iter:  64  Loss:  1.2985401153564453\n",
      "Val Iter:  65  Loss:  1.1543190479278564\n",
      "Val Iter:  66  Loss:  0.8272629380226135\n",
      "Val Iter:  67  Loss:  1.1876620054244995\n",
      "Val Iter:  68  Loss:  1.3177459239959717\n",
      "Val Iter:  69  Loss:  1.2872426509857178\n",
      "Val Iter:  70  Loss:  1.62777841091156\n",
      "Val Iter:  71  Loss:  1.2066766023635864\n",
      "Val Iter:  72  Loss:  1.249672293663025\n",
      "Val Iter:  73  Loss:  1.1268309354782104\n",
      "Val Iter:  74  Loss:  1.0015181303024292\n",
      "Val Iter:  75  Loss:  0.9045183062553406\n",
      "Val Iter:  76  Loss:  1.5608689785003662\n",
      "Val Iter:  77  Loss:  1.1500431299209595\n",
      "Val Iter:  78  Loss:  1.1803920269012451\n",
      "Val Iter:  79  Loss:  1.1099361181259155\n",
      "Val Iter:  80  Loss:  1.0865346193313599\n",
      "Val Iter:  81  Loss:  0.9014732241630554\n",
      "Val Iter:  82  Loss:  0.8840945363044739\n",
      "Val Iter:  83  Loss:  1.0755783319473267\n",
      "Val Iter:  84  Loss:  1.056802749633789\n",
      "Val Iter:  85  Loss:  2.1213231086730957\n",
      "Val Iter:  86  Loss:  1.0266588926315308\n",
      "Val Iter:  87  Loss:  1.1251635551452637\n",
      "Val Iter:  88  Loss:  1.0349364280700684\n",
      "Val Iter:  89  Loss:  1.2145562171936035\n",
      "Val Iter:  90  Loss:  1.2535209655761719\n",
      "Val Iter:  91  Loss:  1.143913984298706\n",
      "Val Iter:  92  Loss:  0.9694383144378662\n",
      "Val Iter:  93  Loss:  1.028066873550415\n",
      "Val Iter:  94  Loss:  1.140066385269165\n",
      "Val Iter:  95  Loss:  1.4376667737960815\n",
      "Val Iter:  96  Loss:  1.4817794561386108\n",
      "Val Iter:  97  Loss:  1.4020715951919556\n",
      "Val Iter:  98  Loss:  1.2028268575668335\n",
      "Val Iter:  99  Loss:  1.5654938220977783\n",
      "Val Iter:  100  Loss:  0.9618558287620544\n",
      "FINAL Mean Train Loss: 1.2742150594790778  Mean Val Loss:  1.2416298979520797\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  learner.eval()\n",
    "  for i in range(len(savesToTest)):\n",
    "    epoch = savesToTest[i]\n",
    "    savefile=savefolder+\"save\"+str(epoch)+\".pth\"\n",
    "    checkpoint = torch.load(savefile)\n",
    "    learner.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    j=0\n",
    "    nTimes=60\n",
    "    for imgs, lbls in dl_train:\n",
    "      j+=1\n",
    "      if (j>nTimes):  # calculate loss of only first 1000 samples (approx)\n",
    "        break\n",
    "\n",
    "      imgs = imgs.to(device=device)\n",
    "      lbls = lbls.to(device=device)\n",
    "\n",
    "      pred = learner(imgs)\n",
    "      lbls = lbls.squeeze(dim=1)\n",
    "      loss = criterion(pred, lbls).item()\n",
    "      total_train_loss+=loss\n",
    "      print(\"Train Iter: \",j, \" Loss: \", loss)\n",
    "      \n",
    "    mean_train_loss = total_train_loss/nTimes\n",
    "\n",
    "    nTimes=100\n",
    "    j=0\n",
    "    for imgs, lbls in dl_val:\n",
    "          \n",
    "      j+=1\n",
    "      if (j>nTimes):  # calculate loss of only first 100 samples (approx)\n",
    "        break\n",
    "\n",
    "      imgs = imgs.to(device=device)\n",
    "      lbls = lbls.to(device=device)\n",
    "\n",
    "      pred = learner(imgs)\n",
    "      lbls = lbls.squeeze(dim=1)\n",
    "      loss = criterion(pred, lbls).item()\n",
    "      total_val_loss+=loss\n",
    "      \n",
    "      print(\"Val Iter: \", j,\" Loss: \", loss)\n",
    "\n",
    "    mean_val_loss = total_val_loss/nTimes\n",
    "    valLoss.append(mean_val_loss)\n",
    "    trainLoss.append(mean_train_loss)\n",
    "    print(\"FINAL Mean Train Loss:\", mean_train_loss,\" Mean Val Loss: \", mean_val_loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "colab_type": "code",
    "id": "-MuIID6ACz4F",
    "outputId": "62e610e4-0b8b-4221-ede1-cf936e0a43dd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(14, 8))\n",
    "plt.plot(savesToTest, trainLoss, label=\"Mean Train Loss\", marker=\"+\", color=\"forestgreen\")\n",
    "plt.plot(savesToTest, valLoss, label=\"Mean Valid Loss\", marker=\"+\", color=\"darksalmon\")\n",
    "plt.ylabel(\"Mean Loss\", fontsize=14)\n",
    "plt.xlabel(\"Number of Epochs\", fontsize=14)\n",
    "plt.xticks(savesToTest)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OPDIFz6Yquj3"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame((trainLoss, valLoss), columns=savesToTest)\n",
    "X.to_csv(savefolder+\"_expermiment_train_val_loss_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEVdauIQ9Pox"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "UNet Training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "58b6bc4a03b8490ea2df94edf76ccac9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "669fc0eba1af418f9e126691035eea72": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a767a54133a4591a50ed379d03a450c",
      "max": 30,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fe5eca4753874493aae6868bcef356a2",
      "value": 30
     }
    },
    "79f5f65cd3654286889f6705916bde59": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a767a54133a4591a50ed379d03a450c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90c4b614c37746448e6eeac765fccea3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e15da71a58a1404d8aea8efee6243d4f",
      "placeholder": "​",
      "style": "IPY_MODEL_58b6bc4a03b8490ea2df94edf76ccac9",
      "value": " 30/30 [5:19:46&lt;00:00, 639.55s/it]"
     }
    },
    "e15da71a58a1404d8aea8efee6243d4f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e76e1334a002410a852b0643a8bf8a57": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_669fc0eba1af418f9e126691035eea72",
       "IPY_MODEL_90c4b614c37746448e6eeac765fccea3"
      ],
      "layout": "IPY_MODEL_79f5f65cd3654286889f6705916bde59"
     }
    },
    "fe5eca4753874493aae6868bcef356a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
